{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00000", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 0, "start_word": 0, "end_word": 385, "approx_tokens": 500, "text": "Introduction Machine learning has come of age. And just in case you might think this is a mere platitude, let me clarify. The dream that machines would one day be able to learn is as old as computers themselves, perhaps older still. For a long time, however, it remained just that: a dream. True, Rosenblatt’s perceptron did trigger a wave of activity, but in retrospect, the excitement has to be deemed short-lived. As for the attempts that followed, these fared even worse; barely noticed, often ignored, they never made a breakthrough— no software companies, no major follow-up research, and not much support from funding agencies. Machine learning remained an underdog, condemned to live in the shadow of more successful disciplines. The grand ambition lay dormant. And then it all changed. A group of visionaries pointed out a weak spot in the knowledge-based systems that were all the rage in the 1970s’ artiﬁcial intelligence: where was the “know- ledge” to come from? The prevailing wisdom of the day insisted that it should take the form of if-then rules put together by the joint effort of engineers and ﬁeld experts. Practical experience, though, was unconvincing. Experts found it difﬁcult to communicate what they knew to engineers. Engineers, in turn, were at a loss as to what questions to ask and what to make of the answers. A few widely publicized success stories notwithstanding, most attempts to create a knowledge base of, say, tens of thousands of such rules proved frustrating. The proposition made by the visionaries was both simple and audacious. If it is so hard to tell a machine exactly how to go about a certain problem, why not provide the instruction indirectly, conveying the necessary skills by way of examples from which the computer will—yes—learn! Of course, this only makes sense if we can rely on the existence of algorithms to do the learning. This was the main difﬁculty. As it turned out, neither Rosenblatt’s perceptron nor the techniques developed after it were very useful. But the absence of the requisite machine-learning techniques was not an obstacle; rather, it was a challenge that inspired quite a few brilliant minds. The idea of endowing computers with learning skills opened new horizons and created a large amount of excitement. The world was beginning to take notice. xi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00001", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 1, "start_word": 385, "end_word": 857, "approx_tokens": 613, "text": "xii Introduction The bombshell exploded in 1983. Machine Learning: The AI Approach 1 was a thick volume of research papers which proposed the most diverse ways of addressing the great mystery. Under their inﬂuence, a new scientiﬁc discipline was born—virtually overnight. Three years later, a follow-up book appeared and then another. A soon-to-become-prestigious scientiﬁc journal was founded. Annual conferences of great repute were launched. And dozens, perhaps hundreds, of doctoral dissertations, were submitted and successfully defended. In this early stage, the question was not only how to learn but also what to learn and why. In retrospect, those were wonderful times, so creative that they deserve to be remembered with nostalgia. It is only to be regretted that so many great thoughts later came to be abandoned. Practical needs of realistic applications got the upper hand, pointing to the most promising avenues for further efforts. After a period of enchantment, concrete research strands crystallized: induction of theif-then rules for knowledge-based systems; induction of classiﬁers, programs capable of improving their skills based on experience; automatic ﬁne-tuning of Prolog programs; and some others. So many were the directions that some leading personalities felt it necessary to try to steer further development by writing monographs, some successful, others less so. An important watershed was Tom Mitchell’s legendary textbook. 2 This summa- rized the state of the art of the ﬁeld in a format appropriate for doctoral students and scientists alike. One by one, universities started offering graduate courses that were usually built around this book. Meanwhile, the research methodology became more systematic, too. A rich repository of machine-leaning test beds was created, making it possible to compare the performance or learning algorithms. Statistical methods of evaluation became widespread. Public domain versions of most popular programs were made available. The number of scientists dealing with this discipline grew to thousands, perhaps even more. Now, we have reached the stage where a great many universities are offering machine learning as an undergraduate class. This is quite a new situation. As a rule, these classes call for a different kind of textbook. Apart from mastering the baseline techniques, future engineers need to develop a good grasp of the strengths and weaknesses of alternative approaches; they should be aware of the peculiarities and idiosyncrasies of different paradigms. Above all, they must understand the circumstances under which some techniques succeed and others fail. Only then will they be able to make the right choices when addressing concrete applications. A textbook that is to provide all of the above should contain less mathematics, but a lot of practical advice. These then are the considerations that have dictated the size, structure, and style of a teaching text meant to provide the material for a one-semester introductory course. 1Edited by R. Michalski, J. Carbonell, and T. Mitchell. 2T. Mitchell, Machine Learning, McGraw-Hill (1997)."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00002", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 2, "start_word": 857, "end_word": 1191, "approx_tokens": 434, "text": "Introduction xiii The ﬁrst problem is the choice of material. At a time when high-tech companies are establishing machine-learning groups, universities have to provide the students with such knowledge, skills, and understanding that are relevant to the current needs of the industry. For this reason, preference has been given to Bayesian classiﬁers, nearest-neighbor classiﬁers, linear and polynomial classiﬁers, decision trees, the fundamentals of the neural networks, and the principle of the boosting algorithms. Signiﬁcant space has been devoted to certain typical aspects of concrete engineering applications. When applied to really difﬁcult tasks, the baseline techniques are known to behave not exactly the same way they do in the toy domains employed by the instructor. One has to know what to expect. The book consists of 17 chapters, each covering one major topic. The chapters are divided into sections, each devoted to one critical problem. The student is advised to proceed to the next section only after having answered the set of 2–4 “control questions” at the end of the previous section. These questions are here to help the student decide whether he or she has mastered the given material. If not, it is necessary to return to the previous text. As they say, only practice makes perfect. This is why at the end of each chapter are exercises to encourage the necessary practicing. Deeper insight into the diverse aspects of the material will then be gained by going through the thought experiments that follow. These are more difﬁcult, but it is only through hard work that an engineer develops the right kind of understanding. The acquired knowledge is then further solidiﬁed by suggested computer projects. Programming is important, too. Nowadays, everybody is used to downloading the requisite software from the web. This shortcut, however, is not recommended to the student of this book. It is only by being forced to ﬂesh out all the details of a computer program that you learn to appreciate all the subtle points of the machine-learning techniques presented here."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00003", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 3, "start_word": 1191, "end_word": 1292, "approx_tokens": 131, "text": "likes: Johnny does NOT like: A simple machine-learning task: induce a classiﬁer capable of labeling future pies as positive and negative instances of “a pie that Johnny likes” The number of classes can of course be greater. Thus a classiﬁer that decides whether a landscape snapshot was taken in spring, summer, fall,o r winter distinguishes four. Software that identiﬁes characters scribbled on an iPad needs at least 36 classes: 26 for letters and 10 for digits. And document- categorization systems are capable of identifying hundreds, even thousands of different topics. Our only motivation for choosing a two-class domain is its simplicity."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00004", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 4, "start_word": 1292, "end_word": 1659, "approx_tokens": 477, "text": "1.1 Training Sets and Classiﬁers 3 The twelve training examples expressed in a matrix form Crust Filling Example Shape Size Shade Size Shade Class ex1 Circle Thick Gray Thick Dark pos ex2 Circle Thick White Thick Dark pos ex3 Triangle Thick Dark Thick Gray pos ex4 Circle Thin White Thin Dark pos ex5 Square Thick Dark Thin White pos ex6 Circle Thick White Thin Dark pos ex7 Circle Thick Gray Thick White neg ex8 Square Thick White Thick Gray neg ex9 Triangle Thin Gray Thin Dark neg ex10 Circle Thick Dark Thick White neg ex11 Square Thick White Thick Dark neg ex12 Triangle Thick White Thick Gray neg Attribute Vectors To be able to communicate the training examples to the machine, we have to describe them in an appropriate way. The most common mechanism relies on so-called attributes. In the “pies” domain, ﬁve may be suggested: shape (circle, triangle, and square), crust-size (thin or thick), crust-shade (white, gray, or dark), filling-size (thin or thick), and filling-shade (white, gray, or dark). speciﬁes the values of these attributes for the twelve examples in . For instance, the pie in the upper- left corner of the picture (the table calls it ex1) is described by the following conjunction: (shape=circle) AND (crust-size=thick) AND (crust-shade=gray) AND (filling-size=thick) AND (filling-shade=dark) A Classiﬁer to Be Induced The training set constitutes the input from which we are to induce the classiﬁer. But what classiﬁer? Suppose we want it in the form of a boolean function that is true for positive examples and false for negative ones. Checking the expression [(shape=circle) AND (filling-shade=dark)] against the training set, we can see that its value is false for all negative examples: while it is possible to ﬁnd negative examples that are circular, none of these has a dark ﬁlling. As for the positive examples, however, the expression istrue for four of them and false for the remaining two. This means that the classiﬁer makes two errors, a transgression we might refuse to tolerate, suspecting there is a better solution. Indeed, the reader will easily verify that the following expression never goes wrong on the entire training set: [ (shape=circle) AND (filling-shade=dark) ] OR [ NOT(shape=circle) AND (crust-shade=dark) ]"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00005", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 5, "start_word": 1659, "end_word": 1890, "approx_tokens": 300, "text": "1.2 Minor Digression: Hill-Climbing Search 5 • In the “pies” domain, ﬁnd a boolean expression that correctly classiﬁes all the training examples from . 1.2 Minor Digression: Hill-Climbing Search Let us now formalize what we mean by search, and then introduce one pop- ular algorithm, the so-called hill climbing . Artiﬁcial Intelligence deﬁnes search something like this: starting from an initial state, ﬁnd a sequence of steps which, proceeding through a set of interim search states, lead to a predeﬁned ﬁnal state. The individual steps—transitions from one search state to another—are carried out by search operators which, too, have been pre-speciﬁed by the programmer. The order in which the search operators are applied follows a speciﬁc search strategy . Hill Climbing: An Illustration One popular search strategy is hill climbing.L e t us illustrate its essence on a well-known brain-teaser, the sliding-tiles puzzle. The board of a trivial version of this game consists of nine squares arranged in three rows, eight covered by numbered tiles (integers from 1 to 8), the last left empty. We convert one search state into another by sliding to the empty square a tile from one of its neighbors. The goal is to achieve a pre-speciﬁed arrangement of the tiles. Search Operators Search Strategy Final StateInitial State Search Agent A search problem is characterized by an initial state, ﬁnal state, search operators, and a search strategy"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00006", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 6, "start_word": 1890, "end_word": 2126, "approx_tokens": 306, "text": "1.2 Minor Digression: Hill-Climbing Search 7 Final State: d=1 2 1 2 3 4 567 8 1 2 d=1 3 d=1 4 d=1 3 2 7 1 4 583 6 Hill Climbing initial state 2 7 1 4 583 62 7 1 4 583 6 2 7 1 4 583 6 3 d=1 2 2 7 1 4 583 6 4 d=1 3 d=1 4 d=1 4 2 7 1 4 583 6 2 7 1 4 583 6 2 7 1 4 5 83 6 5 Hill climbing. Circled integers indicate the order in which the search states are visited. d is a state’s distance from the ﬁnal state as calculated by the given evaluation function. Ties are broken randomly state, and a ﬁfth sorts the “child” states according to the distances thus calculated and places them at the front of the listL. And the last function checks if a termination criterion has been satisﬁed.1 One last observation: at some of the states in , no “child” offers any improvement over its “parent,” a lowerd-value being achieved only after temporary compromises. This is what a mountain climber may experience, too: sometimes, he has to traverse a valley before being able to resume the ascent. The mountain- climbing metaphor, by the way, is what gave this technique its name. 1For simplicity, the pseudocode ignores termination criteria other than reaching, or failing to reach, the ﬁnal state."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00007", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 7, "start_word": 2126, "end_word": 2538, "approx_tokens": 535, "text": "1.4 The Induced Classiﬁer’s Performance 11 1.4 The Induced Classiﬁer’s Performance So far, we have measured the error rate by comparing the training examples’ known classes with those recommended by the classiﬁer. Practically speaking, though, our goal is not to re-classify objects whose classes we already know; what we really want is to label future examples, those of whose classes we are as yet ignorant. The classiﬁer’s anticipated performance on these is estimated experimentally. It is important to know how. Independent Testing Examples The simplest scenario will divide the available pre-classiﬁed examples into two parts: the training set, from which the classiﬁer is induced, and the testing set, on which it is evaluated . Thus in the “pies” domain, with its 12 pre-classiﬁed examples, the induction may be carried out on randomly selected eight, and the testing on the remaining four. If the classiﬁer then “guesses” correctly the class of three testing examples (while going wrong on one), its performance is estimated as 75%. Reasonable though this approach may appear, it suffers from a major drawback: a random choice of eight training examples may not be sufﬁciently representative of the underlying concept—and the same applies to the (even smaller) testing set. If we induce the meaning of a mammal from a training set consisting of a whale, a dolphin, and a platypus, the learner may be led to believe that mammals live in the sea (whale, dolphin), and sometimes lay eggs (platypus), hardly an opinion a biologist will embrace. And yet, another choice of trainingexamples may result in a Pre-classiﬁed examples are divided into the training and testing sets set training testing set available examples classiﬁer satisfying the highest standards. The point is, a different training/testing set division gives rise to a different classiﬁer—and also to a different estimate of future performance. This is particularly serious if the number of pre-classiﬁed examples is small. Suppose we want to compare two machine learning algorithms in terms of the quality of the products they induce. The problem of non-representative training sets can be mitigated by so-called random subsampling.3 The idea is to repeat the random division into the training and testing sets several times, always inducing a classiﬁer from the i-th training set, and then measuring the error rate, Ei,o nt h ei-th testing set. The algorithm that delivers classiﬁers with the lower average value of E i’s is deemed better—as far as classiﬁcation performance is concerned. 3Later, we will describe some other methodologies."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00008", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 8, "start_word": 2538, "end_word": 3067, "approx_tokens": 687, "text": "1.5 Some Difﬁculties with Available Data 13 1.5 Some Difﬁculties with Available Data In some applications, the training set is created manually: an expert prepares the examples, tags them with class labels, chooses the attributes, and speciﬁes the value of each attribute in each example. In other domains, the process is computerized. For instance, a company may want to be able to anticipate an employee’s intention to leave. Their database contains, for each person, the address, gender, marital status, function, salary raises, promotions—as well as the information about whether the person is still with the company or, if not, the day they left. From this, a program can obtain the attribute vectors, labeled as positive if the given person left within a year since the last update of the database record. Sometimes, the attribute vectors are automatically extracted from a database, and labeled by an expert. Alternatively, some examples can be obtained from a database, and others added manually. Often, two or more databases are combined. The number of such variations is virtually unlimited. But whatever the source of the examples, they are likely to suffer from imperfec- tions whose essence and consequences the engineer has to understand. Irrelevant Attributes To begin with, some attributes are important, while others are not. While Johnny may be truly fond of poppy ﬁlling, his preference for a pie will hardly be driven by the cook’s shoe size. This is something to be concerned about: irrelevant attributes add to computational costs; they can even mislead the learner. Can they be avoided? Usually not. True, in manually created domains, the expert is supposed to know which attributes really matter, but even here, things are not so simple. Thus the author of the “pies” domain might have done her best to choose those attributes she believed to matter. But unsure about the real reasons behind Johnny’s tastes, she may have included attributes whose necessity she suspected—but could not guarantee. Even more often the problems with relevance occur when the examples are extracted from a database. Databases are developed primarily with the intention to provide access to lots of information—of which usually only a tiny part pertains to the learning task. As to which part this is, we usually have no idea. Missing Attributes Conversely, some critical attributes can be missing. Mindful of his parents’ ﬁnances, Johnny may be prejudiced against expensive pies. The absence of attribute price will then make it impossible to induce a good classiﬁer: two examples, identical in terms of the available attributes, can differ in the values of the vital “missing” attribute. No wonder that, though identically described, one example is positive, and the other is negative. When this happens, we say that the training set is inconsistent. The situation is sometimes difﬁcult to avoid: not only may the expert be ignorant of the relevance of attributeprice; it may be impossible to provide this attribute’s values, and the attribute thus cannot be used anyway. Redundant Attributes Somewhat less damaging are attributes that are redundant in the sense that their values can be obtained from other attributes. If the database contains a patient’s date-of-birth as well as age, the latter is unnecessary"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00009", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 9, "start_word": 3067, "end_word": 3395, "approx_tokens": 426, "text": "20 2 Probabilities: Bayesian Classiﬁers The prior probabilities, P.pos/ D 6 12 and P.thick/ D 8 12 ;t h e conditional probabilities, P.posjthick/ D 3 8 and P.thickjpos/ D 3 6 ;a n dt h e joint probability, P.likes;thick/ D 3 12 Johnny likes Thick Filling of an example being positive given thatfilling-size=thick” is 37.5%—this is what the relative frequency of positive examples among those with thick ﬁlling implies: P.posjthick/ D Nposjthick Nthick D 3 8 D 0:375 (2.2) Applying Conditional Probability to Classiﬁcation Importantly, the relative fre- quency is calculated only for pies with the given attribute value. Among these same eight pies, ﬁve represented the negative class, which means that P.negjthick/ D 5=8 D 0:625. Observing that P.negjthick/> P.posjthick/, we conclude that the probability of Johnny disliking a pie with thick ﬁlling is greater than the probability of the opposite case. It thus makes sense for the classiﬁer to label all examples with filling-size=thick as negative instances of the “pie that Johnny likes.” Note that conditional probability, P.posjthick/, is more trustworthy than the prior probability, P.pos/, because of the additional information that goes into its calculation. This is only natural. In a DayCare center where the number of boys is about the same as that of girls, we expect a randomly selected child to be a boy with P.boy/ D 0:5. But the moment we hear someone call the child Johnny, we increase this expectation, knowing that it is rare for a girl to have this name. This is why P.boyjJohnny/> P.boy/. Joint Probability Conditional probability should not be confused with joint probability of two events occurring simultaneously. Be sure to use the right notation: in joint probability, the terms are separated by commas, P.pos;thick/;i n conditional probability, by a vertical bar, P.posjthick/. For a randomly picked pie, P.pos;thick/ denotes the probability that the example is positive and its ﬁlling is thick; whereas P.posjthick/ refers to the occurrence of a positive example among those that havefilling-size=thick."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00010", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 10, "start_word": 3395, "end_word": 3659, "approx_tokens": 343, "text": "2.2 Vectors of Discrete Attributes 23 Illustrating the principle of Bayesian decision making Let the training examples be described by a single attribute, filling-size, whose value is either thick or thin. We want the machine to recognize the positive class ( pos). Here are the eight available training examples: ex1 ex2 ex3 ex4 ex5 ex6 ex7 ex8 Size thick thick thin thin thin thick thick thick Class pos pos pos pos neg neg neg neg The probabilities of the individual attribute values and class labels are obtained by their relative frequencies. For instance, three out of the eight examples are characterized by filling-size=thin; therefore, P.thin/ D 3=8. P.thin/ D 3=8 P.thick/ D 5=8 P.pos/ D 4=8 P.neg/ D 4=8 The conditional probability of a concrete attribute value within a given class is, again, determined by relative frequency. Our training set yields the following values: P.thinjpos/ D 2=4 P.thickjpos/ D 2=4 P.thinjneg/ D 1=4 P.thickjneg/ D 3=4 Using these values, the Bayes formula gives the following conditional probabilities: P.posjthin/ D 2=3 P.posjthick/ D 2=5 P.negjthin/ D 1=3 P.negjthick/ D 3=5 (note that P.posjthin/ C P.negjthin/ D P.posjthick/ C P.negjthick/ D 1) Based on these results, we conclude that an example with filling-size=thin should be classiﬁed as positive because P.posjthin/> P.negjthin/. Conversely, an example withfilling-size = thick should be classiﬁed as negative because P.negjthick/> P.posjthick/. The denominator being the same for each class, we choose the class that maximizes the numerator,P.xjci/P.ci/. Here, P.ci/ is easy to estimate by the relative frequency of ci in the training set. As for P.xjci/, however, things are not so simple."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00011", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 11, "start_word": 3659, "end_word": 4116, "approx_tokens": 594, "text": "24 2 Probabilities: Bayesian Classiﬁers A Vector’s Probability P.xjci/ is the probability that a randomly selected repre- sentative of class ci is described by vector x. Can its value be estimated by relative frequency? Not really. In the “pies” domain, the size of the instance space was 108 different examples, of which the training set contained twelve. These twelve vectors were each represented by one training example, while none of the other vectors (the vast majority!) was represented at all. The relative frequency of x among the six positive examples was thus either P.xjpos/ D 1=6, when x was among them, or P.xjpos/ D 0, when it was not. Any x identical to a training example “inherits” this example’s class label; if the vector is not found in the training set, we have P.xjc i/ D 0 for any ci. The numerator in the Bayes formula thus being always P.xjci/P.ci/ D 0, we are unable to choose the most probable class. Evidently, we will not get very far calculating the probability of an event that occurs only once or not at all. This, fortunately, is not the case with the individual attributes. For instance, shape=circle occurs four times among the positive examples and twice among the negative, the corresponding probabilities thus being P.shape Dcirclejpos/ D 4=6 and P.shape Dcirclejneg/ D 2=6.I f an attribute can acquire only two or three values, chances are high that each of these values is represented in the training set more than once, thus offering better grounds for probability estimates. Mutually Independent Attributes What is needed is a formula that combines probabilities of individual attribute values into the probability of the given attribute vector in the given class: P.xjc i/. As long as the attributes are independent of each other, this is simple. If P.xijcj/ is the probability that the value of the i-th attribute of an example from classcj is xi, then the probability,P.xjcj/, that a random representative of cj is described by x D .x1; x2;:::; xn/, is calculated as follows: P.xjcj/ D nY iD1 P.xijcj/ (2.5) An object will be labeled with cj if this class maximizes the following version of the Bayes formula’s numerator: P.cj/ /SOH nY iD1 P.xijcj/ (2.6) The Naive Bayes Assumption The reader may complain that the assumption of mutually independent attributes is rarely justiﬁed. Indeed, can the interrelation of diverse variables ever be avoided? An object’s weight grows with its size, the quality of health care may be traced to an individual’s income, an object’s color can be derived from its physical properties. In short, domains where no two attributes are in any way related to each other are rare. No wonder that the above-described approach is known under the unﬂattering name, Naive Bayes."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00012", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 12, "start_word": 4116, "end_word": 4539, "approx_tokens": 549, "text": "28 2 Probabilities: Bayesian Classiﬁers For each successive trial, the second row gives the observed outcome; the third, the relative frequency of heads; the last, the m-estimate of the probability, assuming /EMheads D 0:5 and m D 2 Toss number 1 2 3 4 5 Outcome Heads Heads Tails Heads Tails Relative frequency 1.00 1.00 0.67 0.75 0.60 m-estimate 0.67 0.75 0.60 0.67 0.57 frequency. Thus after two trials, m-estimate suggests a 0.75 chance of heads, whereas anybody espousing relative frequency will have to concede that, based on the two experiments, there is a zero chance that the coin will come up tails. As the number of trials increases, though, the values returned by m-estimate and relative frequency tend to converge. The Impact of the User’s Conﬁdence Let us take a closer look at the effect of m, the user’s conﬁdence. A lot is revealed if we compare the two different settings below: m D 100 on the left and m D 1 on the right (in both cases, /EM heads D 0:5). Nheads C 50 Nall C 100 Nheads C 0:5 Nall C 1 The version with m D 100 allows the prior estimate to be modiﬁed only if really substantial evidence is available (Nheads /GS 50; Nall /GS 100). By contrast, the version with m D 1 allows the user’s opinion to be controverted with just a few experimental trials. Domains with More Than Two Outcomes Although we have used a two-outcome domain, the formula is applicable also in multi-outcome domains. Rolling a fair die can result in six different outcomes, and we expect that the probability of seeing, say, three points is /EM three D 1=6.U s i n gm D 6, we obtain the following: Pthree D Nthree C m/EMthree Nall C m D Nthree C 6 /SOH1 6 Nall C 6 D Nthree C 1 Nall C 6 Again, if Nall is so high thatm D 6 and m/EMthree D 1 can be neglected, the formula converges to relative frequency: Pthree D Nthree Nall . If we do not want this to happen prematurely (perhaps because we have high conﬁdence in the prior estimate,/EMthree), we prevent it by choosing a higher m. The Limits of m-Estimates We should not forget that the m-estimate is only as good as the parameters it relies on. If we start from an unrealistic prior estimate, the result can be disappointing. Suppose that /EMheads D 0:9 and m D 10. then turns into the following: Pheads D Nheads C 9 Nall C 10"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00013", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 13, "start_word": 4539, "end_word": 4923, "approx_tokens": 499, "text": "44 3 Similarities: Nearest-Neighbor Classiﬁers Counting the numbers of differences between pairs of discrete-attribute vectors Crust Filling Example Shape Size Shade Size Shade Class # differences x Square Thick Gray Thin White ? – ex1 Circle Thick Gray Thick Dark pos 3 ex2 Circle Thick White Thick Dark pos 4 ex3 Triangle Thick Dark Thick Gray pos 4 ex4 Circle Thin White Thin Dark pos 4 ex5 Square Thick Dark Thin White pos 1 ex6 Circle Thick White Thin Dark pos 3 ex7 Circle Thick Gray Thick White neg 2 ex8 Square Thick White Thick Gray neg 3 ex9 Triangle Thin Gray Thin Dark neg 3 ex10 Circle Thick Dark Thick White neg 3 ex11 Square Thick White Thick Dark neg 3 ex12 Triangle Thick White Thick Gray neg 4 Of the 12 training examples,ex5 is the one most similar to x The simplest version of the k-NN classiﬁer Suppose we have a mechanism to evaluate the similarly between attribute vectors. Let x denote the object whose class we want to determine. 1. Among the training examples, identify the k nearest neighbors of x (examples most similar to x). 2. Let ci be the class most frequently found among these k nearest neighbors. 3. Label x with ci. Dealing with continuous attributes is just as simple. The fact that each example can be represented by a point in an n-dimensional space makes it possible to calculate the geometric distance between any pair of examples, for instance, by the Euclidean distance (Sect. 3.2 will have more to say about how to measure distance between vectors). And again, the closer to each other the examples are in the instance space, the greater their mutual similarity. This, by the way, is how the nearest-neighbor classiﬁer got its name: the training example with the smallest distance from x in the instance space is, geometrically speaking,x’s nearest neighbor. From a Single Neighbor to k Neighbors In noisy domains, the testimony of the nearest neighbor cannot be trusted. What if this single specimen’s class label is incorrect? A more robust approach identiﬁes not one, but several nearest neighbors, and then lets them vote. This is the essence of the so-called k-NN classiﬁer, where k is the number of the voting neighbors—usually a user-speciﬁed parameter. The pseudocode in summarizes the algorithm."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00014", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 14, "start_word": 4923, "end_word": 5300, "approx_tokens": 490, "text": "3.1 The k-Nearest-Neighbor Rule 45 Note that, in a two-class domain,k should be an odd number so as to prevent ties. For instance, a 4-NN classiﬁer might face a situation where the number of positive neighbors is the same as the number of negative neighbors. This will not happen to a 5-NN classiﬁer. As for domains that have more than two classes, using an odd number of nearest neighbors does not prevent ties. For instance, the 7-NN classiﬁer can realize that three neighbors belong to class C 1, three neighbors belong to class C2, and one neighbor belongs to class C3. The engineer designing the classiﬁer then needs to deﬁne a mechanism to choose between C1 and C2. An Illustration Certain “behavioral aspects” of this paradigm can be made obvious with the help of a ﬁctitious domain where the examples are described by two numeric attributes, a situation easy to visualize. shows several positive and negative training examples, and also some objects (the big black dots) whose classes the k-NN classiﬁer is to determine. The reader can see that objects 1 and 2 are surrounded by examples from the same class, and their classiﬁcation is therefore straightforward. On the other hand, object 3 is located in the “no man’s land” between the positive and negative regions, so that even a small amount of attribute noise can send it to either side. The classiﬁcation of such borderline examples is unreliable. In the right-hand part of the picture, object 4 ﬁnds itself deep in the positive region, but class noise has mislabeled its nearest neighbor in the training set as negative. Whereas the 1-NN classiﬁer will go wrong, here, the 3-NN classiﬁer will give the correct answer because the other two neighbors, which are positive, will outvote the single negative neighbor. + __ _ _ _ _ _ __ + + + + + ++ + x y 1 3 2 + __ _ _ _ _ _ __ + + + + ++ + x y 4_ + Object 3, ﬁnding itself in the borderline region, is hard to classify. In the noisy domain shown in the right-hand part , the 1-NN classiﬁer will misclassify object 4, but the mistake is corrected if the 3-NN classiﬁer is used"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00015", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 15, "start_word": 5300, "end_word": 5600, "approx_tokens": 390, "text": "3.4 Performance Considerations 53 Increasing the Number of Neighbors From the perspective of the 1-NN classiﬁer, the results from are rather discouraging. On the other hand, we know that the classiﬁer’s performance might improve when we use the more generalk-NN (for k >1 ), where some of the noisy nearest neighbors get outvoted by better-behaved ones. Does mathematics lend some support to this expectation? Yes it does. Under the above-mentioned ideal circumstances, the error rate has been proven to decrease with the growing value of k, until it converges to that of Ideal Bayes for k !1 . At least in theory, then, the performance of the nearest- neighbor classiﬁer is able to reach the absolute maximum. Practical Limitations of Theories The engineer’s world is indifferent to theoret- ical requirements. In a realistic application, the training examples will but sparsely populate the instance space, and increasing the number of voting neighbors can be counterproductive. More often than not, the error rate does improve with the growing k, but only up to a certain point from which it starts growing again—as illustrated in where the horizontal axis represents the values of k, and the vertical axis represents the error rate that is measured on an independent testing set. The explanation is simple: the more distant “nearest neighbors” may ﬁnd themselves in regions (in the instance space) that are already occupied by other classes; as such, they only mislead the classiﬁer. Consider the extreme: if the training 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 BAYE SIAN ERROR RATE ERROR RATE OF NEAREST NEIGHBORS The theoretical error rate of the 1-NN rule compared to that of the Ideal Bayes.T w o classes: the solid curve; many classes: the dotted curve"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00016", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 16, "start_word": 5600, "end_word": 5962, "approx_tokens": 470, "text": "58 3 Similarities: Nearest-Neighbor Classiﬁers the mathematician who ﬁrst used them a few decades ago. 4 A pair of examples, x and y, are said to form aTomek Linkif the following three requirements are satisﬁed at the same time: 1. x is the nearest neighbor of y 2. y is the nearest neighbor of x 3. the class of x is not the same as the class of y. These conditions being characteristic of borderline examples, and also of exam- ples surrounded by examples of anotherclass, it makes sense to remove from the Dotted lines connect all Tomek links. Each p a r t i c i p a n ti naT o m e kL i n ki s its partner’s nearest neighbor, and each of the two examples has a different class + __ _ _ _ _ _ __ + + + + + + x y + + training set all such pairs. Even this may not be enough. Sometimes, the removal of existing Tomek Linksonly creates new ones so that the procedure has to be repeated. The algorithm is summarized by the pseudocode in , and a few instances of Tomek Links are shown in . Note that there are only these three; no other pair of examples satisﬁes here the criteria for being called a Tomek Link. One side-effect perhaps deserves to be mentioned: once the training set has been cleaned, the number (k) of the voting nearest neighbors can be reduced because the main reason for using a k >1 is to mitigate the negative impact of noise—which the removal of Tomek Linkshas reduced. It can even happen that the 1-NN classiﬁer will now be able to achieve the performance of ak-NN classiﬁer that uses the entire original training set. A Limitation Nothing is perfect. The technique of Tomek Links does not identify all misleading examples; and, conversely, some of the removed examples might have been “innocent,” and thus deserved to be retained. Still, experience has shown that the removal of Tomek Links usually does improve the overall quality of the data. 4It is fair to mention that he used them for somewhat different purposes."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00017", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 17, "start_word": 5962, "end_word": 6088, "approx_tokens": 163, "text": "64 3 Similarities: Nearest-Neighbor Classiﬁers noise-free data set, create 5 ﬁles, each obtained by changingp percent of the class labels, using p 2f 5; 10; 15; 20; 25g (thus obtaining different levels of class-label noise). Divide each data ﬁle into two parts, the ﬁrst to be reclassiﬁed by the k-NN classiﬁer that uses the second part. Observe how different values of k result in different behaviors under different levels of class-label noise. 4. Implement the Tomek-link method for the removal of harmful examples. Repeat the experiments above for the case where thek-NN classiﬁer uses only examples that survived this removal. Compare the results, observing how the removal affected the classiﬁcation behavior of the k-NN classiﬁer for different values of k and for different levels of noise."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00018", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 18, "start_word": 6088, "end_word": 6501, "approx_tokens": 536, "text": "4.4 Domains with More Than Two Classes 77 Converting a 4-class problem into four 2-class problems attribute vector CCCC12 3 4 A 4-class training set, T, converted to 4 binary training sets, T1 ::: T4 TT 1 T2 T3 T4 e1 C2 e1 0 e1 1 e1 0 e1 0 e2 C1 e2 1 e2 0 e2 0 e2 0 e3 C3 e3 0 e3 0 e3 1 e3 0 e4 C4 e4 0 e4 0 e4 0 e4 1 e5 C2 e5 0 e5 1 e5 0 e5 0 e6 C4 e6 0 e6 0 e6 0 e6 1 illustrates the principle. On the left is the original training set,T, where each example is labeled with one of the four classes. On the right are four “derived” sets, T1 through T4, each consisting of the same six examples which, however, have been re-labeled so that an example that in the original set, T, represents class Ci is labeled with c.x/ D 1 in Ti and with c.x/ D 0 in all other sets. The Need for a Master Classiﬁer The training sets, Ti, are presented to a learner which induces from each of them a linear classiﬁer dedicated to the corresponding class. This is not the end of the story, though. The training examples may poorly represent the classes, they may be corrupted by noise, and even the requirement of linear separability may not be satisﬁed. As a result of these complications, the induced classiﬁers may “overlap” each other in the sense that two or more of them will respond to the same example, x, with h i.x/ D 1, leaving the wrong impression that x belongs to more than one class. This is why a “master classiﬁer” is needed, its task being to choose from the returned classes the one most likely to be correct. This is not difﬁcult. The reader will recall that a linear classiﬁer labels x as positive if the weighted sum ofx’s attribute values exceeds zero:† n iD0wixi >0 .T h i s sum (usually different in each of the classiﬁers that have returned hi.x/ D 1) can be interpreted as the amount of evidence in support of the corresponding class. The master classiﬁer then simply gives preference to the class whose binary classiﬁer delivered the highest † n iD0wixi. A Numeric Example The principle is illustrated in . Here, each of the rows represents a different class (with the total of four classes). Of course, each"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00019", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 19, "start_word": 6501, "end_word": 6886, "approx_tokens": 500, "text": "4.5 Polynomial Classiﬁers 79 4.5 Polynomial Classiﬁers It is now time to abandon the requirement that the positive examples be linearly separable from the negative; because fairly often, they are not. Not only can the linear separability be destroyed by noise. The very shape of the region occupied by one of the classes can make it impossible to separate it by a linear decision surface. Thus in the training set shown in , no linear classiﬁer ever succeeds in separating the two classes, a feat that can only be accomplished by a non-linear curve such as the parabola shown in the picture. Non-linear Classiﬁers The point having been made, we have to ask how to induce the more sophisticated non-linear classiﬁers. There is no doubt that they exist. For instance, math teaches us that any n-dimensional function can be approximated to arbitrary precision with a polynomial of a sufﬁciently high order. Let us therefore take a look at how to use—and induce—the polynomials for our classiﬁcation purposes. Polynomials of the Second Order The good news is that the coefﬁcients of polynomials can be induced by the same techniques that we have used for linear classiﬁers. Let us explain how. For the sake of clarity, we will begin by constraining ourselves to simple domains with only two boolean attributes, x 1 and x2. The second-order polynomial function is then deﬁned as follows: w0 C w1x1 C w2x2 C w3x2 1 C w4x1x2 C w5x2 2 D 0 (4.12) The expression on the left is a sum of terms that all have one thing in common: a weight, wi, that multiplies a product xk 1xl 2. In the ﬁrst term, we have k C l D 0, because w0x0 1x0 2 D w0; next come the terms with k C l D 1, concretely, w1x1 1x0 2 D w1x1 and w2x0 1x1 2 D w1x2; and the sequence ends with the three terms that have k C l D 2: speciﬁcally, w3x2 1, w4x1 1x1 2, and w5x2 2. The thing to remember is that the expansion of the second-order polynomial stops when the sum of the exponents reaches 2. In some domains, no linear classiﬁer can separate the positive examples from the negative. Only a non-linear classiﬁer can do so 2x x1 − − + +"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00020", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 20, "start_word": 6886, "end_word": 7101, "approx_tokens": 279, "text": "4.6 Speciﬁc Aspects of Polynomial Classiﬁers 81 The second-order polynomial function over three attributes is deﬁned by the follow- ing function: 0= w0 + w1x1 + w2x2 + w3x3 + w4x2 1 + w5x1x2 + w6x1x3 +w7x2 2 + w8x2x3 + w9x2 3 Using the multipliers, we obtain the following: 0= w0 + w1z1 + w2z2 + w3z3 + w4z4 + w5z5 + w6z6 + w7z7 + w8z8 + w9z9 Below is the schema of the whole “device” with multipliers. Before reaching the linear classiﬁer, each signal zi is multiplied by the corresponding weight, wi. 1 z z z z z z z z z 4 5 6 7 8 9 linear classifier h x x x 1 1 1 1 2 x3 x x x x x x x x x x x 2 1 3 2 2 2 3 3 3 1 2 3 A polynomial classiﬁer can be converted into a linear classiﬁer with the help of multipliers that pre-process the data 4.6 Speciﬁc Aspects of Polynomial Classiﬁers To be able to use a machine-learning technique with success, the engineer must understand not only its strengths, but also its limitations, shortcomings, and pitfalls. In the case of polynomial classiﬁers, there are a few that deserve our attention. Let us brieﬂy discuss them."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00021", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 21, "start_word": 7101, "end_word": 7597, "approx_tokens": 644, "text": "Boundaries: Linear and Polynomial Classiﬁers The two classes are linearly separable, but noise has caused one negative example to be mislabeled as positive. The high-order polynomial on the right overﬁts the data, ignoring the possibility of noise x x 1 2 x1 x2 Overﬁtting Polynomial classiﬁers tend to overﬁt noisy training data. Since the problem of overﬁtting is encountered also in other machine-learning paradigms, we have to discuss its essence in some detail. For the sake of clarity, we will abandon the requirement that all attributes should be boolean; instead, we will rely on two- dimensional continuous domains that are easy to visualize. The eight training examples in fall into two groups. In one of them, all examples are positive; in the other, all save one are negative. Two attempts at separating the two classes are made. The one on the left is content with a linear classiﬁer, shrugging off the minor inconvenience that one training example remains misclassiﬁed. The one on the right resorts to a polynomial classiﬁer in an attempt to avoid any error being make on the training set. An Inevitable Trade-Off Which of the two is to be preferred? This is not an easy question. On the one hand, the two classes may be linearly separable, and the only cause for one positive example to be found in the negative region is class-label noise. If this is the case, the single error made by the linear classiﬁer on the training set is inconsequential, whereas the polynomial, cutting deep into the negative area, will misclassify examples that ﬁnd themselves on the wrong side of the curve. On the other hand, there is also the chance that the outlier does represent some legitimate, even if rare, aspect of the positive class. In this event, using the polynomial will be justiﬁed. On the whole, however, the assumption that the single outlier is nothing but noise is more likely to be correct than the “special-aspect” alternative. A realistic training set will contain not one, but quite a few, perhaps many examples that seem to ﬁnd themselves in the wrong part of the instance space. And the inter-class boundary that our classiﬁer attempts to model may indeed be curved, though how much curved is anybody’s guess. The engineer may lay aside the linear classiﬁer as too crude an approximation, and opt instead for the greater ﬂexibility offered by the polynomials. This said, a very-high-order polynomial will avoid any error even in a very noisy training set—and then fail miserably on future data. The ideal solution is often somewhere between the extremes of linear classiﬁers and high-order polynomials. The best choice can be determined experimentally. The Number of Weights The total number of the weights to be trained depends on the length of the attribute vector, and on the order of the polynomial. A simple analysis reveals that, in the case of n attributes and the r-th order polynomial, the number is determined by the following combinatorial expression:"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00022", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 22, "start_word": 7597, "end_word": 7815, "approx_tokens": 283, "text": "98 5 Artiﬁcial Neural Networks For a given example, each set of weights implies a certain mean square error. Training should reduce this error as quickly as possible In the technique discussed here, we want weight changes that will bring about the steepest descent along the error function. Recalling the terminology from , this is a job for hill-climbing search. The best-known technique used to this end in multilayer perceptrons is backpropagation of error. Backpropagation of Error The speciﬁc weight-adjusting formulas can be derived from by ﬁnding the function’s gradient. However, as this book is meant for practitioners, and not for mathematicians, we will skip the derivation, and focus instead on explaining the learning procedure’s behavior. To begin with, it is reasonable to assume that the individual neurons differ in their contributions to the overall error. Some of them “spoil the game” more than the others. If this is the case, the reader will agree that the links leading to these neurons should undergo greater weight changes than the links leading to less offending neurons. Fortunately, each neuron’s amount of “responsibility” for the overall error can be easily obtained. Generally speaking, the concrete choice of formulas depends on what transfer function has been used. In the case of the sigmoid (see ), the responsibility is calculated as follows:"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00023", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 23, "start_word": 7815, "end_word": 8269, "approx_tokens": 590, "text": "5.5 Architectural Issues 105 The error rate measured on testing examples depends on the number of neurons in the hidden layer number of hidden neurons error rate on testing data 0.4 0.3 0.2 0.1 with a different number of hidden neurons. The networks are trained until no reduction of the training-set error rate is observed. After this, the error rate on the testing data is measured. Optimum Number of Neurons When plotted in a graph, the results will typically look something like the situation depicted in . Here, the horizontal axis represents the number of hidden neurons; the vertical axis, the error rate measured on the testing set. Typically, the error rate will be high in the case of very small networks because these lack adequate ﬂexibility, and also suffer from the danger of getting stuck in local minima. These two weaknesses can be mitigated if we increase the number of hidden neurons. As shown in the graph, the larger networks then exhibit lower error rates. But then, networks that are too large are vulnerable to overﬁtting. This is why, after a certain point, the testing-set error starts growing again (the right tail of the graph). The precise shape of the curve depends on the complexity of the training data. Sometimes, the error rate is minimized when the network contains no more than 3–5 hidden neurons. In other domains, however, the minimum is reached only when hundreds of hidden neurons are used. Yet another case worth mentioning is the situation where the training examples are completely noise-free. In a domain of this kind, overﬁtting may never become an issue, and the curve’s right tail may not grow at all. Search for Appropriate Size The scenario described above is too expensive to be employed in practical applications. After all, we have no idea whether we will need just a few neurons, or dozens of them, or hundreds, and we may have to re-run the computationally intensive training algorithm a great many times before being able to establish the ideal size. Instead, we would like to have at our disposal a technique capable of ﬁnding the appropriate size more efﬁciently. One such technique is summarized by the pseudocode in . The idea is to start with a very small network that only has a few hidden neurons. After each epoch, the learning algorithm checks the sum of the mean square errors observed on the training set. This sum of errors is likely to keep decreasing with the growing number of epochs—but only up to a certain point. When this is reached, the network’s performance no longer improves, either because of its insufﬁcient ﬂexibility that makes correct classiﬁcation impossible, or because it “fell” into a local minimum."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00024", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 24, "start_word": 8269, "end_word": 8743, "approx_tokens": 616, "text": "6.1 Eight training examples described by three symbolic attributes and classiﬁed as positive and negative examples of a given class crust filling Example size shape size Class e1 big circle small pos e2 small circle small pos e3 big square small neg e4 big triangle small neg e5 big square big pos e6 small square small neg e7 small square big pos e8 big circle big pos tests, the edges indicate how to proceed in the case of diverse test results, and the leafs1 contain class labels. An example to be classiﬁed is ﬁrst subjected to the test prescribed at the topmost node, the root. The result of this test then decides along which edge the example is to be sent down, and the process continues until a leaf node is reached. Once this happens, the example is labeled with the class associated with this leaf. Let us illustrate the process using the tree from b. The root asks about the shape, each of whose three values is represented by one edge. In examples e1; e2, and e8, we ﬁnd the value shape=circle which corresponds to the left edge. The example is sent down along this edge, ending in a leaf that labeled pos.T h i s indeed is the class common to all these three examples. Ine4,shape=triangle, and the corresponding edge ends in a leaf labeled neg—again, the correct class. Somewhat more complicated is the situation with examplese3; e5; e6, and e7 where shape=square. For this particular value, the edge ends not in a leaf, but only at a test-containing node, this one inquiring about the value offilling-size.I nt h e case of e5 and e7,t h ev a l u ei sbig, which leads to a leaf labeled with pos.I nt h e other two examples, e3 and e6,t h ev a l u ei ssmall, and this sends them to a leaf labeled withneg. We have shown that the decision tree from b identiﬁes the correct class for all training examples. By way of a little exercise, the reader is encouraged to verify that the other trees shown in the picture are just as successful. 2 Interpretability Comparing this classiﬁer with those introduced earlier, we can see one striking advantage: interpretability. If anybody asks why example e1 is deemed positive, the answer is, “because its shape is circle.” Other classiﬁers do not offer explanations of this kind. Especially the neural network is a real black box: when presented with an example, it simply returns the class and never offers any 1Both spellings are used: leaves and leafs. The latter is probably more appropriate because the “leaf” in question is supposed to be a data abstraction that has nothing to do with the original physical object. 2In as sense, the decision tree can be seen as a simple mechanism for data compression."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00025", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 25, "start_word": 8743, "end_word": 8983, "approx_tokens": 312, "text": "6.1 Decision Trees as Classiﬁers 115 + + filling size shape big small − tri sq circle c (C) −+ sqc shape −+ c tri sq shape small big crust size small + big fillingsize (D) shape filling size − − + + circle sq smallbig triangle (B) big crustsize small fillingsize big + small shape tri shape circle + filling size big −+ sqcircle +− small (A) sq Example decision trees for the “pies” domain. Note how they differ in size and in the order of tests. Each of them classiﬁes correctly all training examples listed in ,tri,sq, andc stand fortriangle,square,a n dcircle, respectively insight as to why this particular label has been given preference over other labels. The situation is not much better in the case of Bayesian and linear classiﬁers. Only the k-NN classiﬁer offers a semblance of a—rather rudimentary—argument. For instance, one can say that, “ x should be labeled with pos because this is the class of the training example most similar to x.” Such a statement, however, is a far cry from the explicit attribute-based explanation made possible by the decision tree. One can go one step further and interpret a decision tree as a set of rules such as “if shape=square and filling-size=big, then the example belongs to class pos.” A domain expert inspecting these rules may then decide whether they are intuitively appealing, and whether they agree with his or her “human"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00026", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 26, "start_word": 8983, "end_word": 9443, "approx_tokens": 598, "text": "consists of two attribute tests; the largest, of ﬁve. Differences of this kind may have a strong impact on the classiﬁer’s behavior. Before proceeding to the various aspects of this phenomenon, however, let us emphasize that the number of nodes in the tree is not the only criterion of size. Just as important is the number of tests that have to be carried out when classifying an average example. For instance, in a domain where shape is almost always circle or triangle (and only very rarelysquare), the average number of tests prescribed by the tree from b will only slightly exceed 1 because bothshape=circle and shape=triangle immediately point at leafs with class labels. But if the prevailingshape issquare, the average number tests approaches 2. Quite often, then, a bigger tree may result in fewer tests than a smaller one. Small Trees Versus Big Trees There are several reasons why small decision trees are preferred. One of them is interpretability. A human expert ﬁnds it easy to analyze, explain, and perhaps even correct, a decision tree that consists of no more than a few tests. The larger the tree, the more difﬁcult this is. Another advantage of small decision trees is their tendency to dispose of irrelevant and redundant information. Whereas the relatively large tree from a employs all three attributes, the smaller one from bi sj u s ta s good at classifying the training set—without ever consideringcrust-size. Such economy will come handy in domains where certain attribute values are expensive or time-consuming to obtain. Finally, larger trees are prone tooverﬁt the training examples. This is because the divide-and-conquer method keeps splitting the training set into smaller and smaller subsets, the number of these splits being equal to the number of attribute tests in the tree. Ultimately, the resulting training subsets can become so small that the classes may get separated by an attribute that only by chance—or noise—has a different value in the remaining positive and negative examples. Induction of Small Decision Trees When illustrating the behavior of the divide- and-conquer technique on the manual tree-building procedure, we picked the attributes at random. When doing so, we observed that some choices led to smaller trees than others. Apparently, the attributes differ in how much information they convey. For instance, shape is capable of immediately labeling some examples as positive (if the value is circle) or negative (if the value is triangle); but crust-size cannot do so unless assisted by some other attribute. Assuming that there is a way to measure the amount of information provided by each attribute (and such a mechanism indeed exists, see Sect. 6.3), we are ready to formalize the technique for induction of decision trees by a pseudocode. The reader will ﬁnd it in ."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00027", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 27, "start_word": 9443, "end_word": 9939, "approx_tokens": 644, "text": "6.5 Pruning 127 Error Estimate Pruning is typically carried out in a sequence of steps: ﬁrst replace with a leaf one subtree, then another, and so on, as long as the replacements appear to be beneﬁcial according to some reasonable criterion. The term “beneﬁcial” is meant to warn us that small-tree advantages should not be outbalanced by reduced classiﬁcation performance. Which brings us to the issue of error estimate. The principle is illustrated in .L e tm be the number of training examples that reach test t 3 in the decision tree on the left. If we replace the subtree rooted in t3 by a leaf (as happened in the tree on the right), some of these m examples may become misclassiﬁed. Denoting the number of these misclassiﬁed examples by e, we may be tempted to estimate the probability that an example will be misclassiﬁed at this leaf by the relative frequency: e=m. But admitting that small values of m may render this estimate problematic, we prefer the following formula whereN is the total number of training examples: Eestimate D e C 1 N C m (6.6) The attentive reader may want to recall (or re-read) what Sect. 2.3 had to say about the difﬁculties of probability estimates of rare events. Error Estimates for the Whole Tree Once again, let us return to . The tree on the left has two subtrees, one rooted at t2, the other at t5.L e tm2 and m5 be the numbers of the training examples reaching t2 and t5, respectively; and let E2 and E5 be the error estimates of the two subtrees, obtained by . For the total of N D m2 C m5 training examples, the error rate of the whole subtree is estimated as the weighted average of the two subtrees: ER D m2 N E2 C m5 N E5 (6.7) Of course, in a situation with more than just two subtrees, the weighted average has to be taken over all of them. This should present no major difﬁculties. As for the values of E2 and E5, these are obtained from the error rates of the speciﬁc subtrees, and these again from the error rates of their sub-subtrees, and so on, all the way down to the lowest level tests. The error-estimating procedure is a recursive undertaking. Suppose that the tree to be pruned is the one rooted att 3, which happens to be one of the two children of test t2. The error estimate for t2 is calculated as the weighted average of E3 and the error estimate for the other child of t2 (the leaf labeled with the positive class). The resulting estimate would then be combined withE5 as shown above. Post-pruning The term refers to the circumstance that the decision tree is pruned after it has been fully induced from data (an alternative is the subject of the next subsection). We already know that the essence is to replace a subtree with a leaf"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00028", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 28, "start_word": 9939, "end_word": 10288, "approx_tokens": 453, "text": "6.6 Converting the Decision Tree into Rules 131 The algorithm for rule pruning Re-write the decision tree as a set of rules. Let c be a user-set constant controlling the extent of pruning (1) In each rule, calculate the increase in error estimate brought about by the removal of individual tests. (2) Choose those removals where this increase, Dmin is smallest. Remove the tests, but only if Dmin < c. (3) In the set of rules, search for the weakest rules to be removed. (4) Choose the default class. (5) Order the rules according to their strengths (how many training examples they cover). Illustration of the algorithm for rule pruning Suppose that the decision from the left part of has been converted into the following set of rules (neg is the default label to be used when the if-parts of all rules are false). t1 ^ t2 ! pos t1 ^: t2 ^ t3 ^ t4 ! pos :t1 ^ t5 ^ t6 ! pos else neg Suppose that the evaluation of the tests in the rules has resulted in the conclusion that t3 in the second rule and t5 in the third rule can be removed without a major increase in the error estimate. We obtain the following set of modiﬁed rules. t1 ^ t2 ! pos t1 ^: t2 ^ t4 ! pos :t1 ^ t6 ! pos else neg The next step can reveal that the second (already modiﬁed) rule can be removed without a major increase in the error estimate. After the removal, the set of rules will look as follows. t1 ^ t2 ! pos :t1 ^ t6 ! pos else neg This completes the pruning. rules can be deleted. This is done by the rule-pruning algorithm summarized by the pseudocode in and illustrated by the example in . Here, the initial set of rules was obtained from the (now familiar) tree in the left part of . The ﬁrst pruning step removes those tests that do not appear to contribute much to the overall classiﬁcation performance; the next step deletes the weakest rules."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00029", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 29, "start_word": 10288, "end_word": 10696, "approx_tokens": 530, "text": "2. Take the decision tree from a and remove from it the bottom-right test onfilling-size. Based on the training set from , what will be the error estimate before and after this “pruning”? 3. Choose one of the decision trees in and convert it to a set of rules. Pick one of these rules and decide which of its tests can be removed with the minimum increase in the estimated error. 4. Consider a set of ten training examples. Suppose there is a continuous attribute that has the following values: 3:6; 3:2; 1:2; 4:0; 0:8; 1:2; 2:8; 2:4; 2; 2; 1:0. Sup- pose that the ﬁrst ﬁve of these examples, and also the last one, are positive, all other examples being negative. What will be the best binary split of the range of this attribute’s values? Give It Some Thought 1. The baseline performance criteria used for the evaluation of decision trees are error rate and the size of the tree (the number of nodes). These, however, may not be appropriate in certain domains. Suggest applications where either the size of the decision tree or its error rate may be less important. Hint: Consider the costs of erroneous decisions and the costs of obtaining attribute values. 2. What are likely to be the characteristics of a domain where a decision tree clearly outperforms the baseline 1-NN classiﬁer? Hint: Consider such characteristics as noise, irrelevant attributes, or the size of the training set; and then make your own judgement as to what inﬂuence each of them is likely to have on the classiﬁer’s behavior. 3. On what kind of data may a linear classiﬁer do better than a decision tree? Give at least two features characterizing such data. Rely on the same hint as the previous question. 4. Having found the answers to the previous two questions, you should be able to draw the logical conclusion: applying to the given data both decision-tree induction and linear-classiﬁer induction, what will their performance betray about the characteristics of the data? 5. The decision tree as described in this chapter gives only “crisp” yes-or-no decisions about the given example’s class (in this sense, one can argue that Bayesian classiﬁers or multilayer perceptrons are more ﬂexible). By way of mitigating this weakness, suggest a mechanism that would modify the decision- trees framework so as to give, for each example, not only the class label, but also the classiﬁer’s conﬁdence in this class label."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00030", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 30, "start_word": 10696, "end_word": 11151, "approx_tokens": 591, "text": "7.2 Examples of PAC Learnability 141 7.2 Examples of PAC Learnability Inequality (7.5) tells us how learnability, deﬁned by the (/SI;ı)-requirements, depends on the size of the hypothesis space. Let us illustrate this result on two concrete types of classiﬁers. Conjunctions of Boolean Attributes: Hypothesis Space Suppose that all attributes are boolean, that all data are noise-free, and that an example’s class is known to be determined by a logical conjunction of attribute values: if true, then the example is positive, otherwise it is deemed negative. For instance, the labels of the training examples may be determined by the following expression: att1 = true AND att3 = false This means that an example is labeled as pos if the value of the ﬁrst attribute is true and the value of the third attribute is false, regardless of the value of any other attribute. An example that fails to satisfy these two conditions is labeled asneg. The task for the machine-learning program is to ﬁnd a conjunction that correctly labels all training examples. The set of all conjunctions permitted by our informal deﬁnition forms the hypothesis space, jHj. What is the size of this space? In a logical conjunction of the kind speciﬁed above, each attribute is either true or false or irrelevant. This gives us three possibilities for the ﬁrst attribute, times three possibilities for the second, and so on, times three possibilities for the last, n-th, attribute. The size of the hypothesis space is therefore jHjD 3n. Conjunctions of Boolean Attributes: PAC-Learnability Suppose that a training set consisting of noise-free examples is presented to some machine-learning pro- gram that is capable of inducing classiﬁers of the just-deﬁned form. 3 To satisfy the last of the assumptions listed at the beginning of Sect. 7.1, we will assume that at least one of the logical conjunctions classiﬁes correctly all training examples. Since ln jHjD ln 3n D n ln 3, we can re-write Inequality ( 7.5) in the following form: m > 1 /SI.n ln 3 C ln 1 ı / (7.6) With this, we have obtained a conservative lower bound on the number of training examples that are needed if our (/SI;ı)-requirements are to be satisﬁed: with probability ı , the induced classiﬁer (error-free on the training set) will exhibit error rate less than /SIon the entire instance space. Note that the value of this expression grows linearly in the number of attributes, n. Theoretically speaking, then, if n is doubled, then twice as many training examples will be needed if, with probability limited by ı , classiﬁers with error rates above the predeﬁned/SIare to be weeded out. 3For instance, a variation of the hill-climbing search from Sect. 1.2 might be used to this end."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00031", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 31, "start_word": 11151, "end_word": 11650, "approx_tokens": 648, "text": "Function: Hypothesis Space Let us now investigate a broader class of classiﬁers, namely those deﬁned by any boolean function, allowing for all three basic logical operators (AND, OR, and NOT) as well as any combination of parentheses. As before, we will assume that the examples are described byn boolean attributes, and that they are noise-free. What is the size of this hypothesis space? From n boolean attributes, 2 n different examples can be created. This deﬁnes the size of the instance space. For any subset of these 2n examples, there exists at least one logical function that is true for all examples from this subset (labeling them as pos) and false for all examples from outside this subset (labeling them as neg). Two logical functions will be regarded as identical from the classiﬁcation point of view if each of them labels any example with the same class; that is, if they never differ in their “opinion” about any example’s class. The number of logical functions that are mutually distinct in their classiﬁcation behavior then has to be the same as the number of the subsets of the instance space. A set consisting of X elements is known to have 2 X subsets. Since our speciﬁc instance space consists of 2n examples, the number of its subsets is 22n —and this is the size of our hypothesis space: jHjD 22n (7.7) Any Boolean Function: PAC-Learnability Since ln jHjD ln 22n D 2n ln 2, Inequality (7.5) acquires the following form: m > 1 /SI.2n ln 2 C ln 1 ı / (7.8) We conclude that the lower bound on the number of the training examples that are needed if the (/SI, ı )-requirements are to be satisﬁed grows here exponentially in the number of attributes, n. Such growth is prohibitive for any realistic value of n. For instance, even if we add only a single attribute, n C 1,t h ev a l u eo fl njHj will double because ln jHjD 2nC1 ln 2 which is twice a much as 2n ln 2. And if we add ten attributes, n C 10, then the value of ln jHj increases a thousand times because ln jHjD 2nC10 ln 2 D 210 /SOH2n ln 2 D 1024 /SOH2n /SOHln 2. This observation is enough to convince us that a classiﬁer in this general form is not PAC-learnable. A Word of Caution We must be careful not to jump to conclusions. As pointed out earlier, the derivation of Inequality ( 7.5)—a worst-case analysis of sorts— relied on quite a few simplifying assumptions that render the obtained bounds very conservative. In reality, the number of the training examples needed for the induction of a reliable classiﬁer is much lower than that indicated by our “magic formula.” For the engineer seeking to choose the most appropriate learning technique, the main contribution of Inequality (7.5) is that it helps him compare PAC-learnability of classiﬁers constrained by different “languages.” For instance, we have seen that a"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00032", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 32, "start_word": 11650, "end_word": 12128, "approx_tokens": 621, "text": "ﬁnd a classiﬁer that labels correctly the entire training set, and yet exhibits poor performance on future examples. Put bluntly, the induced classiﬁer cannot be trusted. It is inthis sense that we say, with a grain of salt, that “there is no learning without ab i a s . ” The thing to remember is that the machine-learning adventure can only succeed if the engineer constrains the hypothesis space by a meaningful bias. It stands to reason, however, that this bias should not be misleading. The reader will recall that our analysis assumed that the hypothesis space does contain the solution, and that the examples are noise-free. 4 Occam’s Razor Quite often, the engineer has an opportunity to choose from two or more biases. For instance, the class to be learned is described by a conjunction of attribute values, in which each single attribute plays a part. A weaker bias that permits the absence of some attributes from the conjunctions also includes the case where zero attributes are absent, and is therefore correct, too. In the former case, we have lnjHjD n ln 2; and in the latter, lnjHjD n ln 3, which is bigger. We thus have two correct biases, each deﬁning a hypothesis space of a different size. Which of them to prefer? The attentive reader no doubt already knows the answer. In Inequality ( 7.5), the number of training examples needed for successful learning depends on ln jHj.A lower value of this term indicates that fewer examples are needed if we want to satisfy the given ./SI; ı)-requirements. The engineer will thus beneﬁt by choosing the bias whose hypothesis space is smaller. By the way, scientists have been using a similar rule for centuries: in a situation where two different hypotheses can explain a certain phenomenon, it is assumed that the simpler one has a higher chance of success. The fact that this principle,Occam’s Razor, has been named after a scholastic theologian who died in the fourteenth century indicates that the use of this rule pre-dates modern science. The word razor is here to emphasize that, when formulating a hypothesis, we better slice away all unnecessary information. That mathematicians have now been able to ﬁnd a formal proof of the validity of this principle in the ﬁeld of machine learning, and even to quantify the scope of its utility, is a remarkable achievement. Irrelevant and Redundant Attributes In the types of classes investigated in Sect. 7.2, the lower bound on the number of examples, m, depended on the number of attributes, n. For instance, in the case of conjunctions of attribute values, we now know that ln jHjD n ln 3; and the number of examples needed to satisfy given ./SI; ı/-requirements thus grows linearly in n. 4Analysis of situations where these requirements are not satisﬁed would go beyond the scope of an introductory textbook."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00033", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 33, "start_word": 12128, "end_word": 12430, "approx_tokens": 392, "text": "8.2 Oil-Spill Recognition 155 • Why should the classiﬁer be allowed to refuse to classify certain examples? Discuss the trade-off between error rate and rejection rate; comment on the interplay between performance and utility. 8.2 Oil-Spill Recognition shows a radar image of the sea surface as taken by a satellite-born device. Against the grayish background, the reader can see several dark regions of the most varied characteristics: small or large, sharp or barely discernible, and of all possible shapes. What interests us here primarily is the sharp and elongated object in the vicinity of the upper-right corner: an oil spill, illegally dumped by a tanker’s captain who has chosen to get rid of the residues in its bilges in the open sea rather than doing so in a specially designed terminal as required by the law. As such, this particular oil spoil is of great interest to the Coast Guard. For all relevant sea-surface areas (close to major ports, for example), satellites take many of such “snapshots” and send them down to collaborating ground stations. Here, experts pore over these images in search for signs of illegal oil spills. Whenever they detect one, an airplane is dispatched and veriﬁes the suspicion by a spectrometer (which is unavailable to the satellite), collects evidence, and perhaps even identiﬁes the perpetrator. Unfortunately, human experts are expensive—and not always available. They may be on holidays, on a sick leave, or not present for any number of other reasons. Besides, in view of the high number of images, the work is tedious, and prone to human errors. This is why the idea came up to develop a computer program to automate the oil-spill recognition process. A radar image of a sea surface. The “wiggly” elongated dark region in the upper-right corner represents environmental hazard: an oil spill"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00034", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 34, "start_word": 12430, "end_word": 12942, "approx_tokens": 665, "text": "has been selected out of many, the main criterion being its rare clarity. Indeed, the oil spill it contains is so different from the other dark regions that even an untrained eye will easily recognize it as such. Even so, the reader will ﬁnd it difﬁcult to specify the oil-spill’s distinguishing features in a manner that can be used in a computer program. In the case of more realistic objects, the task will be even more difﬁcult. At any rate, to hard-code the oil-spill recognition ability is quite a challenge. Again, machine learning got its chance, the intention being to let the machine develop the requisite skills automatically, by induction from training examples. The general scenario of the adventure can be summarized into the following steps. 1. collect a set of radar images containing oil spills; 2. use some image-processing software capable of identifying, in these images, the dark regions of interest; 3. ask an expert to label the oil spills as positive examples, and the other dark regions (so-called “look-alikes”) as negative examples; 4. describe all examples by attribute vectors, and let a machine-learning program induce the requisite classiﬁer from the training set thus obtained. As in the previous application, we will try to glean some useful lessons by taking a closer look at certain critical aspects of this undertaking. Attributes and Class Labels State-of-the-art image-processing techniques easily discover dark regions in an image. For these to be used by the machine-learning tool, we need to describe them by attributes that are hoped to capture those aspects that distinguish spills from “look-alikes.” Preferably, their values should be unaffected by the given object’s size and orientation. The attributes that have been used in this project include the region’s mean intensity, average edge-gradient (which quantiﬁes the sharpness of the edges), the ratio between the lengths of the object’s minor-axis and major-axis, variance of the background intensity, variance of the edge gradient, and so on. All in all, more than forty such attributes were selected in a rather ad hoc manner. Which of them would really be useful was hard to tell because experts were unable to reach consensus about the attributes’ respective relevance and redundancy. The ﬁnal choice was left to the machine-learning software. Labeling the training examples with classes was not any easier. The objects in were easy to categorize; in other images, they were much more ambiguous. On many occasions, the best the expert could say was, “yes, this looks like an oil spill” or, “I rather doubt this is what we are looking for.” The correctness of the selected class labels was thus uncertain, resulting in class-label noise. The presence of class-label noise reduces our expectations: the classiﬁer can be only as good as the data that have been used during its induction. Choosing the Classiﬁer The examples were described by some forty attributes. Among these, some, perhaps most, were suspected of being either irrelevant or redundant. This is an important circumstance; the reader will recall that the presence of irrelevant and redundant attributes makes some classiﬁers underperform. In this"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00035", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 35, "start_word": 12942, "end_word": 13336, "approx_tokens": 512, "text": "8.3 Sleep Classiﬁcation 159 hours of sleep2468 stage 3/4 stage 2 stage 1 REM move wake An example hypnogram that records the sleep stages experienced by a subject during an 8-h sleep Why Is It Important Medical practice needs to be able to recognize speciﬁc sleep stages. A case in point is the so-called sudden infant death syndrome (SIDS): an infant dies without any apparent cause. A newborn suspected of being in danger has to be watched, in a hospital, so that resuscitation can be started immediately. Fortunately, SIDS is known almost always to occur during the REM stage. This means that it is not necessary to watch the patient all the time, but only during this period of increased risk. For instance, a device capable of recognizing the onset of the REM stage might alert the nurse that more attention might be needed during the next ﬁve of so minutes. The hypnogram, in turn, is a useful diagnostic tool because the distribution of the sleep stages during the night may indicate speciﬁc neural disorders such as epilepsy. Why Machine Learning To draw the hypnogram manually is a slow and tedious undertaking, easily taking 3–5 h of a highly qualiﬁed expert’s time. Moreover, the expert is not always available. This is why efforts have been made to develop a computer program capable of identifying the individual sleep stages based on observed data, and, hopefully, even to draw the hypnogram. To be able to help, the computer needs a description of the individual stages. Such description, however, is difﬁcult to obtain. Medical experts rely on skills obtained in the course of long training, and they use features and indications that are too subjective to be converted to a computer program. This motivated an attempt to induce the classiﬁer from pre-classiﬁed data. Speciﬁcally, the data-acquisition process divided the 8-h sleep period into 30-s samples, each of them treated as a separate training example. All in all, a few hours’ sleep thus provided hundreds of training examples. Diverse instruments than provide data to describe each 30-s sample. Attributes and Classes Again, the ﬁrst task was to remove attributes suspected of being irrelevant or redundant. In the previous application, oil-spill recognition, this removal was motivated by the intention to increase the performance of the k-NN classiﬁer (which is known to be sensitive to their presence). In sleep classiﬁcation,"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00036", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 36, "start_word": 13336, "end_word": 13853, "approx_tokens": 672, "text": "fore: the physician wants to minimize the number of measurement devices attached to the sleeping subject. Not only does their presence make the subject feel uncomfortable; they also disturb the sleep, and thus interfere with the results of the sleep analysis. As for the class labels, these are even less reliable than in the oil-spills domain. The differences between “neighboring” (similar) sleep stages are so poorly deﬁned that any two experts rarely agree on more than 70–80% of the class labels. No wonder that the training set contains a lot of class-label noise, and the low quality of the data imposes a limit on the minimum error rate that any realistic classiﬁer induced from the data can achieve. The Classiﬁer and Its Performance The classiﬁer employed in this particular case combined decision trees with a neural network in a manner whose details are unimportant of our needs here. Sufﬁce it to say that the classiﬁer’s accuracy on independent data indeed achieved those 70–80% observed in human experts, which means that the natural performance limits have been reached. It is perhaps worth noting that plain decision trees were a few percent weaker than that. This said, it is important to understand that classiﬁcation accuracy does not give the full picture of the classiﬁer’s behavior (similarly as in the OCR domain from Sect. 8.1). For one thing, the classiﬁer correctly recognized some of the seven classes most of the time, while failing on others. Particularly disappointing was its treatment of the REM state. Here, classiﬁcation accuracy was in the range 90–95%, which, at ﬁrst sight, looked good enough. However, closer inspection of the training data revealed that only less than 10% of all examples belonged to the REM class; this means that a comparable classiﬁcation accuracy could be achieved by a classiﬁer that says, “there is not a single REM example”—and yet this is hardly what the engineers hoped to achieve. We realize that this way of measuring performance is not without its limitations, and that other criteria have to be found. These indeed exist. They will be discussed in . Improving Classiﬁcation Performance by Post-processing The accuracy of the hypnogram can be improved by post-processing whose nature relies on the domain’s logic. Indeed, several rules of thumb can be used here. For instance, the deepest sleep (stage 3/4) is unlikely to occur right after the REM stage, and stage 2 does not happen after move. Also, the hypnogram can be “smoothed out” by the removal of any stage lasting only one 30-s period. Applying such rules in the course of post-processing makes it possible to improve the hypnogram’s informational value. The lesson is worth remembering. In domains where the examples are ordered in time, the classes of the individual examples may not be independent of those preceding or following them. In this event, post-processing can improve the classiﬁer’s performance. Proper Use of the Induced Classiﬁer The original idea was to induce the classiﬁer from examples obtained from a few subjects, and then to classify future data using this induced classiﬁer instead of the much more expensive “manual” classiﬁcation."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00037", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 37, "start_word": 13853, "end_word": 14278, "approx_tokens": 552, "text": "8.4 Brain–Computer Interface 163 actually pressed, the training set was free of class-label noise. As for the attributes, the experimenters suspected many of them to be irrelevant. Classiﬁer Induction and Its Limitations Several machine learning paradigms were experimented with, including multilayer perceptrons and nearest-neighbor classiﬁers with attribute-value ranges normalized so as to avoid scaling-related problems. Relevant attributes were selected by a decision tree, similarly as in the applications discussed in the previous sections. Typical error rates of these induced classiﬁers on testing data were in the range 20–30%, depending on the concrete subject. It is quite possible that better accuracy could not be achieved: the information provided by the electrodes might have been insufﬁcient for this kind of experiment. Importantly, just as in the sleep-classiﬁcation domain, the classiﬁer could only be applied to the subject from whose training data it has been induced. All attempts to induce a “general classiﬁer” (to be used on any future subject) failed—the error rates were so high as to make the classiﬁcations process look random. Testing the Classiﬁer’s Utility As in the previous domain, the error rate measured on independent data does not give the whole picture; the classiﬁer’s practical utility depends on how it is actually employed. In our case, the real question is: will the classiﬁer succeed in sending the cursor to the correct location? The question was answered by an experiment illustrated in . The subject is still sitting in front of a computer monitor, with electrodes attached to the scalp. However, the board with the two buttons has been removed. On the monitor, we can see two rectangles, one on the left and one on the right. In the middle is a cursor in the shape of a little cross. When instructed to move the cursor, say, to the right, the subject onlyimagined he was pushing the right button. The electrodes recorded the neural signals and passed them to the classiﬁer. Based on these, the classiﬁer selected the side (the class,left orright) to which the cursor is to be moved. The movement was very slow so as to give the subject the opportunity to “correct” the wrong direction. To make this possible, the neural signals were sampled repeatedly, each sample becoming one example to be passed on to the classiﬁer. Suppose the subject’s intention is to move the cursor to the right. It is possible that the ﬁrst sample will be misclassiﬁed as The classiﬁer’s task is to move the cross into either the left box or the right box, according to the subject’s “thoughts”"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00038", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 38, "start_word": 14278, "end_word": 14723, "approx_tokens": 578, "text": "8.5 Medical Diagnosis 165 8.5 Medical Diagnosis A physician attempting to ﬁnd the cause of her patient’s complaints behaves a bit like a classiﬁer: based on certain attributes (the patient’s symptoms, results of laboratory tests), she suggests a diagnosis—and a treatment. No wonder that, in the early days of machine learning, many believed medical diagnosis to be its natural, almost straightforward application. Some Results The optimism was motivated by early studies such as the one whose results are summarized in . The numbers give the classiﬁcation accuracy achieved by Bayesian classiﬁers, decision trees, and physicians on the same testing set—patients described on attribute vectors. The four domains differ in the number of classes and in the difﬁculty of the task (e.g., noise in the data, relevance and reliability of the available information). The values indeed seem to conﬁrm machine-learning’s ability to induce classi- ﬁers capable of competing with human experts; indeed, they seem to outperform them. In the multi-class primary tumor domain, the margin is perhaps unim- pressive, but in all the other domains, the machine learning classiﬁers appear to be clearly better. It is only a pity that the table does not tell us more about the variation in the results. At least standard deviations would help, here, but there are other ways how to ascertain whether the results are statistically reliable—Chaps.11 and 12 will have more to say about the methods to be used here. Are the Results Encouraging? The results seem impressive, but we must not jump to conclusions. The ﬁrst question to ask is whether the comparison was fair. Since the examples were described by attributes, the data available to the machine-learning tool, and to the induced classiﬁer, could hardly be the same as used by the physician who could rely also on subjective information that might not have been available to the machine. In this respect, the human enjoyed a certain advantage, and this makes the machine learning’s results all the more impressive. On the other hand, the authors of the study admit that the physicians participating in the study were not necessarily top specialists in the given ﬁeld, and machine learning thus outperformed only general practitioners. Another aspect worth our attention is that this study was conducted in the 1980s when the diagnostic tools were less sophisticated than those in use today. It is hard to tell who will beneﬁt more from modern laboratory tests: human experts or machine learning? Classiﬁcation accuracy of two classiﬁers compared to the performance of physicians in four medical domains Bayesian Decision General classiﬁer tree practitioner Primary tumor 0.49 0.44 0.42 Breast cancer 0.78 0.77 0.64 Thyroid 0.70 0.73 0.64 Rheumatology 0.67 0.61 0.56"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00039", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 39, "start_word": 14723, "end_word": 15257, "approx_tokens": 694, "text": "Classiﬁcation, Also Explanation Is Needed For medical diagnosis, classiﬁcation accuracy is not enough; more accurately, it is not enough for the diagnosis to be correct. A patient will hardly agree with a surgery if the only argument supporting this recommendation is, “this is what the machine says.” The statement that “the machine is on average a few percentage points better than a physician” is unlikely to convince the patient, either. What a reasonable person wants is a convincing explanation of why the surgery is to be preferred over a more conservative treatment. In the case of the domains from , the doctor is usually able to itemize what evidence supports the concrete diagnosis and why, and what treatment options are recommended in this particular case and why. Unfortunately, the baseline machine learning techniques are not very good at explaining their decisions (perhaps with the exception of decision trees). Software for automated reasoning would be needed, here. This, however, is beyond the scope of this book. The Need for Measuring Conﬁdence There is another problem to be aware of. Most of the classiﬁers induced by the techniques treated in the previous chapters only give the ﬁnal verdict; for instance, “the example belongs to class 77.” The physician (as well as the patient) needs to know more. For instance, is the recommended class absolutely certain or is it only just a little more likely than some other class? And if so, which other classes should be considered? In other words, medical domains usually call for classiﬁers capable of telling us how conﬁdent they are in the returned class. For example, such classiﬁer should say that “the example belongs toC 1 with probability 0.8, and toC5 with probability 0.7.” There probabilities are explicitly calculated by Bayesian classiﬁers, so these may be a good choice; similar information can be obtained also from neural networks. Both of these approaches, however, are rather unable to offer explanations. On the other hand, decision trees are capable of offering some kind of explana- tion. As for the conﬁdence, this is only possible with additional functions that we have not treated here. Cultural Barriers There is one last reason why the encouraging results of the early applications of machine learning to medical diagnosis failed to inspire followers. With only minor injustice, they can be called cultural barriers. In a way, they are understandable. Medical doctors surely did not like to be told how easily they would be replaced by computers. No wonder that, in the early days of machine learning, many of them were not eager to collaborate on the development of the requisite software. This, of course, is a misunderstanding. Machine learning is not meant to replace a human expert. At its very best, is only offer advice; the ﬁnal diagnostic decision will always be the responsibility (even in legal terms) of the physician treating the patient. Still, the value of the advice should not be underestimated. For instance, the classiﬁer can alert the doctor that additional, previously unsuspected, disorders my accompany the current diagnosis (the patient often suffers from more than just one problem at a time), and it can even point out the need to take speciﬁc additional laboratory tests."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00040", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 40, "start_word": 15257, "end_word": 15635, "approx_tokens": 491, "text": "learning. Far from it. The truth is, if you work on a project for quite some time, you develop not only personal attachment to it, but also certain deeper understanding that makes this particular application more appropriate for instruction purposes than those which you only read about in the literature. 8.8 Exercises and Thought Experiments The exercises are to solidify the acquired knowledge. The ambition of the suggested thought experiments is to let the reader see this chapter’s ideas in a different light and, somewhat immodestly, to provoke his or her independent thinking. Give It Some Thought 1. Suppose that you know that the correctness of some class labels in your training set is not certain. Would you recommend that these “unreliable” examples be removed from the training set? In your considerations, do not forget that some of the pre-classiﬁed examples will be used as testing examples to assess the classiﬁcation performance of the induced classiﬁer. 2. Discuss the reasons why application-domain users may be reluctant to accept machine-learning tools. Suggest ways to eliminate, or at least diminish, their suspicions and concerns. 3. Section 8.2 mentioned a simple mechanism by which the k-NN classiﬁer can manipulate the two kinds of error (false negatives versus false positives). Suggest a mechanism that a Bayesian classiﬁer or a neural network can use to the same end. How would you go about implementing this mechanism in a linear classiﬁer? 4. In more than one of the domains discussed in this chapter, it was necessary to reduce the number of irrelevant and/or redundant attributes. In the projects reported here, decision trees were used. Suggest another possibility that would use a technique from one of the previous chapters. Discuss its advantages and disadvantages. 5. Suppose you wanted to implement a program that would decide whether a given position in the tic-tac-toe game (see is winning. What attributes would you use? How would you collect the training examples? What can you say about expected noise in the data thus collected? What classiﬁer would you use? What difﬁculties are to be expected? In tic-tac-toe, the goal is to achieve three crosses (or circles)i nar o wo r a column or on a diagonal o o o x o x x o x o x x x"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00041", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 41, "start_word": 15635, "end_word": 16105, "approx_tokens": 611, "text": "9.1 The algorithm of Bagging Input: the training set, T, and the user’s choice of the induction technique 1. Using random sampling with replacement, create from T several training subsets, T1; T2;::: Tn. Each subset consists of the same number of examples. 2. From each Ti, induce classiﬁer Ci (for i D 1:::; n). 3. When presented with a new example, submit it to all Ci’s in parallel and let each classiﬁer, Ci, offer its recommendation for the example’s class. 4. A “master classiﬁer” decides which of the two classes received more votes. Assuming that each of the participating classiﬁers represents a somewhat different aspect of the recognition task, the classiﬁer group (sometimes called a “voting assembly”) is expected to outperform any individual. Induction of the Voting Assembly The principle of bagging is summarized in . The idea is to take the original training set, T, and to create from it a certain number of training subsets, T 1;::: Tn, each of the same size. Once the subsets Ti have been created, a machine-learning algorithm induces a classiﬁer, Ci, separately from each of them. For this, any of the induction techniques from the previous chapters can be used. However, the baseline version of bagging assumes that the same technique (say, induction of decision trees with the same user-set parameters) is always used. Bootstrapping Let us now explain how the training examples for Ti are selected. Each example has the same chance of being picked. Once it has been included in Ti, it is “returned” to T, by which we mean that it will participate in the selection of examples for TiC1 with the same probability as any other example. For a training set, T, consisting of N examples, the selection is repeated n times in a process known as bootstrapping. An example can appear in Ti more than once and, conversely, some examples will fail to appear in Ti. This means that each Ti consists of N examples (with duplicities), but each of these training subsets is different. As a result, each of the induced classiﬁers will focus on a different aspect of the learning problem. Why It Works explains how the voting can help reduce the error rate. Three classiﬁers are considered. If the errors are rare, there is a chance that each classiﬁer will err on different examples. In this event, each example will be misclassiﬁed at most once, the other two class labels being correct. An individual’s occasional mistake will thus be “outvoted” (and thus corrected) by the others. Of course, the situation will rarely be so convenient. If an example is misclas- siﬁed by two out of the three classiﬁers, the voting will result in the wrong class. One can stipulate, however, that this unfavorable situation might be improved if the number of classiﬁers is increased."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00042", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 42, "start_word": 16105, "end_word": 16548, "approx_tokens": 575, "text": "9.2 Schapire’s Boosting Although the bagging approach often achieves impressive results, it suffers from a serious shortcoming: the voting classiﬁers have all been induced independently of each other from randomly selected data. One would surmise that a smarter—and perhaps more successful—approach should rely on a mechanism that makes the classiﬁers complement each other. For instance, this can be done by inducing each of them from training examples that are perceived as difﬁcult by the other classiﬁers. Schapire’s boosting was invented with this idea in mind. Induction of Three Mutually Complementing Classiﬁers Suppose that a random subset, T 1 2 T,o f m training examples has been created. These are used to induce the ﬁrst classiﬁer, C1. When testing this classiﬁer on the entire training set, T,w e will observe that it misclassiﬁes a certain number of examples. Suppose we now create another training subset, T2 2 T. Let it consist of m examples selected in a manner that ensures that the previously inducedC1 classiﬁes correctly 50% of them, misclassifying the remaining 50%. This means that T2 is so difﬁcult for C1 that the classiﬁer will not outperform a ﬂipped coin. From the training subset thus created, the second classiﬁer, C2, is induced. The two classiﬁers, C1 and C2, having been induced each from different examples, will inevitably differ in how they label certain instances. A tie-breaker is therefore needed. To this end, a third training subset, T 3, is created, consisting only of examples on which C1 and C2 differ. From this third subset, T3, the third classiﬁer, C3, is induced. The principle is summarized by the pseudocode in . When an example is presented, the master classiﬁer collects the labels recommended by the three classiﬁers, and then returns the class that has received more votes. Ideally, each of the training sets, Ti, is of the same size, m. Recursive Implementation The principle just described can be implemented recursively. shows how. The resulting triplet of classiﬁers (in the dotted rectangle) is treated as a single classiﬁer. In the next step, a new training subset,T4, is created in a manner that ensures that the triplet’s error rate on T4 is 50%. From The algorithm of Schapire’s boosting Input: the training set, T, and the user’s choice of the induction technique 1. Create a random training subset, T1, and induce from it classiﬁer C1. 2. Create a training subset T2 in a manner that makes sure that C1 scores 50% on it. Induce from T2 classiﬁer C2. 3. Create T3 such that C1 and C2 disagree on each of the examples it contains. Induce from T3 classiﬁer C3. 4. For classiﬁcation, use plain majority voting."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00043", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 43, "start_word": 16548, "end_word": 17058, "approx_tokens": 663, "text": "Ai. As before, the classiﬁers’ outputs are combined by theweighted majority voting, the weights being obtained (for instance) by perceptron learning. The approach is useful in domains marked by a great many attributes of which most are suspected to be either irrelevant or redundant. Typically, classiﬁers induced using less valuable attribute sets will exhibit poor classiﬁcation performance; and as such, they will receive low (or even negative) weights. The approach can easily be combined with classical bagging: the idea is that, for each classiﬁer, a different set of examples, and also a different set of attributes, should be used. Non-homogeneous Boosting So far, all boosting approaches presented here assumed that the individual classiﬁers are induced from somewhat different data, but always using the same induction technique. But there is no reason for this always having to be the case. The so-called non-homogeneous boosting does the exact opposite: each classiﬁer is induced from the same data, but with a different machine-learning technique. The classiﬁers’ outputs are then, again, combined by weighted majority voting. The main advantage of this approach is the way in which it reduces error. As we will see in , the errors committed by any classiﬁer fall into two fundamental categories. The ﬁrst is caused by the variance in the available data: from a different training set, a somewhat different classiﬁer will be induced, and this will lead to different errors. The second source of error is the bias inherent in the classiﬁer. For instance, a linear classiﬁer cannot avoid misclassifying some examples if the decision surface separating the positive examples from the negative is highly non- linear. Importantly, non-homogeneous boosting is known to reduce both kinds of error: variance-related errors (this reduction happens in all boosting algorithms) as well as bias-related errors (which is an advantage speciﬁc to non-homogeneous boosting). Stacking The idea of non-homogeneous boosting is to take the outputs of a group of classiﬁers, and then reach the ﬁnal classiﬁcation decision by the weighted majority voting. If you think of it, two layers are involved here: at the lower level are the base-level classiﬁers; and at the upper level is the master classiﬁer combining their outputs. Note that the master classiﬁer itself has to be induced; for instance, with the help of perceptron learning—because this is essentially a linear classiﬁer. The principle can be generalized to the so-called stacking approach. As before, a set of diverse classiﬁers is used at the lower lever. The upper level then goes beyond the bounds of a linear classiﬁer. Indeed, any paradigm can be used for the master classiﬁer: Bayesian, nearest-neighbor based, a decision tree, or a neural network. The linear classiﬁer used in non-homogeneous boosting is just one out of many possibilities. Likewise, the base-level classiﬁers may come from the most diverse paradigms. Sometimes, however, the individual classiﬁers differ only in the concrete choice of parameter values. For instance, there can be a few decision trees differing in the extent of pruning. At any rate, the base-level classiﬁers should all differ in the way they classify the examples."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00044", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 44, "start_word": 17058, "end_word": 17601, "approx_tokens": 705, "text": "ﬁnd the answer typically explores various notions offered by the number theory, such as primes, odd numbers, integers whose values exceed a certain threshold, results of arithmetic operations, and so on. After a lot of effort, some property satisfying the training examples is found. Usually, however, the discovered rule is ridiculously complicated and awkward to say the least. And yet, there is a simple solution which the students almost never hit upon. Thing is, the underlying property does not come from the realm of arithmetics. What the positive examples have in common (and the negative examples lack) is that they all begin with the letter t: two, three, ::: , all the way to twenty-eight. Conversely, none of the integers in the set of negative examples begins with at. The reason this simple solution is so difﬁcult to ﬁnd is that most people search for it in the wrong place: arithmetics. Expressed in the language of machine learning, they rely on the wrong bias. Once they are given the correct answer, their mindset will be willing to take this experience into account in the future. If you give them another puzzle of a similar nature, they will subconsciously think not only about arithmetics, but also about the English vocabulary—they will incorporate into their thinking also this new bias. Representational Bias Versus Procedural Bias As far as machine learning is concerned, biases come in different forms. A so-called representational bias is determined by the language in which we want the classiﬁer to be formulated. For instance, in domains with continuous-valued attributes, one possible repre- sentational bias can consist in the choice of a linear classiﬁer; another can be the preference for polynomials, and yet another the preference for neural networks. If all attributes are discrete-valued, the engineer may prefer conjunctions of attribute values, or perhaps even decision trees. Of course, all of these biases then have its advantages as well as shortcomings. Apart from this, the engineer usually also has to opt for a certain procedural bias. By this we mean preference to a certain method of searching for the solution, the selection of a speciﬁc machine-learning procedure. For instance, one such bias relies on the assumption that pruning will improve the classiﬁcation performance of a decision tree on future data. Another procedural bias is the choice of a concrete set of parameters in a neural-network’s training. And yet another, in the ﬁeld of linear classiﬁers, can be the engineer’s decision to employ theperceptron learning instead of WINNOW—or the other way round. The Strength of a Bias Versus the Correctness of a Bias Suppose the engineer wants to decide whether to approach the given machine-learning problem with a linear classiﬁer or with a neural network. If the positive and negative examples are linearly separable, then the linear classiﬁer is clearly the better choice. While both paradigms are capable of ﬁnding the solution, neural networks tend to overﬁt the training set, thus poorly generalizing to future data. On the other hand, if the tentative boundary separating the two classes is highly non-linear, then the linear classiﬁer will lack the necessary ﬂexibility, whereas neural networks will probably manage quite easily. The reader now begins to understand that each bias has two critical aspects: strength, and correctness."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00045", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 45, "start_word": 17601, "end_word": 18151, "approx_tokens": 715, "text": "10.1 A Learner’s Bias 193 Ab i a si sstrong if it deﬁnes only a narrow class of classiﬁers. In this sense, the bias of linear classiﬁers is much stronger than that of neural networks: the former only allow for linear decision surfaces, while the latter can model virtually any decision surface. Ab i a si scorrect if it is the right one for the task at hand. For instance, the linear classiﬁer’s bias is correct only in a domain where the positive examples are linearly separable from the negative ones. A conjunction of boolean attributes is correct only if the underlying class can indeed be described by a conjunction of attributes. Of course, the opposite term, incorrect bias, is not a crisp concept. Some gradation is involved; some biases are only slightly incorrect, others signiﬁcantly so. A Useful Rule of Thumb: Occam’s Razor Ideally, the engineer wants to use a bias (representational or procedural) that is correct. And if there is a possibility to choose between two or more biases that are all correct, the stronger bias is to be preferred because it has a higher chance of success—this is what we learned in where the advice to choose the simpler solution was presented under the name of the Occam’s Razor. Unfortunately, we rarely know in advance the correctness/incorrectness of all possible biases. An educated guess is the best we can hope for. In some paradigms, say, high-order polynomials, the bias is so weak that there is a high probability that a classiﬁer from this class will classify the entire training set with zero error rate; and yet its performance on future data is uncertain on account of its problems with PAC-learnability. Strengthening the bias (say, by reducing a polynomial’s order) will reduce the VC-dimension, increasing the chances on future data—but only as long as the bias remains correct. At a certain moment, further strengthening of the bias will do more harm than good because the bias becomes incorrect, perhaps very much so. What we need to remember is the existence of an almost inescapable trade-off: a mildly incorrect but strong bias can be better than a correct but very weak bias. But what the term, “mildly incorrect bias,” means in a concrete application can usually be decided only based on the engineer’s experience or by additional experimentation (see . “Lifelong Learning” In some applications, the machine-learning software is to learn a series of concepts or classes, all of which are expected to have a solution within the realm of the same speciﬁc bias. In this event, it makes sense to organize the learning procedure in two tiers. At the lower level, the task is to identify the most appropriate bias; at the higher level, the software induces the classiﬁer using this bias. The term used for this strategy, “lifelong learning,” reminds us of something typical of our own human difﬁculties in learning: the need to “learn how to learn” in a given ﬁeld. Two Sources of the Classiﬁer’s Errors The observations made so far will help us get a better grasp of the two main sources of a classiﬁer’s errors. The ﬁrst is the variance in the training examples. Thing is, the data used for the induction of the concrete classiﬁer almost never capture all aspects of the"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00046", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 46, "start_word": 18151, "end_word": 18647, "approx_tokens": 644, "text": "10.2 Imbalanced Training Sets 195 imbalanced representation of the two classes is not without serious consequences for machine learning. This section will explain the cause of the difﬁculties, then proceed to some very simple ways of reducing their negative impact. A Simple Experiment Suppose we have at our disposal a training set that is so small that it consists of only 50 positive examples and 50 negative examples. Let us subject this set to a ﬁvefold crossvalidation1: we divide it into ﬁve equally sized parts; then, in ﬁve different experimental runs, we always remove one of the parts, induce a classiﬁer from the union of the remaining four, and test the classiﬁer on the removed part. In this manner, we eliminate, or at least reduce, the effect of randomness in the choice of the concrete training set. At the end, we write down the average results of the testing: classiﬁcation accuracy on the positive examples, classiﬁcation accuracy on the negative examples, and the geometric mean of the two classiﬁcation accuracies. Suppose now that we realize we have many more negative examples at our disposal than we originally thought. In order to ﬁnd out how this newly discovered bounty is going affect the learning, we add to the previous training set another 50 negative examples (the positive examples remain the same), repeat the experimental procedure, and then write down the new result. We then continue in the same spirit, always adding to the training set another batch of 50 negative examples while keeping the same original 50 positive examples. Observation If we plot the results of the above series of experiments, we will obtain a graph that, in all likelihood, will look very much like the one shown in where the 1-NN classiﬁer was used. The reader can see that as the number of the majority-class examples increases, the induced classiﬁers become biased toward this class, gradually converging to a situation where the classiﬁcation accuracy on the negative examples (the majority class) approaches 100%, while the classiﬁcation accuracy on the positive examples (the minority class) drops to well below 20%. The geometric mean of the two values keeps dropping, too. The observation may appear somewhat counterintuitive. Surely the induced classiﬁers should become more powerful if more training examples are made available, even if these added examples all happen to belong to the same class? In turns out, however, that the unexpected behavior described above is typical of many machine-learning techniques. Engineers usually call it the problem of imbalanced class representation. Majority-Class Undersampling (The Mechanical Approach) The experiment has convinced us that adding more examples from the majority class may cause degradation of the induced classiﬁer’s performance on the minority class. This may be a serious shortcoming. Thus in the oil-spill domain, the minority class represents the oil spills, the primary target of the machine-learning undertaking. In medical diagnosis, any disorder we want to recognize is typically a minority class, too. And 1An evaluation methodology introduced in Sect.11.5."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00047", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 47, "start_word": 18647, "end_word": 18942, "approx_tokens": 383, "text": "0 100 200 300 400 500 600 700 800 10 20 30 40 50 60 70 80 90 100 1 nearest neighbor accuracy number of negative exambles Dotted curve : classiﬁcation accuracy on the neg class; dashed curve : classiﬁcation accuracy on thepos class; solid curve: geometric means of the two classiﬁcation accuracies the same applies to software whose task is to alert a company to misuse of its product (e.g., a wrongful use of calling cards, or credit-card fraud). In domains of this kind, it is the minority class that interests us. We now know that blindly adding more and more majority-class examples to the training set is likely to do more harm than good. Suppose we are provided with a heavily imbalanced training set where, say, nine out of ten examples are negative. In this event, we will often beneﬁt from the removal of many negative examples. In the simplest possible approach, this removal can be made at random: for instance, each negative example will face a 50% chance of being deleted from the training set. As we noticed above, the classiﬁer induced from this reduced set is likely to outperform a classiﬁer induced from the entire training set. Identifying the Cause The mechanical solution indicated in the previous para- graph will hardly satisfy the thoughtful engineer who wants to understand why the data-removing trick worked—or, conversely, why increasing the number of majority-class examples may have such detrimental consequences. Suppose the 1-NN classiﬁer uses a training set where the vast majority of the examples are negative, and only a few are positive. Moreover, the data are known to suffer from a considerable amount of class-label noise. Limiting itself to an easy- to-visualize two-class domain, the left part of shows one such training set."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00048", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 48, "start_word": 18942, "end_word": 19382, "approx_tokens": 572, "text": "10.2 Imbalanced Training Sets 197 + + + + + + __ _ _ _ _ _ _ _ _ __ _ _ _ _ _ _ _ + + + + + + __ _ _ _ _ _ _ _ _ __ __ _ In noisy domains where negative examples heavily outnumber positive examples, the removal of negative examples that participate in Tomek links may increase classiﬁcation performance The reader is sure to have noticed the point: in consequence of the noise, the nearest neighbor of almost every positive example is negative. In reality, these neighbors are probably positive, their negative labels being explained by errors made in the course of the creation of the training set. Be it as it may, the 1-NN classiﬁer misclassiﬁes these positive examples, and this is why there are so many false negatives, and only a few (if any) false positives. Of course, not all machine-learning paradigms will suffer from this situation as dramatically as the 1-NN classiﬁer. But most of them will suffer to some degree, and we now understand the reason why. An Informed Solution: One-Sided Selection Knowing the source of our troubles, we are ready to suggest a remedy. To wit, the cause of our woes is the presence of many class-noisy examples in the positive region; the situation should therefore improve if we remove primarily these examples (rather than resorting to a random selection as in the mechanical approach suggested above). In , we encountered a simple algorithm capable of identifying “suspi- cious” examples: the technique of Tomek links. The reader will recall that two examples, .x; y/, are said to participate in a Tomek link if three conditions are satisﬁed: (1) each of the two examples has a different class label; (2) the nearest neighbor of x is y; and (3) the nearest neighbor of y is x. In the situation depicted in , many of the noisy examples on the left indeed do participate in Tomek links. This indicates that we may do improve the classiﬁer’s behavior if we delete from the training set the negative participants of each Tomek-link pair. The principle is known as one-sided selection because only one side of the Tomek link is selected for inclusion in the training set. Applying the technique to the training set shown in the left part of ,w e will obtain something like the smaller training set shown in the right part. It is easy to see that the frequency of false negatives is now going to be lower. The efﬁciency of this methods is usually higher than just mechanical removal of randomly picked negative examples."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00049", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 49, "start_word": 19382, "end_word": 19846, "approx_tokens": 603, "text": "10.3 Context-Dependent Domains 199 10.3 Context-Dependent Domains Up till now, we have tacitly assumed that the underlying “meaning” of a given class is ﬁxed and immutable, that a single classiﬁer, once induced, will under all circumstances exhibit the same (or at least similar) behavior. This, however, is not always the case. Context-Dependent Classes Some classes change their essence with circum- stances. If you think of that, this is the case of many concepts used in daily life. Thus the meaning of “fashionable dress” changes in time, and different cultures have a different idea of what they want to wear. “State-of-the-art technology” was something else a 100 years ago that it is today. Even the intended meaning of such notorious terms as “democracy” or “justice” depends on political background and historical circumstances. And if you want a more technical example, consider the problems encountered by speech-recognition software: everybody knows that the same word is often pronounced differently in England than in North America; but the software should “understand” speakers from both backgrounds. Context-Dependent Features For the needs of this book, context is understood as a “a feature that has no bearing on the class if taken in isolation, but still affects the class when combined with other features.” For instance, suppose you want to induce a classiﬁer capable of suggesting medical diagnosis, of recognizing X based on a set of symptoms. Some attributes, say, gender, do not have any predictive power; the patient being male is no proof of prostate-cancer; but the attribute value gender=female is a clear indication that the class is not prostate-cancer. This, of course, was an extreme sample. In other diagnoses, the impact of gender will be limited to inﬂuencing the critical values of certain laboratory tests, say,p D 0:5 being a critical threshold for male patients and p D 0:7 for female patients. Alternatively, the prior probabilities will be affected, breast-cancer being more typical of females, although men can suffer from it, too. Induction in Context-Dependent Domains Suppose you want to induce a speech- recognition system, and you have a set of training examples coming both from British and American speakers. Suppose the attribute vector describing each example contains the “context” attribute, the speaker’s origin. The other attributes capture the properties of the concrete digital signal. Each class label represents a different phoneme. For the induction of a classiﬁer that for each attribute vector decides which phoneme it represents, the engineer can essentially follow two different strategies. The ﬁrst takes advantage of the contextual attribute, and divides the training examples into two subsets, one for British speakers and one for American speakers; then it induces a separate classiﬁer from each of these training subsets. The second strategy mixes all examples in one big training set, and induces one “universal” classiﬁer."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00050", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 50, "start_word": 19846, "end_word": 20320, "approx_tokens": 616, "text": "that, in applications of this kind, the ﬁrst strategy performs better, provided that the real-time system in which the classiﬁers are embedded knows which of them to use. This decision can be assisted by an additional two-valued classiﬁer that is trained to distinguish British speakers from American speakers. Concept Drift Sometimes, the context changes in time. The “fashionable dress” example mentioned earlier belongs to this category, as do the political terms. In this event, machine-learning specialists talk about a so-called concept drift. What they have in mind is that, in the course of time, the essence or meaning of a class drifts from one context to another. The drift has many aspects. One of them is the extent to which the context has changed the meaning of the class. In some rare domains, this change is so substantial that the induced classiﬁer becomes virtually useless, and a new one has to be induced. Much more typical, however, is a less severe change that results only in a minor reduction of the classiﬁcation performance. The old classiﬁer can then still be used, perhaps after some ﬁne-tuning. Another feature worth consideration is the “speed” of the drift. At one extreme is an abrupt change. At a certain moment, one context is simply replaced, at it were, by another. More typically, however, the change is gradual in the sense that there is a certain “transition” period during which one context is, step by step, replaced by the other. In this event, the engineer may ask how fast the transition is, and whether (or when) the concept drift will necessitate special actions. Induction of Time-Varying Classes Perhaps the simplest scenario in which concept drift is encountered is the one shown in . Here, a classiﬁer is faced with a stream of examples that arrive one at a time, either in regular or irregular intervals. Each time an example arrives, the classiﬁer labels it with a class. There may or may not be a feedback loop that tells the system (immediately or after some delay) whether the classiﬁcation was correct, and if not, what the correct class label should have been. If there is a reason to suspect the possibility of an occasional concept drift, it may be a good idea to take advantage of a sliding window such as the one shown in the picture. The classiﬁer is then induced only from the examples “seen through the window.” Each time a new example arrives, it is added to the window. Whenever deemed appropriate, older examples are removed, either one at a time, or in groups, A window passes over a stream of examples; “current classiﬁer” is periodically updated to reﬂect changes in the underlying class. Occasionally, the system can retrieve some of the previous classiﬁers if the underlying context recurs current classifier window examples store of old classifiers"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00051", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 51, "start_word": 20320, "end_word": 20742, "approx_tokens": 548, "text": "10.4 Unknown Attribute Values In many domains, certain attribute values are not known. A patient refused to give his age, a measurement device failed, and some information got lost—or is unavailable for any other reason. As a result, we get an imperfect training set such as the one shown in where some of the values are replaced with question marks. In some domains, the question marks represent a considerable portion of all attribute-value ﬁelds, and this may complicate the learning task. The engineer needs to understand what kind of damage the unknown values may cause, and what solutions exist. Adverse Effects In the case of the plain version of thek-NN classiﬁer, the distance between two vectors can only be calculated if all values in the vectors are known. True, the distance metric can be modiﬁed so that it quantiﬁes also the distance between, say, red and unknown; but distances calculated in this manner tend to be rather ad hoc. The situation is not any better in the case of linear and polynomial classiﬁers. Without the knowledge of all attribute values, it is impossible to calculate the weighted sum, † w ixi, whose sign tells the classiﬁer which class label to choose. Likewise, unknown attribute values complicate the use of Bayesian classiﬁers and neural networks. Decision trees are more ﬂexible, in this sense. When classifying an example, it is quite possible that the attribute whose value is unknown will not have to be tested (will not ﬁnd itself on the path from the root node to the terminal node). Trivial Approaches to Filling-In Missing Values In a domain with a sufﬁciently large training set that contains only a few question marks, there is usually no harm in removing all examples that have unknown attribute values. This, however, will become impractical in domains where the number of question marks is so high Training examples with missing attribute values Crust Filling Example Shape Size Shade Size Shade Weight Class ex1 Circle Thick Gray Thick Dark 7 pos ex2 Circle Thick White Thick Dark 2 pos ex3 Triangle Thick Dark Thick Gray 2 pos ex4 Circle Thin White ? Dark 3 pos ex5 Square Thick Dark ? White 4 pos ex6 Circle Thick White Thin Dark ? pos ex7 Circle Thick Gray Thick White 6 neg ex8 Square Thick ? Thick Gray 5 neg ex9 Triangle Thin Gray Thin Dark 5 neg ex10 Circle Thick Dark Thick ? ? neg ex11 Square Thick White Thick Dark 9 neg ex12 Triangle Thick White Thick Gray 8 neg"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00052", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 52, "start_word": 20742, "end_word": 21255, "approx_tokens": 666, "text": "10.4 Unknown Attribute Values 203 that the removal of all affected examples would destroy most of the training set, disposing in the process of valuable information. In this event, we may try to replace the question marks with some values, even though these may be incorrect. This is easily done. When the attribute is discrete, then we may simply replace the question mark with the attribute’s most frequent value. Thus in , exampleex8, the unknown value ofcrust-shade will be replaced with white because this is the most frequent value of this attribute in this particular training set. In the case of a continuous-valued attribute, the average value can be used. In ex6 and ex10,t h ev a l u eo fweight is unknown. Among the 10 examples where it is known, the average value isweight=5.1, and this is the value we will use inex6 andex10. When doing so, caution is called for. The reader has to keep in mind that using the most frequent or average values will render the examples’ description unreliable, perhaps even dubious. The technique should therefore be used sparingly. When many values are missing, more sophisticated methods (such as the one below) should be used. Learning to Fill-in Missing Values Sometimes, using the most common or average values can mislead the learning program. A better idea of how to ﬁll the empty slots is built around the observation that attributes are rarely independent from each other. For instance, the taller the man, the greater his body weight. If the weight of someone with height=6.5 is unknown, it would be foolish to use the average weight calculated over the whole population; after all, our rather tall individual is certainly heavier than the average person. Seeking a way out, we will probably do better calculating the average weight among those withheight >6 . So much for a pair of mutually dependent attributes. Quite often, however, the interrelations are more complicated than that, easily involving three or more attributes. One simple mechanism to predict unknown values in situations of this kind will rely on the idea of decision-tree induction. A pseudocode of the technique is provided in . The idea is quite simple. Suppose that at is an attribute that has, in the training set, many question marks. We want to replace the question marks with the most likely values. We decide to so by means of a decision tree. To this end, we convert An algorithm to determine unknown attribute values Let T be the original training set. Let at be the attribute with unknown values. 1. Create a new training set, T0,i nw h i c hat becomes the class label; the examples are described by all the remaining attributes, the former class label (e.g., pos versusneg) being treated like just another attribute. 2. Remove from T0 all examples in which the value of at is unknown. 3. From this ﬁnal version of T0, induce a decision tree. 4. Use the decision tree thus induced to determine the values of at in those examples in which its values were unknown."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00053", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 53, "start_word": 21255, "end_word": 21784, "approx_tokens": 687, "text": "10.5 Attribute Selection 205 multilayer neural network is used: each additional attribute increases the number of weights to be trained, thus adding to the calculations. Moreover, examples described by thousands of attributes are inevitably sparse, which is known to mislead many machine-learning approaches. For instance, the problem of sparsity in k-NN classiﬁers was explained in . And yet it is known that, for all intents and purposes, most of the attributes are useless, and as such, should be disposed of. Filter Approaches to Attribute Selection Perhaps the simplest approach to attribute selection is based on what machine learning calls ﬁltering. The idea is to calculate for each attribute its “utility” for the classiﬁcation task at hand, and then order them according to this criterion. The intention to select the topN percent. The choice of the “cut-off” point, N, is usually made by trial and error. When ordering the attributes, the information gain from Sect. 6.3 can be used if the attributes are discrete. Thanks to the existence of mechanisms for binarization (see Sect. 6.4), information gain can actually be employed even in the case of continuous-valued attributes; for this, however, statistical approaches to correlation measurement are usually preferred. One reason to criticize attribute ﬁltering is that this approach ignores the relations that exist among the attributes. This makes it difﬁcult, almost impossible, to identify redundant attributes. As we know, a redundant attribute does not bring any additional information beyond that provided by the other attributes; and yet its information gain can be high. There is a relatively simple way to overcome this weakness. We induce a decision tree, and then use only those attributes that are encountered in the tests in the tree’s internal nodes. The careful reader will recall that this approach was used in some of the simple applications discussed in . Wrapper Approaches to Attribute Selection More powerful, but also more computationally expensive, is the so-called wrapper approach to attribute selection. Here is the underlying principle. Suppose we want to compare the quality of two attribute sets, A 1 and A2. From the original training set, T, we create two training sets, T1 and T2. In both, all examples have the same class labels as inT. However,T1 describes the examples by A1 and T2 uses A2. From the two newly created training subsets, two classiﬁers are induced and evaluated on some independent evaluation set, TE. The attribute set that results in the higher performance is better. This is the principle used in the search-based algorithm whose pseudocode is provided in . The input consists of a training set, T, and of a set of attributes, A. The output is a subset, S 2 A, of the most useful attributes. At the beginning, S is empty. At each step, the approach chooses the best attribute from A, and adds it to S. What is “best” is determined by the classiﬁcation performance (on an independent testing set) of the classiﬁer induced from the examples described by the attributes from S. The algorithm is terminated if no addition to S leads to an improvement of the classiﬁcation performance—or if there are no more attributes to be added to S."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00054", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 54, "start_word": 21784, "end_word": 22322, "approx_tokens": 699, "text": "11.1 Basic Performance Criteria 213 Rejecting an Example When discussing the problem of optical character recog- nition, Sect. 8.1, suggested that the classiﬁer should sometimes be allowed to refuse to classify an example if the evidence supporting the winning class is not strong enough. The motivation is quite simple: in some domains, the penalty for misclassiﬁcation can be much higher than the penalty for not making any classiﬁcation at all. An illustrative example is not difﬁcult to ﬁnd. Thus the consequence of a classiﬁer’s refusal to return the precise value of the ZIP code is that the decision where the letter should be sent will have to be made by a human operator. To be sure, this manual processing is more expensive than automatic processing, but not excessively so. On the other hand, an incorrect value returned by the classiﬁer results in the letter being sent to a wrong destination, which can cause a serious delay in delivery. This latter cost is often much higher than the cost of “manual” reading. Similarly, an incorrect medical diagnosis is often more expensive than no diagnosis at all; lack of knowledge can be remedied by additional tests, but a wrong diagnosis may result in choosing a treatment that does more harm than good. This is why the classiﬁer should sometimes refuse to classify an example if the evidence favoring either class is insufﬁcient. In some machine-learning paradigms, the term insufﬁcient evidence is easy to deﬁne. Suppose, for instance, that, in a 7-NN classiﬁer, four neighbors favor thepos class, and the remaining three favor theneg class. The ﬁnal count being four versus three, the situation seems “too close to call.” More generally, the engineer may deﬁne a threshold for the minimum difference between the number of votes favoring the winning class and the number of votes favoring the runner-up class. In bayesian classiﬁers, the technique is easily implemented, too. If the difference between the probabilities of the two most strongly supported classes falls short of a user-speciﬁed minimum, the example is rejected as too ambiguous to classify. Something similar can be done also in neural networks: compare the signals returned by the corresponding output neurons—and refuse to classify if there is no clear-cut winner. In other classiﬁers, such as decision trees, implementation of the rejection mech- anism is not so straightforward, and is only made possible by the implementation of “additional tricks.” Advantages and Disadvantages of a Rejection to Classify The classiﬁer that occasionally refuses to make a decision about an example’s class is of course less likely to go wrong. No wonder that its error rate will be lower. Indeed, the more examples are rejected, the lower the error rate. But the caution should not be exaggerated. It may look like a good thing that the error rate is reduced almost to zero. But if this low rate is achieved only thanks to the refusal to classify almost all examples, the classiﬁer becomes impractical. Which of these two aspects (low error rate versus rare classiﬁcations) plays a more important role will depend on the concrete circumstances of the given application. graphically illustrates the essence of the trade-off involved in decisions of this kind. The horizontal axis represents a parameter capable of"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00055", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 55, "start_word": 22322, "end_word": 22752, "approx_tokens": 559, "text": "11.2 Precision and Recall 215 are positive, and all the remaining 98% are negative. A “classiﬁer” that returns the negative class for any example in the set will be correct 98% of the time—which may look like a remarkable feat. And yet, the reader will agree, a classiﬁer that never recognizes a positive example is useless. Imbalanced Classes Revisited This observation is worth keeping in mind because domains with imbalanced classes are quite common. We encountered some of them in Chaps. 8 and 10, and other applications can be found. Thus in automated information retrieval, the user may want to ﬁnd a scientiﬁc document dealing with, say, “performance evaluation of classiﬁers.” No matter how attractive the topic may appear to this particular person, papers dealing with it will represent only a small fraction of the millions of documents available in the digital library. Likewise, patients suffering from a speciﬁc medical disorder are in the entire population relatively rare. And the same goes for any undertaking that seeks to recognize a rare event such as a default on mortgage payments or a fraudulent use of a credit card. A seasoned engineer will go so far as to say that the majority of realistic applications are in some degree marked by the phenomenon of imbalanced classes. In domains of this kind, error rate and classiﬁcation accuracy will hardly tell us anything reasonable about the classiﬁer’s practical utility. Rather than averaging the performance over both (or all) classes, we need criteria that focus on a class which, while important, is represented by only a few examples. Let us take a quick look at some of them. Precision By this we mean the percentage of true positives, N TP, among all examples that the classiﬁer has labeled as positive: NTP C NFP. The value is thus obtained by the following formula: Pr D NTP NTP C NFP (11.3) Put another way,precision is the probability that the classiﬁer is right when labeling an example as positive. Recall By this we mean the probability that a positive example will be correctly recognized as such (by the classiﬁer). The value is therefore obtained by dividing the number of true positives,NTP, by the number of positives in the given set:NTPCNFN . Here is the formula: Re D NTP NTP C NFN (11.4) Note that the last two formulas differ only in the denominator. This makes sense. Whereas precision is the frequency of true positives among all examples deemed positive by the classiﬁer, recall is the frequency of the same true positives among all positive examples in the set."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00056", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 56, "start_word": 22752, "end_word": 23109, "approx_tokens": 464, "text": "11.2 Illustration of the two criteria: precision and recall Suppose a classiﬁer has been induced. Evaluation on a testing set gave the results summarized in this table: Labels returned by the classiﬁer pos neg True labels: pos 20 50 neg 30 900 From here, the following values of precision, recall, and accuracy are obtained: precision D 20 50 D 0:40I recall D 20 70 D 0:29I accuracy D 920 1000 D 0:92 Suppose the classiﬁer’s parameters were modiﬁed with the intention to improve its behavior on positive examples. After the modiﬁcation, evaluation on a testing set gave the results summarized in the table below. Labels returned by the classiﬁer pos neg True labels: pos 30 70 neg 20 880 From here, the following values of precision, recall, and accuracy are obtained. precision D 30 50 D 0:60I recall D 30 100 D 0:30I accuracy D 910 1000 D 0:91 The reader can see that precision has considerably improved, while recall remained virtually unaffected. Note that classiﬁcation accuracy has not improved, either. Illustration of the Two Criteria illustrates the behavior of the two criteria in a simple domain with an imbalanced representation of two classes, pos andneg. The induced classiﬁer, while exhibiting an impressive classiﬁcation accuracy, suffers from poor precision and recall. Speciﬁcally, precision of Pr D 0:40 means that of the 50 examples labeled as positive by the classiﬁer, only 20 are indeed positive, the remaining 30 being nothing but false positives. With recall, things are even worse: out of the 70 positive examples in the testing set, only 20 are correctly identiﬁed as such by the classiﬁer. Suppose that the engineer decides to improve the situation by modifying some of the classiﬁer’s internal parameters, and suppose that this modiﬁcation results in an increased number of true positives (from NTP D 20 to NTP D 30) and also a drop in the number of false positives (from NFP D 30 to NFP D 20). On the other hand, the number of false negatives has gone up, too: from NFN D 50 to NFN D 70.T h e calculations in indicate that recall was thus barely affected, butprecision"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00057", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 57, "start_word": 23109, "end_word": 23606, "approx_tokens": 646, "text": "11.2 Precision and Recall 217 has improved, from Pr D 0:40 to Pr D 0:60. Classiﬁcation accuracy remained virtually unchanged—actually, it has even gone down a bit, in spite of the improved precision. When High Precision Matters In some domains, precision is more important than recall. For instance, when you purchase something from an e-commerce web site, their recommender system often reacts with a message to the effect that, “Customers who have bought X buy also Y .” The obvious intention is to cajole you into buying Y as well. Recommender systems are sometimes created with the help of machine learning techniques applied to the company’s historical data. 1 When evaluating their perfor- mance, the engineer wants to achieve highprecision. The customers better be happy about the recommended merchandise, or else they will ignore the recommendations in the future. The value of recall is here unimportant. The list offered on the web site has to be of limited size, and so it does not matter much that the system identiﬁes only a small percentage of all items that the customers may like. When High Recall Matters In other domains, by contrast, recall is more impor- tant. This is often the case in medical diagnosis. A patient suffering from X, and properly diagnosed as such, represents a true positive. A patient suffering from X but not diagnosed as such represents a false negative, something the doctor wants to avoid—which means that N FN should be small. In the deﬁnition of recall, Re D NTP NTPCNFN , the number of false negatives appears in the denominator; consequently, a small value of NFN implies a high value of recall. ROC Curves In many classiﬁers, tweaking certain parameters can modify the values of NFP and NFN , thus affecting (at least to some extent) the classiﬁer’s behavior, for instance, by improving recall at the cost of worsened precision or vice versa. This possibility can be useful in domains where the user has an idea as to which of these two quantities is more important. The reader will ﬁnd it easy to suggest various ways to do the “tweaking.” Thus in the k-NN classiﬁer, the engineer may request that an example be labeled as negative unless some very strong evidence supports the positive label. For instance, if four out of seven nearest neighbors are positive, the classiﬁer can be instructed still to return the negative label (in spite of the small majority recommending the positive class). In this way, the number of false positives can be reduced, though this often means to the price in terms of an increased number of false negatives. Even stronger reduction in N FP (and an increase in NFN ) can be achieved by requesting that any example be deemed negative unless at least ﬁve (or six) of the seven nearest neighbors are positive. 1The concrete techniques employed to this end are somewhat more advanced than those discussed in this textbook, and are thus not treated here."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00058", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 58, "start_word": 23606, "end_word": 23985, "approx_tokens": 492, "text": "11.4 Learning Curves and Computational Costs The ﬁrst four sections of this chapter dealt with the problem of performance evaluation of the induced classiﬁers. Let us now turn our attention to the evaluation of the learning algorithm itself. How efﬁcient is the given induction technique computationally? And how good are the classiﬁers it induces? Will better results be achieved if we choose some other machine-learning framework? In this section, we will give some thought to the costs of learning—in terms of the number of examples needed for successful induction, as well as in terms of the computational time consumed. The other aspect, the ability to induce a tool with high classiﬁcation performance will be addressed in the following section. The Learning Curve When evaluating a human subject’s ability to learn how to solve a certain problem, psychologists rely on a learning curve, a notion that machine learning has borrowed for its own purposes. From our perspective, the learning curve simply shows how the classiﬁcation performance of the induced classiﬁer depends on the size of the training set. Two such curves are shown in . The horizontal axis represents the number of training examples; and the vertical axis represents the classiﬁcation accuracy of the classiﬁer induced from these examples. Usually, though not always, this classiﬁcation accuracy is evaluated on independent testing examples. Most of the time, a larger training set means higher classiﬁcation performance— at least until the moment when no further improvement is possible. Ideally, we would like to achieve maximum performance from the smallest possible training set. For one thing, training examples can be expensive to obtain, and their source can be limited no matter how much we are willing to spend on them. For another, the more examples we use, the higher the computational costs on induction. Comparing Learners with Different Learning Curves shows the learning curves of two learners, l 1 and l2. The reader can see that the learning curve of the former, l1, rises very quickly, only to level off at a point beyond which virtually no improvement is possible—the limitation may be imposed by an A learning curve shows how the achieved classiﬁcation accuracy depends on the number of examples used during learning 100% classification accuracy # training examples l l 2 1"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00059", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 59, "start_word": 23985, "end_word": 24480, "approx_tokens": 643, "text": "Signiﬁcance A set of binary values returned by a random-number generator set to return a one 80% of the time 0 0 1 0 1 1 1 0 1 1 6 1 1 0 1 1 1 1 1 1 1 9 1 1 1 0 1 1 1 1 1 1 9 1 1 1 1 1 1 0 0 1 1 8 1 1 1 0 1 0 1 0 1 1 7 1 1 1 1 1 1 1 1 1 1 10 1 1 1 1 1 1 1 1 0 1 9 1 1 1 0 1 1 1 0 1 1 8 1 1 1 0 1 0 1 1 1 1 8 1 0 1 1 1 1 1 0 1 1 8 9 8 9 5 10 8 9 5 9 10 82 In reality, there are 82 ones and 18 zeros. At the ends of the rows and columns are the cor- responding sums The numbers on the side and at the bottom of the table tell us how many ones are found in each row and column. Based on these, we can say that the proportions of ones in the ﬁrst two rows are 0.6 and 0.9, respectively, because each row contains 10 numbers. Likewise, the proportions of ones in the ﬁrst two columns are 0.9 and 0.8. The average of these four proportions is.0:6 C 0:9 C 0:9 C 0:8/=4 D 0:80, and the standard deviation is 0:08. 1 For a statistician, each row or column represents a sample of the population. All samples have the same size: n D 10. Now, suppose we increase this value to, say, n D 30. How will the proportions be distributed then? Returning to the table, we can see that the ﬁrst three rows combined contain 6 C 9 C 9 D 24 ones, the next three rows contain 8 C 7 C 10 D 25 of them, the ﬁrst three columns contain 9 C 8 C 9 D 26, and the next three columns contain 5C10C8 D 23. Dividing each of these numbers byn D 30, we obtain the following proportions: 24 30 D 0:80; 25 30 D 0:83; 26 30 D 0:87, and 23 30 D 0:77. Calculating the average and the standard deviation of these four values, we get 0:82 ˙ 0:02. If we compare the results observed in the case of n D 10 with those for n D 30, we notice two things. First, there is a minor difference between the average calculated for the bigger samples (0.82) versus the average calculated for the smaller samples (0.80). Second, the bigger samples exhibit a clearly smaller standard deviation: 0.02 for n D 30 versus 0.08 for n D 10. Are these observations explained by mere coincidence, or are they the consequence of some underlying law? 1Recall that standard deviation is the square root of variation; this, in turn, is calculated by from ."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00060", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 60, "start_word": 24480, "end_word": 24975, "approx_tokens": 643, "text": "spectrum, we ﬁnd domains where only some examples belong to more than one class, the majority being labeled with only a single one. Whatever the characteristics of the concrete data, the task for machine learning is to induce a classiﬁer (or a set of classiﬁers) satisfying two basic requirements. First, the tool should for a given example return as many of its true classes as possible; missing any one of them would constitute a false negative. At the same time, the classiﬁer should not label the example with a class to which the example does not belong—each such “wrong” class would constitute a false positive. Neural Networks explained the essence of a multilayer perception , MLP, a popular architecture of artiﬁcial neural networks. The reader will recall that the output layer consists of one neuron for each class, the number of inputs equals the number of attributes, and the ideal size of the hidden layer reﬂects the complexity of the classiﬁcation problem at hand. On the face of it, using an MLP in multi-label domains should not pose any major problems. For instance, suppose the network has been presented with a training example that is labeled with classes C 3; C6; and C7. In this event, the target values for training will be set to, say, ti D 0:8, in the case of output neurons with indices i 2f 3; 6; 7g, and to ti D 0:2 for all the other output neurons.1 The backpropagation- of-error technique can then be used in the same manner as in single-label domains. A Word of Caution Multilayer perceptions may not necessarily be the best choice here. Indeed, multi-label domains have been less intensively studied, in the neural- networks literature, than other approaches, and not without reason. For one thing, the training of plain MLPs is known to be vulnerable to local minima, and there is always the architecture-related question: what is the best number of hidden neurons if we want to strike a reasonable compromise between overﬁtting the data if the network is too large, and suffering from insufﬁcient ﬂexibility if the network is too small? Also the notoriously high computational costs can be a reason for concern. The fact that each training example can belong to more than one class certainly complicates the learning process. Sensing the difﬁculty of the task, the engineer is sometimes tempted to increase the number of hidden neurons. This, however, not only adds to the already high computational costs, but also increases the danger of overﬁtting. It is always good to keep in mind that training neural networks is more art than science. While a lot can be achieved through ingenuity and experience, beginners are often disappointed. And in the case of a failure, the machine-learning expert should be prepared to resort to some alternative, less dangerous technique. 1The reader will recall that the target values 0:8 and 0:2 are more appropriate for the backpropagation-of-error algorithm than 1 and 0. See ."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00061", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 61, "start_word": 24975, "end_word": 25519, "approx_tokens": 707, "text": "13.2 Treating Each Class Separately: Binary Relevance 255 Thus in the very ﬁrst of them,T1, examplesex1 andex3 are labeled with 1 because these (and only these) two examples contain the label C1 in the original T.T h e remaining examples are labeled with 0. The baseline learner is applied separately to each of the ﬁve new sets, inducing from each Ti the corresponding classiﬁer Ci. An Easy-to-Overlook Pitfall In each of the training sets thus obtained, every example is labeled as a positive or negative representative of the given class. When the induced binary classiﬁers are used in parallel (to classify somex), it may happen that none of them returns 1. This means that no label for x has been identiﬁed. When writing the machine-learning software, we must not forget to instruct the classiﬁer what to do in this event. Usually, the programmer chooses from the following two alternatives: (1) return a default class, perhaps the one most frequently encountered in T, or (2) reject the example as too ambiguous to be classiﬁed. Discussion The thing to remember is that the idea behind binary relevance is to transform the multi-label problem into a set of single-label tasks that are then addressed by classical machine learning. To avoid disappointment, however, the engineer needs to be aware of certain difﬁculties which, unless properly addressed, may lead to underperformance. Let us brieﬂy address them. Problem 1: Imbalanced Classes Some of the new training sets, T i, are likely to suffer from the problem of imbalanced class representation which was discussed in Sect. 10.2. In , this occurs in the case of sets T4 and T5. In each of them, only one example out of ﬁve (20%) is labeled as positive, and all others are labeled as negative. In situations of this kind, we already know, machine-learning techniques tend to be biased toward the majority class—in this particular case, the class labeled as 0. The solution is not difﬁcult to ﬁnd. The two most straightforward approaches are majority-class undersampling or minority-class oversampling. Which of them to choose will of course depend on the domain’s concrete circumstances. As a rule of thumb, one can base the decision on the size of the training set. In very big domains, majority-class undersampling is better; but when the examples are scarce, the engineer cannot afford to “squander” them, and thus prefers minority- class oversampling. Problem 2: Computational Costs Some multi-label domains are very large. Thus the training set in a text categorization domain may consist of hundreds of thousands of examples, each described by tens of thousands of attributes and labeled with a subset of thousands of different classes. It stands to reason that to induce thousands of decision trees from a training set of this size will be expensive, perhaps prohibitively so. We can see that, when considering candidates for the baseline learner, we may have to reject some of them because of computational costs. Another possibility is to resort to the technique discussed in Sect. 9.5 in the context of boosting techniques: for each class, we create multiple subsets of the training examples, some of them perhaps described by different subsets of attributes. The idea is to induce for each class a group of subclassiﬁers that then vote. If (in"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00062", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 62, "start_word": 25519, "end_word": 25987, "approx_tokens": 608, "text": "13.3 Classiﬁer Chains 257 With the exception of C1, the input of each classiﬁer consists of the original attribute vector plus the label returned by the previous classiﬁer The second classiﬁer is then induced from examples labeled as positive or negative instances of class C2. To describe these latter examples, however, one extra attribute is added to the original attribute vector: the output ofC1. The same principle is then repeated in the course of the induction of all the remaining classiﬁers: for each, the training examples are described by the original attribute vector plus the class label returned by the previous classiﬁer. When using the classiﬁer chain for the classiﬁcation of some future example, x, the same pattern is followed. The leftmost classiﬁer receives x described by the original attributes. To all other classiﬁers, the system presents x described by the original attribute vector plus the label delivered by the previous classiﬁer. Ultimately, x is labeled with those classes whose classiﬁers returned 1. An Important Assumption (Rarely Satisﬁed) In the classiﬁer-chain technique, the ordering of the classes from left to right is the responsibility of the engineer. In some applications, this is easy because the classes form a logical sequence. Thus in document classiﬁcation,science subsumesphysics, which in turn subsumes quantum mechanics, and so on. If a document does not belong toscience,i t is unlikely to belong to physics, either; it thus makes sense to choosescience as the leftmost node in the graph in , and to placephysics next to it. In other applications, class subsumption is not so obvious, but the sequence can still be used without impairing the overall performance. Even when the subsumptions are only intuitive, the engineer may always resort to a sequence backed by experiments: she can suggest a few alternative versions, test them, and then choose the one with the best results. Another possibility is to apply the classiﬁer chain only to some of the classes (where the interrelations are known), treating the others as if only plain binary relevance was to be employed. Hierarchically Ordered Classes Class interrelation does not have to be linear. It can acquire forms that can only be reﬂected by a more sophisticated data structure, perhaps a graph. In that case, we will need more advanced techniques such as the one described in Sect. 13.5. One Shortcoming of the Classiﬁer-Chain Approach More often than not, the engineer lacks any a priori knowledge about class interrelations. If she then still wants to employ classiﬁer chains, the best she can do is to create the classiﬁer sequence randomly. Of course, such ad hoc method cannot be guaranteed to work; to insist on an inappropriate classiﬁer sequence may be harmful to the point where the classiﬁcation performance of the induced system may fail to reach even that of plain binary relevance."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00063", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 63, "start_word": 25987, "end_word": 26381, "approx_tokens": 512, "text": "13.4 Another Possibility: Stacking 259 The stacking technique. The upper-layer classiﬁers use as input the original attribute vector. For the lower-layer classiﬁers, this vector is extended by the vector of the class labels returned by the upper layer C2C1 C3 C4 C4’C3’C2’C1’ relevance (independently induced binary classiﬁers, one for each class). More interesting is the bottom layer. Here, the classiﬁers are induced from the training sets where the original attribute vectors are extended by the list of the class labels returned by the upper-layer classiﬁers. In the concrete case depicted in , each attribute vector is preceded by four new binary attributes (because there are four classes): the i-th attribute has value 1 if the i-th classiﬁer in the upper layer has labeled the example as belonging to class i; otherwise, this attribute’s value is 0. Classiﬁcation When the class labels of some future example x are needed, x is presented ﬁrst to the upper-layer classiﬁers. After this, the obtained classes labels are added at the front of x’s original attribute vector as N new binary attributes (assuming there are N classes), and the newly described example is presented in parallel to the lower-layer classiﬁers. Finally, x is labeled with the classes whose lower-layer classiﬁers have returned 1. The underlying philosophy rests on the intuition that the performance of classiﬁer Ci may improve if this classiﬁer is informed about the “opinions” of the other classiﬁers—about the other classes to which x belongs. An Example Consider an example that is described by a vector of four attributes with these values: x Df a; f ; r; zg. Suppose that the upper-layer classiﬁers return the following labels: C1 D 1; C2 D 0; C3 D 1; c4 D 0. In this event, the lower-layer classiﬁers are all presented with the following example description: x Df 1; 0; 1; 0;a; f ; r; zg. The classiﬁcation behaviors of the lower-level classiﬁers can differ from those in the upper layer. For example, if the lower-layer classiﬁers return1; 1; 1; 0, the overall system will label x with C 1; C2, and C3, ignoring the original recommendations of the upper layer. Some Comments Intuitively, this approach is more ﬂexible than classiﬁer chains because stacking makes it possible for any class to inﬂuence the recognition of any other class. The engineer does not provide any a priori information about class"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00064", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 64, "start_word": 26381, "end_word": 26781, "approx_tokens": 520, "text": "13.5 A Note on Hierarchically Ordered Classes 261 machine learning decision trees k−NN classif. irrel. attrib.pruning Sometimes, the classes are hierarchically organized in a way known to the engineer in advance In the picture, the relations are represented by arrows that point from parent nodes to child nodes. A node can have more than one parent, but a well-deﬁned class hierarchy must avoid loops. The data structure deﬁning class relations of this kind is known as a directed acyclic graph. In some applications, each node (except for the root node) has one and only one parent. This more constrained structure is known as a generalization tree. Induction in Domains of This Kind Induction of hierarchically ordered classes is organized in a way similar to binary relevance. For each node, the corresponding training set is constructed, and from this training set, the baseline learner induces a classiﬁer. By doing so, the most common approach proceeds in a top-down manner where the output of the parent class instructs the choice of the examples for the induction of a child class. Here is a way to carry this out in a domain where the classes are organized by a generalization tree. First, the entire original training set is used for the induction of the class located at the root of the tree. Next, the training set is divided into two parts, one containing training examples that belong to the root class, the other containing training examples that do not belong to this class. The lower-level classes are then induced only from the relevant training sets. A Concrete Example In the problem from , the ﬁrst step is to induce a classiﬁer for the classmachine learning. Suppose that the original training set consists of the seven examples shown in . The labels of those examples are then used to decide which examples to include in the training sets for the induction of the child classes. For instance, note that only positive examples of machine learning are included in the training sets for decision trees and k-NN classifiers. Conversely, only negative examples of machine learning are included in the training set for the induction ofprogramming. Two Major Difﬁculties to Be Aware Of The induction process is not as simple as it looks. The ﬁrst problem complicating the task is, again, the phenomenon of error propagation. Suppose an example represents a text document from the ﬁeld"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00065", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 65, "start_word": 26781, "end_word": 27068, "approx_tokens": 373, "text": "13.2 Illustration of a domain with hierarchically ordered classes Machine learning ex1 pos ex2 pos ex3 pos ex4 pos ex5 neg ex6 neg ex7 neg Decision trees k-NN Programming ex1 1 ex1 0 ex5 1 ex2 1 ex2 0 ex6 0 ex3 0 ex3 1 ex7 0 ex4 0 ex4 1 In some lower-level classes, the training sets contain only those training examples for which the parent classiﬁer returned pos; in others, only those for which the parent classiﬁer returned neg circuit analysis. If this example is mistakenly classiﬁed as belonging to machine-learning, the classiﬁer, misled by this information, will pass it on to the next classiﬁers, such as decision trees, thus potentially propagating the error down to lower levels.2 Another complication is that the training sets associated with the individual nodes in the hierarchy are almost always heavilyimbalanced. Again, appropriate measures have to be taken—usually undersampling or oversampling. Where Does the Class Hierarchy Come From? In some rare applications, the complete class hierarchy is available right from the start, having been created manually by the customer who has the requisite background knowledge about the concrete domain. This is the case of some well-known applications from the ﬁeld of text categorization. Caution is needed, though. Customers are not infallible, and the hierarchies they develop often miss important details. They may suffer from subjectivity—with consequences similar to those explained when we discussed classiﬁer chains. In some domains, only parts of the hierarchy are known. In this event, the engineer has to ﬁnd a way of incorporating this partial knowledge in the binary relevance framework discussed earlier. 2The reader has noticed that the issue is similar to the one we have encountered in the section dealing with classiﬁer chains."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00066", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 66, "start_word": 27068, "end_word": 27493, "approx_tokens": 552, "text": "13.3 In a domain with a manageable number of class-label combinations, it is often possible to treat each combination as a separate class Classes ex1 C1, C2 ex2 C2 ex3 C1, C3 ex4 C2, C3 ex5 C1 C1 C2 C1 AND C2 C1 AND C3 C2 AND C3 ex1 0 ex1 0 ex1 1 ex1 0 ex1 0 ex2 0 ex2 1 ex2 0 ex2 0 ex2 0 ex3 0 ex3 0 ex3 0 ex3 1 ex3 0 ex4 0 ex4 0 ex4 0 ex4 0 ex4 1 ex5 1 ex5 0 ex5 0 ex5 0 ex5 0 they recommend. If two or more classiﬁers return 1, the master classiﬁer simply chooses the one with the highest conﬁdence. The choice is more complicated in the case of classiﬁers that only return 1 or 0 without offering any information about their conﬁdence in the given decision. In principle, one may consider merging the sets of classes. For example, suppose that, for some example x, two classiﬁers return 1, and that one of the classiﬁers is associated with classes C 1; C3, and C4, and the other is associated with classes C3 and C5. In this event, x will be labeled with C1; C3; C4, and C5. Note, however, that this may easily result in x being labeled with “too many” classes. The reader already knows that this may give rise to many false positives, and thus lead to low precision. Alternative Ways of Aggregation In the approach illustrated in ,t h e leftmost table (the one headed by C1) contains only one positive label because there is only one training example in T labeled solely with this class. If we want to avoid having to deal with training sets that are so extremely imbalanced, we need a “trick” that would improve the class representations in Ti’s. Here is one possibility. In Ti, we will label with 1 each example whose set of class labels in the original T contains C1. By doing so, we must not forget that ex1 will thus be labeled as positive also in the table headed with (C1 AND C2). Similarly, we will label with 1 all subsets of the set of classes found in a given training set. For instance, if an example is labeled withC1, C3, and C4, we will label it with 1 in all training sets that represent nonempty subsets of {C1, C3, C4}. This, of course, improves only training sets for relatively “small” combinations (combining, say, only one or two classes). For larger combinations, the problem persists."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00067", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 67, "start_word": 27493, "end_word": 27776, "approx_tokens": 367, "text": "13.7 Criteria for Performance Evaluation 267 Illustration of performance evaluation in multi-label domains The following table gives, for ﬁve testing examples, the known class labels versus the class labels returned by the classiﬁer. True Classiﬁer’s classes classes ex1 C1; C2 C1; C2; C3; ex2 C2 C2; C4; ex3 C1; C3; C5 C1; C5; ex4 C2; C3 C2; C3; ex5 C2; C4 C2; C5; Separately for each class, here are the values of true positives, false positives, and false negatives. Next to them are the corresponding values for precision and recall, again separately for each class. NTP1 D 2 NFP1 D 0 NFN1 D 0 Pr1 D 2 2C0 D 1 Re1 D 2 2C0 D 1 NTP2 D 4 NFP2 D 0 NFN2 D 0 Pr2 D 4 4C0 D 1 Re2 D 4 4C0 D 1 NTP3 D 1 NFP3 D 1 NFN3 D 1 Pr3 D 1 1C1 D 0:5 Re3 D 1 1C1 D 0:5 NTP4 D 0 NFP4 D 1 NFN4 D 1 Pr4 D 0 0C1 D 0 Re4 D 0 0C1 D 0 NTP5 D 1 NFP5 D 1 NFN5 D 0 Pr5 D 1 1C1 D 0:5 Re5 D 1 1C0 D 1 This is how the macro-averages are calculated: PrM D 1C1C0:5C0C0:5 5 D 0:6 ReM D 1C1C0:5C0C1 5 D 0:7 Here is how the micro-averages are calculated: Pr/SYND 2C4C1C0C1 .2C0/C.4C0/C.1C1/C.0C1/C.1C1/ D 0:73 Re/SYND 2C4C1C0C1 .2C0/C.4C0/C.1C1/C.0C1/C.1C0/ D 0:8 These discrepancies are then reﬂected in the numbers of true positives, false positives, and false negatives. These, in turn, make it possible to calculate for each class its precision and recall. After this, the table shows the calculations of the macro- and micro-averages of these two criteria."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00068", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 68, "start_word": 27776, "end_word": 28093, "approx_tokens": 412, "text": "13.5 An example of a multi-label domain True classes ex1 C1 ex2 C1; C2 ex3 C1, C3 ex4 C2, C3 ex5 C2 ex6 C1 Classiﬁer’s classes C1; C2 C1; C2 C1 C2, C3 C2 C1; C2 Exercises 1. Consider the multi-label training set shown in the left part of . Show how the auxiliary training sets will be created when the principle of binary relevance is to be used. 2. For the same training set, create the auxiliary training sets for the approach known as class aggregation. How many such sets will we need? 3. Draw the schema showing how the problem from would be addressed by stacking. Suppose the examples in the original training set are described by ten attributes. How many attributes will the lower-level classiﬁers have to use? 4. Suggest the classiﬁer-chain schema for a domain with the following four classes: decision trees, machine learning, classification, pruning. 5. Returning to the set of examples from , suppose that a classiﬁer has labeled them as indicated in the rightmost column. Calculate the macro- and micro-averages of precision and recall. Give It Some Thought 1. Suggest a multi-label domain where the principle of classiﬁer chain can be a reasonable strategy to follow. What would be the main requirement for such data? 2. Consider a domain where the majority of training examples are labeled each with only a single class, and only a small subset of the examples (say, 5%) are labeled with more than one class. Suggest a machine learning approach to induce reliable classiﬁers from such data. 3. Suppose that you have a reason to assume that a few classes are marked by strong interdependence while most of the remaining classes are mutually independent. You are thinking of using the stacking approach. What is the main problem that might compromise the performance of the induced classiﬁers? Can you suggest a mechanism that overcomes this pitfall?"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00069", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 69, "start_word": 28093, "end_word": 28447, "approx_tokens": 460, "text": "A two-dimensional domain with clusters of examples height weight Visual identiﬁcation of such groups in a two-dimensional space is easy, but in four or more dimensions, humans can neither visualize the data nor see the clusters. These can only be detected by cluster-analysis algorithms. Representing Clusters by Centroids To begin with, we have to decide how the clusters are to be described. A few alternatives can be considered: it is possible to specify the clusters’ locations, sizes, boundaries, and perhaps some other aspects. But the simplest approach relies on centroids.1 If all attributes are numeric, the centroid is identiﬁed with the averages of the individual attributes. For instance, suppose a two-dimensional cluster consists of the following examples: .2; 5/; .1; 4/; .3; 6/. In this case, the centroid is described by vector .2; 5/ because the ﬁrst attribute’s average is 2C1C3 3 D 2 and the second attribute’s average is 5C4C6 3 D 5. The averages can be calculated even when the attributes are discrete if we know how to turn them into numeric ones. Here is a simple way of doing so. If the attribute can acquire three or more different values, we can replace each attribute-value pair with one boolean variable (say, season=fall, season=winter, etc.). The values of the boolean attributes are then represented by 0 or 1 instead of false and true, respectively. What Should the Clusters Be Like? Clusters should not overlap each other: each example must belong to one and only one cluster. Within the same cluster, the examples should be relatively close to each other, certainly much closer than to the examples from the other clusters. An important question will ask how many clusters the data contain. In , we noticed that the human observer discerns either three or four clusters. However, the scope of existing options is not limited to these two possibilities. At one extreme, the entire training set can be thought of as forming one big cluster; at the other, 1Machine learning professionals sometimes avoid the term “center” which might imply mathemat- ical properties that are for the speciﬁc needs of cluster analysis largely irrelevant."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00070", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 70, "start_word": 28447, "end_word": 28657, "approx_tokens": 273, "text": "13 examples are described by a single numeric attribute. Suppose the examples have been initially divided into the three groups indicated here by the vertical bars . The following sequence shows how two examples (marked by circles) are moved from one cluster to another. examples centers initial clustering + in a wrong cluster + + in a wrong cluster + + + + + + now: all examples in correct clusters After the second transfer, the clusters are perfect and the calculations can stop. Illustration of the k-means procedure in a domain with one numeric attribute Numeric Example In , a set of nine two-dimensional examples has been randomly divided into three groups (because the user speciﬁed k D 3), each containing the same number of examples. The table also provides the centroids for each group. k-means goes through these examples systematically, one by one—in this concrete case, starting with group-2. For each example, its distance from each centroid is calculated. It turns out that the ﬁrst example from group-2 already ﬁnds itself in the right cluster. However, the second example is closer to group-1 than to group-2 and, for this reason, has to be transferred from its original cluster to group- 1. After this, the affected centroids are recalculated."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00071", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 71, "start_word": 28657, "end_word": 29103, "approx_tokens": 579, "text": "(in which SD stands for summed distances)s u m su pt h e distances of all examples from their clusters’ centroids. Here, x.j/ i denotes the i-th example in the j-th cluster, K is the number of clusters,nj is the number of examples in the j-th cluster, and cj is the j-th cluster’s centroid. SD D KX jD1 njX iD1 d.x.j/ i ; cj/ (14.3) In cluster analysis, we seek to minimize SD. When calculating this quantity, we must not forget that the value obtained by Eq. ( 14.3) will go down if we increase the number of clusters (and thus decrease their average size), reaching SD D 0 in the extreme case when each cluster is identiﬁed with one and only one training example. The formula is therefore useful only if we compare solutions that have similar numbers of clusters. Using Alternative Initializations Knowing that the composition of the result- ing clusters depends on the algorithm’s initialization, we can suggest a simple improvement. We will deﬁne two or more sets of initial code vectors, and apply k-means separately to each of them. After this, we will evaluate the quality of all the alternative data partitionings thus obtained, using the criterion deﬁned by . The best solution is the one for which we get the lowest value. This solution is then retained, and the others discarded. Experimenting with Different Values of k One obvious weakness of k-means is the requirement that the user should provide the value of k. This is easier said than done because, more often than not, the engineer has no idea into how many clusters the available data naturally divide. Unless more sophisticated techniques are used (about these, see later), the only way out is to try a few different values, and then pick the best according to an appropriate criterion (such as the one deﬁned in ). As we already know, the shortcoming of this criterion is that it tends to give preference to small clusters. For this reason, data analysts often normalize the value of SD by k, the number of clusters. Post-processing: Merging and Splitting Clusters The quality of the set of clusters created by k-means can often be improved by post-processing techniques that either increase the number of clusters by splitting, or decrease the number by merging. As for merging, two neighboring clusters will be merged if their mutual distance is small. To ﬁnd out whether the distance merits the merging, we simply calculate the distance of two centroids, and then compare it with the average cluster-to-cluster distance calculated by the following sum, where c i and cj are centroids: S D X i¤ j d.ci; cj/ (14.4)"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00072", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 72, "start_word": 29103, "end_word": 29427, "approx_tokens": 421, "text": "Hierarchical aggregation after ﬁrst two steps (left)a n da f t e rﬁ r s tn i n es t e p s(right). Note how the clusters are gradually developed • What kind of clusters cannot be detected by the k-means algorithm? • What distance metric is used in hierarchical aggregation? What are the advan- tages and disadvantages of this metric? • Describe the principle of the hierarchical-aggregation approach to clustering. For what kind of clusters is it particularly suited? 14.5 Self-Organizing Feature Maps: Introduction Let us now introduce yet another approach to unsupervised learning, this time borrowing from the ﬁeld of neural networks. The technique is known as a self- organizing feature map, SOFM . 3 Another name commonly used in this context is Kohonen networks, to honor its inventor. The Idea Perhaps the best way to explain the nature of SOFM is to use, as a metaphor, the principle of physical attraction. A code vector, initially generated by a random-number generator, is subjected to the inﬂuence of a sequence of examples (attribute vectors), each “pulling” the vector in a different direction. In the long run, the code vector settles in a location that represents a compromise over all these conﬂicting forces. The whole network consists of a set of neurons arranged in a two-dimensional matrix such as the one shown in . Each node (a neuron) in this matrix represents a code vector that has the same length (the same number of attributes) as the training examples. At the bottom is an input attribute vector that is connected to all neurons in parallel. The idea is to achieve by training a situation where neighboring neurons respond similarly to similar input vectors. For the sake of simplicity, the input vector in the picture only has two attributes,x 1 and x2. In reality, it can have a great many. 3In statistics, and in neural networks, scientists often use the term feature instead of attribute."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00073", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 73, "start_word": 29427, "end_word": 29811, "approx_tokens": 499, "text": "298 15 Classiﬁers in the Form of Rulesets Twelve training examples expressed in a matrix form Crust Filling Example Shape Size Shade Size Shade Class ex1 Circle Thick Gray Thick Dark pos ex2 Circle Thick White Thick Dark pos ex3 Triangle Thick Dark Thick Gray pos ex4 Circle Thin White Thin Dark pos ex5 Square Thick Dark Thin White pos ex6 Circle Thick White Thin Dark pos ex7 Circle Thick Gray Thick White neg ex8 Square Thick White Thick Gray neg ex9 Triangle Thin Gray Thin Dark neg ex10 Circle Thick Dark Thick White neg ex11 Square Thick White Thick Dark neg ex12 Triangle Thick White Thick Gray neg the positive class. If the expression isfalse, the classiﬁer labels the example with the negative class. Importantly, the expression can be converted into the following two rules: R1: if [ (shape=circle) AND (filling-shade=dark) ] then pos. R2: if [ NOT(shape=circle) AND (crust-shade=dark) ] then pos. else neg. In the terminology of machine learning, each rule consists of an antecedent (the if -part), which in this context is a conjunction of attribute values, and a consequent (the then-part) which points to a concrete class label. Note that the consequents of both rules indicate the positive class. For an example to be labeled as positive, it is necessary that the conditions in the antecedent of at least one rule be satisﬁed. Otherwise the classiﬁer will label the example with the default class which, in this case, is neg. We will remember that when working with rulesets in domains of this kind, one must not forget to specify the default class. Simplifying Assumptions Throughout this chapter, we will rely on the following simplifying assumptions: 1. All training examples are described by discrete-valued attributes. 2. The training set is noise-free. 3. The training set is consistent: examples described by the same attribute vectors must belong to the same class. The Machine-Learning Task Our goal is an algorithm for the induction of rulesets from data that satisfy the simplifying assumptions from the previous paragraph. We will limit ourselves to rules whose consequents point to the positive class, the default always being the negative class. Since the training set is supposed to be consistent and noise-free, we will be interested in classiﬁers that correctly classify all training examples. This means that"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00074", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 74, "start_word": 29811, "end_word": 30268, "approx_tokens": 594, "text": "15.1 A Class Described By Rules 299 for each positive example, the antecedent of at least one rule will be true. For any negative example, no rule’s antecedent is true, and the example is labeled with the default (negative) class. A Rule “Covers” An Example Let us introduce one useful term: an example either is or is not covered by a rule. A simple illustration will clarify the notion. Consider the following rule: R: if (shape=circle) then pos. If we apply this rule to the examples from , we will observe that the antecedent’s condition, shape=circle, is satisﬁed by the following set of examples: fex1; ex2; ex4; ex6; ex7; ex0g. We will say that R covers these six examples. Generally speaking, a rule covers an example if the expression in the rule’s antecedent is true for this example. Note that four of the examples covered by this particular rule are positive and two are negative. Rule Specialization Suppose we modify the above rule by adding to its antecedent another condition,filling-shade=dark, obtaining the following: R1: if (shape=circle) AND (filling-shade=dark) then pos Checking R1 against the training set, we realize that it covers the following examples: fex 1; ex2; ex4; ex6g. We observe that this is a subset of the six examples originally covered by R. Conveniently, only positive (and no negative) examples are now covered. This leads us to the deﬁnition of another useful term. If a modiﬁcation of a rule’s antecedent reduces the set of covered examples to a subset, we say that the modiﬁcation has specialized the rule. In other words, specialization narrows the set of covered examples to a proper subset. A typical way of specializing a rule is to add a new condition to the rule’s antecedent. Rule Generalization Conversely, a rule is generalized if its modiﬁcation enlarges the set of covered examples to a superset—if the new version covers all examples that were covered by the previous version, plus some additional ones. The easiest way to generalize a rule is by removing a condition from its antecedent. For instance, this happens when we drop from rule R1 the condition (filling-shade=dark). Specialization and Generalization of Rulesets We have said we are interested in induction of rulesets that label an example with the positive class if the antecedent of at least one rule istrue for the example. For instance, this is the case of the ruleset consisting of the rules R1 and R2 above. If we remove one rule from a ruleset, the ruleset may no longer cover some of the previously covered examples. This, we already know, is called specialization. Conversely, adding a new rule to the ruleset will generalize the ruleset because the new rule will add to the set of covered examples."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00075", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 75, "start_word": 30268, "end_word": 30681, "approx_tokens": 536, "text": "15.2 Inducing Rulesets by Sequential Covering 301 The sequential covering algorithm Input: training set T. Sequential covering. Create an empty ruleset. While at least one positive example remains in T: 1. Create a rule using the algorithm below. 2. Remove from T all examples that satisfy the rule’s antecedent. 3. Add the rule to the ruleset. Create a single rule Create an initial version of the rule, R: if () then pos 1. If R does not cover any negative example, stop. 2. Add to R’s antecedent a condition, ai D vj, and return to the previous step. results in a rule that covers four positive and two negative examples. Adding one more condition, filling-shade=dark, specializes the rule so that, while still covering the four positive examples, it now no longer covers any negative example. We have obtained a rule that covers examples fex 1; ex2; ex4; ex6g. Note that is the rule R1 from the previous section. If we remove these four examples from the training set, we are left with only two positive examples, ex3 and ex5. The development of another rule again starts from the most general version (empty antecedent). Suppose that we then choose shape=triangle as the initial condition. This covers one positive and two negative examples. Adding to the antecedent the term filling-shade=dark, we succeed in excluding the negative examples while retaining the coverage of the positive example ex3, which can now be removed from the training set. After the creation of this second rule, we are left with one positive exampleex5. We therefore have to create yet another rule whose task will be to cover ex5 without covering any negative example. Once we ﬁnd such rule, ex5 is removed from the training set. After this, we observe that there are no positive examples left, and the procedure can stop. We have created a ruleset consisting of three rules that cover all positive examples and no negative examples. How to Identify the Best Attribute-Value Pair In the previous example, we always chose the condition to be added to the rule’s antecedent more or less at random. But seeing that we could have selected it from quite a few alternatives, we realize that we need a mechanism capable of informing us about the quality of each choice. Perhaps the most natural criterion to be used here is based on information theory, a principle we have encountered in where we used it in the course of induction of decision trees."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00076", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 76, "start_word": 30681, "end_word": 31158, "approx_tokens": 620, "text": "15.3 Predicates and Recursion 303 Predicates: Greater Expressive Power Than Attributes A serious limitation of attribute-value logic is that it is not sufﬁciently ﬂexible to capture certain relations among data. For instance, the fact that y is located between x and z can be stated using the predicatebetween(x,y,z)—more accurately, the predicate is the term “between,” whereas “(x,y,z)” is a list of the predicate’s arguments. The reader will agree that trying to express the same relation by means of attributes and their values would be difﬁcult to say the least. An attribute can be seen as a special case of a one-argument predicate. For instance, the fact that, for a given example,x, the shape is circular can be written as circular(x).B u t the analogy is no longer as obvious in the case of predicates with more arguments. Induction of Rules in Predicate Calculus Here is an example of a rule that says that if x is a parent of y, and at the same time x is a woman, then this parent is actuallyy’s mother: if parent(x,y) ANDfemale(x) thenmother(x,y) We can see that this rule has the same structure as the rules R1 and R2 we have seen above: a list of conditions in the antecedent followed by a consequent. And indeed, the same sequential covering algorithm can be used here. There is one difference, though. When choosing among candidate predicates to be added to antecedent, we must not forget that the meaning of the predicate changes if we change the arguments. For instance, the previous rule’s meaning will change if we replaceparent(x,y) withparent(x,z) because, in this case, the fact that x is a parent ofz surely does not guarantee thatx is mother of some other subject,y. Rulesets Allow Recursive Deﬁnitions The rules can be more interesting than the toy domain from might lead us to believe. For one thing, they can be recursive—which is the case of the following two rules deﬁning the term ancestor. if parent(x,y) thenancestor(x,y). if parent(x,z) ANDancestor(z,y) thenancestor(x,y). The meaning of two rules is easy to see. Ancestor is a parent, or at least the parent’s ancestor. For instance, a grandparent is the parent of a parent—and therefore an ancestor. A Concrete Example of Induction Let us illustrate induction of rulesets using the problem from . Here, two concepts (classes), parent andancestor, are characterized by a list of positive examples under the assumption that any example that is not in this list should be regarded as a negative example. Our goal is to induce the deﬁnition ofancestor, using the predicateparent. We begin with the most-general rule, if () then ancestor(x,y). In the next step, we want to add a condition to the antecedent. To this end, we may consider various possibilities, but the simplest appears to be parent(x,y)—which will also be supported by the information-gain criterion. We have obtained the following rule:"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00077", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 77, "start_word": 31158, "end_word": 31491, "approx_tokens": 432, "text": "The genetic algorithm’s endless loop. Each individual in the population has its chance of survival. Recombination of the genetic information provided by mating partners creates new chromosomes that may be corrupted by mutation population survivors a wheel of mutation recombination mating fortune The Genetic Algorithm’s Loop The genetic algorithm operates in an endless loop depicted in . At each moment, there is a population of individuals, each with a certain value of the ﬁtness function. This value then determines the size of the segment belonging to the individual in a “wheel of fortune” that determines the individual’s chances of survival. It is important to understand the probabilistic nature of the process. While an individual with a larger segment enjoys a higher chance of survival, there is no guarantee of it because the survival game is non-deterministic. In the real world, too, a specimen with excellent genes may perish in a silly accident, while a weakling can make it by mere good luck. But in the long run, and in large populations, the laws of probability will favor genes that contribute to high ﬁtness. The surviving specimens will then choose “mating partners.” In the process of mating, the chromosomes of the participating individuals are recombined (see below), which gives rise to a pair of new chromosomes. These new chromosomes may subsequently be subjected to mutation, which essentially adds noise to the strings of genes. The whole principle is summarized by the pseudocode in . How the Endless Loop Works Once a new population has been created, the process enters a new cycle in which the individuals are subjected to the same wheel of fortune, followed by mating, recombination, and mutation, and the story goes on and on until stopped by an appropriate termination criterion. Note how occasional wrong turns are eliminated by the probabilistic nature of process. A low-quality chromosome may survive the wheel of fortune by a ﬂuke; but if its children’s ﬁtness values remain low, the genes will perish in subsequent generations"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00078", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 78, "start_word": 31491, "end_word": 31948, "approx_tokens": 594, "text": "After exchanging 4-bit tails, two parent chromosomes (upper strings) give rise to two children (lower strings). There is a chance that at least one child will “outperform” both parents x f(x) 0011 1001 0011 1110 0101 1001 0101 1110 show, respectively, the new binary strings, their decadic values, and the values of f .x/. Note that both the average and the maximum value of the ﬁtness function have increased. Do Children Have to Outperform Their Parents? Let us ask what caused this improvement. An intuitive answer is illustrated in that shows the location of two parents and the values of the survival function, f .x/, for each of them (the dashed vertical lines). When the two chromosomes swap their 4-bit tails, two children are created, each relatively close to one of the parents. The fact that each child ﬁnds itself in a region where the values of f .x/ are higher than those of the parents begs the question: are children always more ﬁt than their parents? Far from that. All depends on the length of the exchanged tails and on the shape of the ﬁtness function. Imagine that in the next generation the same two children get paired with each other and that the randomly generated crossover point is at the same location. Then, these children’s children will be identical to the two original strings (their “grandparents”); this means that the survival chances decreased back to the original values. Sometimes, both children outperform their parents; in other cases, they are weaker than their parents; and quite often, we get a mixed bag. What matters is that in a sufﬁciently large population, most of the better specimens will survive because the selection process favors individuals with higher ﬁtness, f .x/. Unﬁt specimens will occasionally make it, but they tend to lose in the long run. If the exchanged string-tails are short, the children are close to their parent chromosomes. Long tails will give rise to children much less similar to their parents. As for mutation, its impact on the distance between the child and its parent depends on which bit is mutated. If it is the leftmost bit, the mutation will cause a big jump along the horizontal axis. If it is the rightmost bit, the jump is short. Either way, mutation complements recombination. Whereas the latter tends to explore the space in the vicinity of the parent chromosomes, the former may look elsewhere. The Shape of the Fitness Function Some potential pitfalls inherent in the deﬁnition of the ﬁtness functions are illustrated in . The function on the left is almost ﬂat. The fact that different individuals have here virtually the same chances to survive defeats the purpose of the survival game. When the survivors are"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00079", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 79, "start_word": 31948, "end_word": 32392, "approx_tokens": 577, "text": "ﬁtness value achieved so far, and then terminate the search when this value no longer improves. There is a catch, though. The fact that the ﬁtness value has reached a plateau may not guarantee that a solution has been found. Rather, the search might have reached the stage called premature degeneration. Suppose that the search from has reached the following population: 01000 01001 01000 01000 What are the chances of improving this population? Recombination will not get us anywhere. If the (identical) last two chromosomes mate, the children will only be copies of the parents. If the ﬁrst two are paired, then 1-point crossover will only swap the rightmost bit, an operation that does not create a new chromosome, either. The only way to cause a change is to use mutation. By changing the appropriate bits, mutation can reignite the search. For instance, this will happen after the mutation of the third bit in the ﬁrst chromosome and the fourth bit (from the left) of the last chromosome. Unfortunately, mutations are rare, and to wait for this to happen may be impractical. For all practical purposes, premature degeneration means the search got stuck. Preventing Premature Degeneration Premature degeneration has a lot to do with the population’s diversity. The worst population is one in which all chromosomes have exactly the same bit string, something the engineer wants to avoid. Any computer implementation will therefore beneﬁt from a module that monitors diversity and takes action whenever it drops below a certain level. A simple way to identify this situation is to calculate the average similarity between pairs of chromosomes, perhaps by counting the number of bits that have the same value in both strings. For instance, the similarity between[ 00100 ] and0110 0] will be 4 (four bits are equal) and the similarity between [ 01010 ] and [ 10101 ] will be 0. Once a drop in average chromosome-to-chromosome similarity has been detected, the system has to react. This is not yet a cause for alarm. Thus in the function-maximization example, advanced generations will be marked by populations where most specimens are already close to the maximum. This kind of “degeneration” will certainly not be deemed “premature.” However, the situation is different if the best chromosome can be shown to be very different from the solution. In this event, we have to increase diversity. Increasing Diversity Several strategies can be used. The simplest will just insert in the current population one or more newly created random individuals. A more sophisticated approach will run the genetic algorithm on two or more populations in parallel, in isolation from each other. Then, either at random intervals, or whenever"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00080", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 80, "start_word": 32392, "end_word": 32794, "approx_tokens": 522, "text": "chromosome. In this event, each parent will “trade” a different substring of its chromosome as indicated below. 1 101 1001 001 001 11 ) 1 001 1001 001 101 11 Random Bit Exchange Yet another variation on the chromosome-recombination theme is the so-called random bit exchange . Here, the random-number generator selects a user-speciﬁed number of locations, and then swaps the bits at these locations as illustrated below. 11011001 00100111 ) 10011101 01100011 Here, the second and the sixth bits (counting from the left) were swapped. Note that nothing will happen if the leftmost bit is exchanged because it has the same value in both chromosomes. The number of exchanged bits can vary but most applications prefer the number to be much smaller than the chromosome’s length. A common practice in realistic applications is to combine two or more recombi- nation operators. For instance, the selected pair of parents will with 50% probability be subjected to a 2-point chromosome, with 30% probability to a random bit exchange, and with 20% probability there will be no recombination at all. Inversion Whereas the recombination operators act on pairs of chromosomes, other operators act on single specimens. One such operator is mutation; another is inversion. In a typical implementation, the random-number generator returns two integers that deﬁne two locations in the binary string (similarly as in the 2-point crossover). Then, the substring between the two positions is inverted as shown below. 110 110 01 ) 110 011 01 Note that the order of the zeros and ones in the substring between the third and the seventh bit (counting from the left) was reversed. The location of the two points determines how much inversion impacts the chromosome. If the two integers are close to each other, say, 4 and 7, then only a small part of the chromosome is affected. In advanced implementations, inversion is used to supplement mutation. For instance, the probability that a given bit is mutated can be set to 0.2% whereas each chromosome may have a 0.7% chance to see its random substring inverted. Similarly as with mutation, care has to be taken to make sure the inversion operator is used rarely. Excessive use may destroy the positive contribution of recombination. Inversion and Premature Degeneration Much more than mutation, inversion is very good at extricating the genetic search from premature degeneration. To see why, take a look at the following degenerated population."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00081", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 81, "start_word": 32794, "end_word": 33139, "approx_tokens": 448, "text": "16.6 Some Advanced Versions 323 At r e e representation of a candidate expression from the “pies” domain shape = circle crust−size =thick crust−shade = grey The situation is slightly different if the chromosomes have the form of strings of symbols. Here, mutation can replace a randomly selected symbol in the chromo- some with another symbol chosen by the random-number generator. For instance, when applied to chromosome[ dsrdwkl ] , the mutation can change from r tos the third symbol from the left, the resulting chromosome being [ dssd wkl ] . Also possible are “mixed” chromosomes where some locations are binary, others numeric, and yet others symbolic. Here, mutation is usually implemented as a combination of the individual approaches. For instance, the program selects a random location in the chromosome, determines whether the location is binary, numeric, or symbolic, and then applies the appropriate type of mutation. Chromosomes Implemented as Tree Structures In some applications, strings of bits, numbers, or symbols are inadequate; a tree-structure may then be more ﬂexible. This, for instance, is the case of classiﬁers in the form of logical expressions—see the example in where the following expression is represented by a tree-like chromosome. (shape=circle ^ crust-size=thick) _: crust-shade=gray The expression consists of attributes, the values of these attributes, and the logical operators of conjunction, disjunction, and negation. Note how naturally this is cast in the tree structure. The internal nodes represent the logical operations and the leaves contain the attribute-value pairs Recombination swaps random subtrees. Mutation can affect the leaves: either attribute names or attribute values or both. Another possibility for mutation is occasionally to replace ^ with _ or the other way round. Special attention has to be paid to the way the initial population is generated. The programmer has to make sure that the population already contains some promising expressions. One possibility is to create a group of random expressions and to insert in it the descriptions of the positive examples. The survival function (to be maximized) can be deﬁned as the classiﬁcation accuracy on the training set."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00082", "source": "/workspace/Disertatie/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "chunk_index": 82, "start_word": 33139, "end_word": 33590, "approx_tokens": 586, "text": "The generic problem: which of the slot machines offers the highest average return? In theory, this should be easy. Why not simply try each machine many times, observe the returns, and then choose the one where these returns have been highest? In reality, though, this is not a good idea. Too many coins may have to be wasted before a reliable decision about the best machine can be made. A Simple Strategy Mindful of the incurred costs, the practically minded engineer will limit the experimentation, and make an initial choice based on just a few trials. Knowing that this early decision is unreliable, she will not be dogmatic. She will occasionally experiment with the other machines: what if some of them might indeed be better? If yes, it will be quite reasonable to replace the “previously best” with this new one. The strategy is quite natural. One does not have to be machine- learning scientist to come up with something of this kind. This then is the behavior that the reinforcement learning paradigm seeks to emulate. In the speciﬁc case from , there are ﬁve actions to choose from. The principle described above combines exploitation of the machine currently believed to be the best, and the exploration of alternatives. Exploitation dominates; exploration is rare. In the simplest implementation, the frequency of the exploration steps is controlled by a user-speciﬁed parameter,/SI. For instance,/SID 0:1 means that the “best” machine (the one that appears best in view of previous trials) is chosen 90% of the time; in the remaining 10% cases, a chance is given to a randomly selected other machine. Keeping a Tally of the Rewards The “best action” is deﬁned as the one that has led to the highest average return. 1 For each action, the learner keeps a tally of the previous returns; and the average of these returns is regarded as this action’squality. For instance, let us refer to the machines in by integers, 1; 2; 3; 4, and 5. Action ai then represents the choice of the i-th machine. Suppose the leftmost machine was chosen three times, and these choices resulted in the following returns r1 D 0; r2 D 9, and r3 D 3. The quality of this particular choice is then Q.a1/ D .r1 C r2 C r3/=3 D .0 C 9 C 3/=3 D 4. To avoid the necessity to store the rewards of all previously taken actions, the engineer implementing the procedure can take advantage of the following formula where Q k.a/ is the quality of action a as calculated from k rewards, and rkC1 is the .k C 1/st reward. 1At this point, let us remark that the returns can be negative—”punishments,” rather."}
