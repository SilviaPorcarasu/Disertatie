{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00000", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "Miroslav/uni00A0Kubat An Introduction to Machine Learning Second Edition"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00001", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "An Introduction to Machine Learning"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00002", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "Miroslav Kubat An Introduction to Machine Learning Second Edition 123"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00003", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:library of congress", "text": "Miroslav Kubat Department of Electrical and Computer Engineering University of Miami Coral Gables, FL, USA ISBN 978-3-319-63912-3 ISBN 978-3-319-63913-0 (eBook) DOI 10.1007/978-3-319-63913-0 Library of Congress Control Number: 2017949183 © Springer International Publishing AG 2015, 2017 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the ma"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00004", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "To my wife, Verunka."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00005", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "Contents 1 A Simple Machine-Learning Task ........................................ 1 1.1 Training Sets and Classiﬁers ......................................... 1 1.2 Minor Digression: Hill-Climbing Search ........................... 5 1.3 Hill Climbing in Machine Learning ................................. 8 1.4 The Induced Classiﬁer’s Performance ............................... 11 1.5 Some Difﬁcul"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00006", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "viii Contents 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers ........... 65 4.1 The Essence .......................................................... 65 4.2 The Additive Rule: Perceptron Learning ............................ 69 4.3 The Multiplicative Rule: WINNOW ................................ 73 4.4 Domains with More Than Two Classes ............................. 76 4.5 Polynomial "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00007", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "Contents ix 8.6 Text Classiﬁcation .................................................... 167 8.7 Summary and Historical Remarks ................................... 169 8.8 Exercises and Thought Experiments ................................ 170 9 Induction of Voting Assemblies ........................................... 173 9.1 Bagging ............................................................... 1"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00008", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "x Contents 13.4 Another Possibility: Stacking ........................................ 258 13.5 A Note on Hierarchically Ordered Classes .......................... 260 13.6 Aggregating the Classes ............................................. 263 13.7 Criteria for Performance Evaluation ................................. 265 13.8 Summary and Historical Remarks ................................... 26"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00010", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "structure, and style of a teaching text meant to provide the material for a one-semester introductory course. 1Edited by R. Michalski, J. Carbonell, and T. Mitchell. 2T. Mitchell, Machine Learning, McGraw-Hill (1997)."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00012", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 1 A Simple Machine-Learning Task You will ﬁnd it difﬁcult to describe your mother’s face accurately enough for your friend to recognize her in a supermarket. But if you show him a few of her photos, he will immediately spot the tell-tale traits he needs. As they say, a picture—an example—is worth a thousand words. This is what we want our technology to emulate. Unable to deﬁne certain obje"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00015", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "4 1 A Simple Machine-Learning Task Problems with a Brute-Force Approach How does a machine ﬁnd a classiﬁer of this kind? Brute force (something that computers are so good at) will not do here. Just consider how many different examples can be distinguished by the given set of attributes in the “pies” domain. For each of the three different shapes, there are two alternativecrust-sizes, the number of"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00016", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "problem is characterized by an initial state, ﬁnal state, search operators, and a search strategy"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00017", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "6 1 A Simple Machine-Learning Task The ﬂowchart in Fig. 1.3 starts with a concrete initial state, in which we can choose between two operators: “move tile-6 up” and “move tile-2 to the left.” The choice is guided by an evaluation function that estimates for each state its distance from the goal. A simple possibility is to count the squares that the tiles have to traverse before reaching their ﬁnal"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00018", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "its name. 1For simplicity, the pseudocode ignores termination criteria other than reaching, or failing to reach, the ﬁnal state."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00019", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "8 1 A Simple Machine-Learning Task Table 1.2 Hill-climbing search algorithm 1. Create two lists, L and Lseen. At the beginning, L contains only the initial state, and Lseen is empty. 2. Let n be the ﬁrst element of L. Compare this state with the ﬁnal state. If they are identical, stop with success. 3. Apply to n all available search operators, thus obtaining a set of new states. Discard those stat"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00020", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "1.3 Hill Climbing in Machine Learning 9 shape = circle e = 4/12 shape = circle e = 2/12 fill shade = dark shape = circle e = 6/12 fill size = thick shape = circle e = 5/12 crust size = thick Addition of (AND) e = 1/12 fill shade = dark ) e = 3/12 e = 2/12 Addition of (OR) crust shade = dark fill shade = dark ) shape = triangle fill size = thin 1 2 3 ( shape = circle fill shade = dark ) (shape = ci"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00021", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "10 1 A Simple Machine-Learning Task Price Weight ($3, 1.2lb) Price Weight ($3, 1.2lb) Fig. 1.5 On the left: a domain with continuous attributes; on the right: some “circular” classiﬁers Hill Climbing in a Domain with Numeric Attributes Initial State A circle is deﬁned by its center and radius. We can identify the initial center with a randomly selected positive example, making the initial radius s"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00023", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "12 1 A Simple Machine-Learning Task The Need for Explanations In some applications, establishing the class of each example is not enough. Just as desirable is to know the reasons behind the classiﬁcation. Thus a patient is unlikely to give consent to amputation if the only argument in support of surgery is, “this is what our computer says.” But how to ﬁnd a better explanation? In the “pies” domain"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00025", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "14 1 A Simple Machine-Learning Task because it can be calculated by subtracting date-of-birth from today’s date. Fortunately, redundant attributes are less dangerous than irrelevant or missing ones. Missing Attribute Values In some applications, the user has no problems identi- fying the right choice of attributes. The problem is, however, that the value of some attributes are not known. For insta"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00026", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "1.6 Summary and Historical Remarks 15 Learning: Application: TrainingConcept ClassiﬁerExamples Classiﬁer ConceptExample Label Fig. 1.7 The training examples are used to induce a classiﬁer. The classiﬁer is then employed to classify future examples • What is meant by “inconsistent training set”? What can be the cause? How can it affect the learning process? • What kinds of noise do we know? What ar"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00027", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "16 1 A Simple Machine-Learning Task Historical Remarks The idea of casting the machine-learning task as search was popular in the 1980s and 1990s. While several “founding fathers” came to see things this way independently of each other, Mitchell [67] is often credited with being the ﬁrst to promote the search-based approach; just as inﬂuential, however, was the family of AQ-algorithms proposed by "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00028", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "1.7 Solidify Your Knowledge 17 Exercises 1. In the sliding-tiles puzzle, suggest a better evaluation function than the one used in the text. 2. Figure 1.8 shows a search tree where each node represents one search state and is tagged with the value of the evaluation function. In what order will these states be visited by hill-climbing search? 3. Suppose the evaluation function in the “pies” domain "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00029", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:computer assignments", "text": "18 1 A Simple Machine-Learning Task Computer Assignments 1. Write a program implementing hill climbing and apply it to the sliding-tiles puzzle. Choose appropriate representation for the search states, write a module that decides whether a state is a ﬁnal state, and implement the search operators. Deﬁne two or three alternative evaluation functions and observe how each of them leads to a different"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00030", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 2 Probabilities: Bayesian Classiﬁers The earliest attempts to predict an example’s class based on the known attribute values go back to well before World War II—prehistory, by the standards of computer science. Of course, nobody used the term “machine learning,” in those days, but the goal was essentially the same as the one addressed in this book. Here is the essence of the solution strat"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00032", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "2.1 The Single-Attribute Case 21 A Concrete Example Figure 2.1 illustrates the terms. The rectangle represents all pies. The positive examples are contained in one circle and those with filling-size=thick in the other; the intersection contains three instances that satisfy both conditions; one pie satisﬁes neither, and is therefore left outside both circles. The conditional probability, P.posjthic"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00033", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "22 2 Probabilities: Bayesian Classiﬁers Comparison of the values calculated by these two formulas will tell us which class,pos ofneg, is more probable. Things are simpler than they look: since the denominator, P.thick/, is the same for both classes, we can just as well ignore it and simply choose the class for which the numerator is higher. A Trivial Numeric Example That this formula leads to corr"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00035", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "other are rare. No wonder that the above-described approach is known under the unﬂattering name, Naive Bayes."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00036", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "2.2 Vectors of Discrete Attributes 25 Yet practical experience is not bad at all. True, the violation of the “independence requirement” renders the probability estimates inaccurate. However, this does not necessarily make them point to the wrong classes. Remember? x is labeled with the class that maximizes P.xjci/ /SOHP.ci/. If the product’s value is 0.8 for one class and 0.2 for the other, then t"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00037", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "high_noisy_token_ratio", "text": "Based on these values, we obtain the following probabilities: P.xjpos/ D nY iD1 P.xijpos/ D 1 6 /SOH5 6 /SOH1 6 /SOH3 6 /SOH1 6 D 15 65 P.xjneg/ D nY iD1 P.xijneg/ D 2 6 /SOH5 6 /SOH2 6 /SOH1 6 /SOH2 6 D 40 65 Since P.xjpos/< P.xjneg/, we label x with the negative class."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00038", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "2.3 Probabilities of Rare Events: Exploiting the Expert’s Intuition 27 Table 2.3 Classiﬁcation with the Naive-Bayes principle The example to be classiﬁed is described by x D .x1;:::; xn/. 1. For each xi, and for each classcj, calculate the conditional probability,P.xijcj/, as the relative frequency of xi among those training examples that belong to cj. 2. For each class, cj, carry out the followin"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00040", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "2.3 Probabilities of Rare Events: Exploiting the Expert’s Intuition 29 When we use this formula to recalculate the values from Table2.4, we will realize that, after ﬁve trials, the probability is estimated as Pheads D 3C9 5C10 D 12 15 D 0:8, surely a less plausible value than the one obtained in the case of/EMheads D 0:5 where we got Pheads D 0:57. The reader is encouraged to verify that the situa"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00041", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "30 2 Probabilities: Bayesian Classiﬁers Table 2.5 An example of one reason for using m-estimates in Bayesian classiﬁcation Let us return to the “pies” domain from Table1.1. Remove from the table the ﬁrst example, then use the rest for the calculation of the probabilities. x = [shape=circle, crust-size=thick, crust-shade=gray filling-size=thick, filling-shade=dark] Let us ﬁrst calculate the probabi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00042", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "toc_dots", "text": "2.4 How to Handle Continuous Attributes 31 The loss will be mitigated if we divide the original domain into not two, but several intervals, say, .0; 10/c141; : : : .90; 100/c141.1 Suppose we get ourselves a separate bin for each of these, and place a little black ball into the i-th bin for each training example whose value ofage falls into the i-th interval. Having done so, we may reach the situat"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00043", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "32 2 Probabilities: Bayesian Classiﬁers Fig. 2.3 When using the pdf, we identify the probability of x 2 Œa; b/c141with the relative size of the area below the corresponding section of the pdf p p(x) ab Gaussian \"bell\" function age Let us be careful about notation. The discrete probability of x is indicated by an uppercase letter, P.x/.T h epdf at x is denoted by a lowercase letter, p.x/.A n di fw "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00044", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "2.5 Gaussian “Bell” Function: A Standard pdf 33 What Have You Learned? To make sure you understand the topic, try to answer the following questions. If needed, return to the appropriate place in the text. • What is the probability density function, pdf, and how does it help us in the context of Bayesian classiﬁcation? • Explain the discretization mechanism that helped us arrive at an informal deﬁn"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00045", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "34 2 Probabilities: Bayesian Classiﬁers Setting the Parameter Values To be able to use this model when approximating pci .x/ in a concrete application, we only need to estimate the values of its parameters, /SYNand /ESC2. This is easy. Suppose that class ci has m representatives among the training examples. If xi is the value of the given attribute in the i- th example, then the mean and variance,"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00046", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "2.6 Approximating PDFs with Sets of Gaussians 35 ups. There may be three peaks if it turns out that body-weight of fathers is distributed around a higher mean than that of the mothers. And the number of peaks can be higher still if the families come from diverse ethnic groups. Combining Gaussian Functions No doubt, a single bell function would misrep- resent the situation. But what if we combine t"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00047", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "36 2 Probabilities: Bayesian Classiﬁers Inspecting the gaussian formula, we realize that the choice of a very small value of /ESC2 causes great sensitivity to the difference between x and /SYNi; the individual bell functions will be “narrow,” and the resulting pdf will be marked by steep peaks separated by extended “valleys.” Conversely, the consequence of a high /ESC2 will be an almost ﬂat pdf. S"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00048", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "2.7 Summary and Historical Remarks 37 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Fig. 2.4 Composing the pdf from three examples wit"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00049", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "38 2 Probabilities: Bayesian Classiﬁers • The so-called m-estimate makes it possible to take advantage of a user’s estimate of an event’s probability. This comes handy in domains with insufﬁcient experimental evidence, where relative frequency cannot be relied on. • In domains with continuous attributes, the role of the discrete probability, P.xjci/, is taken over by pci .x/, the probability densi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00050", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "2.7 Summary and Historical Remarks 39 Fig. 2.5 Composing the pdf ’s separately for the positive and negative class (with /ESC2 D 1). Each row represents one attribute, and each of the left three columns represents one example. The rightmost column shows the composed pdf ’s procedure is the same: the example is labeled with the class that maximizes the product, pci .x/P.ci/. • The concrete shape of"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00051", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "40 2 Probabilities: Bayesian Classiﬁers [14]. The ﬁrst to use the assumption of independent attributes was Good [ 33]. The idea of approximating pdf ’s by the sum of bell functions comes from Parzen [74]. When provided with perfect information about the probabilities, the Bayesian classiﬁer is guaranteed to provide the best possible classiﬁcation accuracy. This is why it is sometimes used as a ref"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00052", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "2.8 Solidify Your Knowledge 41 Give It Some Thought 1. How would you apply the m-estimate in a domain with three possible outcomes, ŒA; B; C/c141, each with the same prior probability estimate, /EMA D /EMB D /EMC D 1=3? What if you trust your expectations of A but are not so sure about B and C?I s there a way to reﬂect this circumstance in the value of the parameter m? 2. Suggest the circumstances"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00053", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 3 Similarities: Nearest-Neighbor Classiﬁers Two plants that look very much alike probably represent the same species; likewise, it is quite common that patients complaining of similar symptoms suffer from the same disease. In short, similar objects often belong to the same class—an observation that forms the basis of a popular approach to classiﬁcation: when asked to determine the class of"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00056", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "46 3 Similarities: Nearest-Neighbor Classiﬁers What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have problems, return to the corresponding place in the preceding text. • How can we measure example-to-example similarity in domains where all attributes are discrete, and how in domains where they are all continuous? • Under what circumstance"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00057", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "3.2 Measuring Similarity 47 Table 3.3 Using the nearest-neighbor principle in a 3-dimensional Euclidean space Using the following training set of four examples described by three numeric attributes, determine the class of object x D Œ 2;4;2 /c141. Distance between exi and Œ 2;4;2 /c141 ex1 fŒ 1 ;3 ;1 /c141 ;posg p .2 /NUL1/2 C .4 /NUL3/2 C .2 /NUL1/2 D p 3 ex2 fŒ 3 ;5 ;2 /c141 ;posg p .2 /NUL3/2 C"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00058", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "48 3 Similarities: Nearest-Neighbor Classiﬁers Attribute-to-Attribute Distances Can Be Misleading We must be careful not to apply Formula (3.2) mechanically, ignoring the speciﬁc aspects of the given domain. Let us brieﬂy discuss two circumstances that make it is easy to go wrong. Suppose our examples are described by three attributes, size, price, and season. Of these, the ﬁrst two are obviously "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00059", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "3.3 Irrelevant Attributes and Scaling Problems 49 What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have problems, return to the corresponding place in the preceding text. • What is Euclidean distance, and what is Hamming distance? In what domains can they be used? How is distance related to similarity? • Write down the distance formula fo"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00060", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "50 3 Similarities: Nearest-Neighbor Classiﬁers Fig. 3.3 The “vertical” attribute is irrelevant for classiﬁcation, and yet it affects the geometric distances between examples. Object 1 is positive, even though its nearest neighbor in the 2-dimensional space is negative .1 body−temperature shoe size _ _ + + __ + _ _ _ _ _ attribute is irrelevant, then the term .x2 /NULy2/2 is superﬂuous—and yet it a"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00061", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "3.3 Irrelevant Attributes and Scaling Problems 51 The distances are dM.x;ex1/ D p 584 and dM.x;ex2/ D p 544. The latter being smaller, the 1-NN classiﬁer will label x as neg. Suppose, however, that the second attribute expressestemperature, and does so in centigrades. If we decide to use Fahrenheits instead, the three vectors will change as follows: ex1 D Œ.10; 50/;pos//c141 ex2 D Œ.20; 32/;neg//c"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00062", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "52 3 Similarities: Nearest-Neighbor Classiﬁers may not be justiﬁed. For instance, if the difference between summer andfall is 1, it will always be bigger than, say, the difference between two normalized body temperatures. Whether this matters or not is up to the engineer’s common sense— assisted by his or her experience and perhaps a little experimentation. What Have You Learned? To make sure you "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00064", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "54 3 Similarities: Nearest-Neighbor Classiﬁers Fig. 3.5 With the growing number of voting neighbors (k), the error rate of the k-NN classiﬁer decreases until it reaches a level from which it starts growing again k error rate set contains 25 training examples, then the 25-NN classiﬁer is useless because it simply labels any object with the class that has the highest number of representatives in the"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00065", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "3.5 Weighted Nearest Neighbors 55 • How does the performance of the k-NN classiﬁer compare to that of the Ideal Bayes? Summarize this separately for k D 1 and k >1 . What theoretical assumptions do these two paradigms rely on? • How will the performance of a k-NN classiﬁer depend on the growing values of k in theory and in a realistic application? • What is understood by the curse of dimensionalit"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00066", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "56 3 Similarities: Nearest-Neighbor Classiﬁers class because the combined weight of the positive neighbors,† C D 0:6C0:7 D 1:3, is greater than that of the negative neighbors, † /NUL D 0:1 C 0:2 C 0:3 D 0:6.J u s t as in Fig. 3.6, the more frequent negative label is outvoted by the positive neighbors because the latter are closer to x. A Concrete Formula Let us introduce a simple formula to calcul"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00067", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "3.6 Removing Dangerous Examples 57 What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have problems, return to the corresponding place in the preceding text. • Why did we argue that each of the voting nearest neighbors should sometimes have a different weight? • Discuss the behavior of the formula recommended in the text for the calculation"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00069", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "3.7 Removing Redundant Examples 59 Table 3.5 The algorithm to identify (and remove) Tomek Links Input: the training set of N examples 1. Let i D 1 and let T be an empty set. 2. Let x be the i-th training example and let y be the nearest neighbor of x. 3. If x and y belong to the same class, go to 5. 4. If x is the nearest neighbor of y,l e tT D T [f x; yg. 5. Let i D i C 1.I f i /DC4N, goto 2. 6. "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00070", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "60 3 Similarities: Nearest-Neighbor Classiﬁers they add to classiﬁcation costs without affecting classiﬁcation performance. This is the case of the domain shown in the upper-left corner of Fig.3.9. Consistent Subset In an attempt to reduce redundancy, we want to replace the training set, T, with itsconsistent subset, S. In our context,S is said to be a consistent subset of T if replacing T with S "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00071", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "3.8 Summary and Historical Remarks 61 Table 3.6 Algorithm to create a consistent training subset by the removal of (some) redundant examples 1. Let S contain one positive and one negative example from the training set, T. 2. Using examples from S, re-classify the examples in T with the 1-NN classiﬁer. Let M be the set of those examples that have in this manner received the wrong class. 3. Copy to "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00072", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "62 3 Similarities: Nearest-Neighbor Classiﬁers • The performance of the k-NN classiﬁer is poor if many of the attributes describing the examples are irrelevant. Another issue is the scaling of the attribute values. The latter problem can be mitigated by normalizing the attribute values to unit intervals. • Some examples are harmful in the sense that their presence in the training set increases err"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00073", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "3.9 Solidify Your Knowledge 63 3. Again, use the training examples from Table 3.7. (a) Are there any Tomek links? (b) can you ﬁnd a consistent subset of the training set by the removal of at least one redundant training example? Give It Some Thought 1. Discuss the possibility of applying the k-NN classiﬁer to the “pies” domain. Give some thought to how many nearest neighbors to use, and what dista"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00075", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers When representing the training examples with points in an n-dimensional instance space, we may realize that positive examples tend to be clustered in regions different from those occupied by negative examples. This observation motivates yet another approach to classiﬁcation. Instead of the probabilities and similarities employed by"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00076", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "66 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers − − − + x1 x 1 1 0 0 1.2 1 2 2 −1.2 + 0.5 x + x = 0 x1 x2 −1.2+0 .5 x1 + x2 Class 1 1 0.3 pos 1 0 −0.7 neg 0 1 −0.2 neg 0 0 −1.2 neg Fig. 4.1 A linear classiﬁer in a domain with two classes and two boolean attributes (using 1 instead of true and 0 instead of false) the example’s class. The table on the right shows how the four examples "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00077", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "4.1 The Essence 67 no straight line will ever succeed. Let us defer further discussion of this issue till Sect. 4.5. For the time being, we will consider only domains where the classes are linearly separable. The Parameters The classiﬁer’s behavior is determined by the coefﬁcients, wi. These are usually called weights. The task for machine learning is to ﬁnd their appropriate values. Not all the w"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00078", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "68 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers values. Consider the linear classiﬁer deﬁned by the following function: 2 C 3x1 /NUL2x2 C 0:1x4 /NUL0:5x6 D 0 (4.8) The ﬁrst thing to notice in the expression on the left side is the absence of attributes x3 and x5. These are rendered irrelevant by their zero weights, w3 D w5 D 0. As for the other attributes, their impacts depend on the"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00079", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "4.2 The Additive Rule: Perceptron Learning 69 Fig. 4.2 The linear classiﬁer outputs h.x/ D 1 whenPn iD0 wixi >0 and h.x/ D 0 whenPn iD0 wixi /DC40, signaling the example is positive or negative, respectively ih( w x )Σ w xΣ i 1 0 i i 4.2 The Additive Rule: Perceptron Learning Having developed some understanding of how the linear classiﬁer works, we are ready to take a closer look at how to induce "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00080", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "70 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers The Weight-Adjusting Formula In summary, the presentation of a training example, x, can result in three different situations. The technique based on “learning from mistakes” responds to them according to the following table: Situation Action c.x/ D 1 while h.x/ D 0 Increase wi for each attribute with xi D 1 c.x/ D 0 while h.x/ D 1 Decre"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00081", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "4.2 The Additive Rule: Perceptron Learning 71 Table 4.1 The perceptron learning algorithm Assumption: the two classes, c.x/ D 1 and c.x/ D 0, are linearly separable. 1. Initialize all weights, wi, to small random numbers. Choose an appropriate learning rate, /DC12 .0; 1/c141. 2. For each training example, x D .x1;:::; xn/, whose class is c.x/: (i) Let h.x/ D 1 ifPn iD0 wixi >0 ,a n dh.x/ D 0 other"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00082", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "72 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers of h.e1/ D 1 and c.e1/ D 0; however, this happens only to w0 and w1 because x2 D 0. In response to e2, all weights of the classiﬁer’s new version are increased because h.e2/ D 0 and c.e2/ D 1, and all attributes have xi D 1. And after e3, the fact that h.e3/ D 1 and c.e1/ D 0 results in the reduction of w0, but not of the other weights "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00083", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "4.3 The Multiplicative Rule: WINNOW 73 This remarkable sensitivity is explained by the high learning rate, /DC1D 0:5.A smaller value, such as /DC1D 0:1, would moderate the changes, thus “smoothing out” the learning. But if we overdo it by choosing, say, /DC1D 0:001, the training process will become way too slow, and a great many epochs will have to pass before all training examples are correctly c"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00084", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "74 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers Situation Action c.x/ D 1 while h.x/ D 0 wi D ˛ wi c.x/ D 0 while h.x/ D 1 wi D wi=˛ c.x/ D h.x/ Do nothing Table 4.3 The WINNOW algorithm Assumption: the two classes, c.x/ D 1 and c.x/ D 0, are linearly separable. 1. Initialize the classiﬁer’s weights to wi D 1. 2. Set the threshold to /DC2D n /NUL0:1 (n being the number of attributes)"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00085", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "4.3 The Multiplicative Rule: WINNOW 75 Table 4.4 Illustration of the WINNOW’s behavior Task. Using the training examples from the table on the left (below), induce the linear classiﬁer. Let ˛ D 2,a n dl e t/DC2D 2:9. Note that the training is here accomplished in two learning steps: presentation of e5 (false negative), and of e2 (false positive). After these two weight modiﬁcations, the resulting "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00086", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "76 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers In the new representation, the same example will be described by six attributes: x1 D 1; x2 D 0; x3 D 1; x4 D 0; x5 D 1; x6 D 0; For these, WINNOW will have to ﬁnd six weights,w1;:::; w6, or perhaps seven, if w0 is used. Comparing It with Perceptron In comparison with perceptron learning,W I N - NOW appears to converge faster in domains"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00088", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "78 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers Table 4.6 Illustration of the master classiﬁer’s behavior: choosing the example’s class from several candidates Suppose we have four binary classiﬁers (the i-th classiﬁer used for the i-th class) deﬁned by the weights listed in the table below. How shall the master classiﬁer label example x D .x1; x2; x3; x4/ D . 1 ;1 ;1 ;0 /? Classiﬁer"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00090", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "80 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers Of course, some of the weights can bewi D 0, rendering the corresponding terms “invisible” such as in 7 C 2x1x2 C 3x2 2 where the coefﬁcients of x1; x2; and x2 1 are zero. Polynomials of the r-th Order More generally, the r-th order polynomial (still in a two-dimensional domain) will consist of a sum of weighted terms, wixk 1xl 2, such "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00093", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "4.6 Speciﬁc Aspects of Polynomial Classiﬁers 83 NW D /DC2n C r r /DC3 (4.14) Of course, NW will be impractically high for large values of n. For instance, even for the relatively modest case of n D 100 attributes and a polynomial’s order r D 3, the number of weights to be trained is NW D 176;851 (103 choose 3). The computational costs thus incurred are not insurmountable for today’s computers. Wha"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00094", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "84 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers 4.7 Numerical Domains and Support Vector Machines Now that we have realized that polynomials do not call for new machine-learning algorithms, we can return to linear classiﬁers, a topic we have not yet exhausted. Time has come to abandon our self-imposed restriction to boolean attributes, and to start considering also the possibility of"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00095", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "4.7 Numerical Domains and Support Vector Machines 85 Conversely, the margin is greater in the case of the solid-line classiﬁer: the nearest positive example on one side of the line, and the nearest example on the other of the line are much farther than in the case of the other classiﬁer. As it turns out, the greater the margin, the higher the chances that the classiﬁer will do well on future data."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00096", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "86 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers Fig. 4.9 The technique of the support vector machine looks for a separating hyperplane that has the maximum margin x x 1 2 margin 4.8 Summary and Historical Remarks • Linear and polynomial classiﬁers deﬁne a decision surface that separates the positive examples from the negative. Speciﬁcally, linear classiﬁers label the examples accordi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00097", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "4.9 Solidify Your Knowledge 87 have been replaced (with the help ofmultipliers) by newly created attributes such as z3 D x2 1 or z4 D x1x2. • Polynomial classiﬁers are prone to overﬁt noisy training data. This is explained by the excessive ﬂexibility caused by the very high number of trainable weights. • The potentially best class-separating hyperplane (among the inﬁnitely many candidates) is iden"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00098", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:computer assignments", "text": "88 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers Give It Some Thought 1. How can induction of linear classiﬁers be used to identify irrelevant attributes? Hint: try to run the learning algorithm on different subsets of the attributes, and then observe the error rate achieved after a ﬁxed number of epochs. 2. Explain in what way it is true that the 1-NN classiﬁer applied to a pair of e"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00099", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "4.9 Solidify Your Knowledge 89 3. Use the same domain as in the previous assignment (ﬁve boolean attributes, and the same deﬁnition of the positive class). Add to each exampleN additional boolean attributes whose values are determined by a random-number generator. VaryN from 1 to 20. Observe how the number of example-presentations needed to achieve the zero error rate depends on N. 4. Again, use t"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00100", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "90 4 Inter-Class Boundaries: Linear and Polynomial Classiﬁers 9. Run induction of linear classiﬁers on selected boolean domains from the UCI repository3 and compare the results. 10. Experimenting with selected domains from the UCI repository, observe the impact of the learning rate, /DC1, on the convergence speed of the perceptron learning algorithm. 11. Compare the behavior of linear and polynomi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00101", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 5 Artiﬁcial Neural Networks Polynomial classiﬁers can model decision surfaces of any shape; and yet their practical utility is limited because of the easiness with which they overﬁt noisy training data, and because of the sometimes impractically high number of trainable parameters. Much more popular are artiﬁcial neural networks where many simple units, called neurons, are interconnected b"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00102", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "92 5 Artiﬁcial Neural Networks Fig. 5.1 A popular transfer function: thesigmoid 1 0.5 Σ w xΣ ii i if( w x ) xx x x y y . . . . . . 1 1 2 3 4 n output neurons input signals hidden neurons output signals Fig. 5.2 An example neural network consisting of two interconnected layers Figure 5.1 shows the curve representing the transfer function. The reader can see that f .†/ grows monotonically with the i"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00103", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:index ", "text": "5.1 Multilayer Perceptrons as Classiﬁers 93 While there is no communication between neurons of the same layer, adjacent layers are fully interconnected. Importantly, each neuron-to-neuron link is associ- ated with a weight. The weight of the link from the j-th hidden neuron to the i-th output neuron is denoted as w.1/ ji , and the weight of the link from the k-th attribute to the j-th hidden neuro"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00104", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "94 5 Artiﬁcial Neural Networks Table 5.1 Example of forward propagation in a multilayer perceptron Task. Forward-propagate x D .x1; x2/ D .0:8; 0:1/ through the network below. 0.70.5 −1.0 0.1 x2=0.1 0.9 −0.3 0.5 −0.1 y1=0.66 y2=0.45 h1=0.32 h2=0.54 x1=0.8 Solution. inputs of hidden-layer neurons: z.2/ 1 D 0:8 /STX./NUL1:0/ C 0:1 /STX0:5 D/NUL0:75 z.2/ 2 D 0:8 /STX0:1 C 0:1 /STX0:7 D 0:15 outputs o"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00105", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "5.2 Neural Network’s Error 95 tell us, though, is how many hidden neurons are needed, and what the individual weight values should be. In other words, we know that the solution exists, yet there is no guarantee we will ever ﬁnd it. What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have problems, return to the corresponding place in the pre"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00106", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "96 5 Artiﬁcial Neural Networks the second network was less certain, the differences of the output values (0.6, 0.6, and 0.7) being so small as to give rise to the suspicion that C3 has won merely by chance. Due to its weaker commitment to the incorrect class, the second network is somehow less wrong than the ﬁrst. This is the circumstance that can be captured by a more appropriate error function, "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00107", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "5.3 Backpropagation of Error 97 • In what sense do we say that the traditional error rate does not provide enough information about a neural network’s classiﬁcation accuracy? • Explain the difference between a neural network’s output, the example’s class, and the target vector. • Write down the formulas deﬁning the error rate and the mean square error. 5.3 Backpropagation of Error In the multilaye"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00108", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "is calculated as follows:"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00109", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "5.3 Backpropagation of Error 99 Output-layer neurons: ı .1/ i D yi.1 /NULyi/.ti /NULyi/ Here, .ti /NULyi/ is the difference between the i-th output and the corresponding target value. This difference is multiplied by yi.1 /NULyi/, a term whose minimum value is reached when y1 D 0 or yi D 1 (a “strong opinion” as to whether x should or should not be labeled with thei-th class); the term is maximize"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00110", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "100 5 Artiﬁcial Neural Networks Table 5.2 Backpropagation of error in a neural network with one hidden layer 1. Present example x to the input layer and propagate it through the network. 2. Let y D .y1;::: ym/ be the output vector, and let t.x/ D .t1;::: tm/ be the target vector. 3. For each output neuron, calculate its responsibility, ı .1/ i , for the network’s error: ı .1/ i D yi.1 /NULyi/.ti /"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00111", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "5.4 Special Aspects of Multilayer Perceptrons 101 Suppose the training set consists of 105 examples, and suppose that the training will continue for 104 epochs. In this event, the number of weight-updates will be 104 /STX105 /STX104 D 1013. This looks like a whole lot, but many applications are even more demanding. Some fairly ingenious methods to make the training more efﬁcient have therefore bee"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00112", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "102 5 Artiﬁcial Neural Networks Table 5.3 (continued) Backpropagation of error (cont. from the previous page) The target vector being t.x/ D .1; 0/, and the output vector y D .0:65; 0:59/, the task is to establish each neuron’s responsibility for the output error. Here are the calculations for the output neurons: • ı .1/ 1 D y1.1 /NULy1/.t1 /NULy1/ D 0:65.1 /NUL0:65/.1 /NUL0:65/ D 0:0796 ı .1/ 2 D"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00113", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "5.4 Special Aspects of Multilayer Perceptrons 103 Local Minima Figure 5.3 illustrated the main drawback of the gradient-descent approach when adopted by multilayer perceptron training. The weights are changed in a way that guarantees descent along the steepest slope. But once the bottom of a local minimum has been reached, there is nowhere else to go—which is awkward: after all, the ultimate goal "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00114", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "104 5 Artiﬁcial Neural Networks For small multilayer perceptrons, this problem is not as painful; they are not ﬂexible enough to overﬁt. But as the number of hidden neurons increases, the network gains in ﬂexibility, and overﬁtting can become a real concern. However, as we will learn in the next section, this doesnot mean that we should always prefer small networks! There is a simple way to discov"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00115", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "of its insufﬁcient ﬂexibility that makes correct classiﬁcation impossible, or because it “fell” into a local minimum."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00116", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "106 5 Artiﬁcial Neural Networks Fig. 5.5 When the training-set mean square error (MSE) does not seem to go down, further improvement can be achieved by adding new hidden neurons. In this particular case, this has been done twice number of epochs MSE When this is observed, a few more neurons with randomly initialized weights are added, and the training is resumed. Usually, the added ﬂexibility make"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00117", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "5.6 Radial-Basis Function Networks 107 output layer yy1K gaussian functions ϕ1ϕ ϕ2 m . . . 1 . . . xx1 2 Fig. 5.6 Radial-basis function network classes that are not linearly separable. In the context of the neural networks, however, this limitation is not necessarily hurtful. Thing is, the original examples have been transformed by the sigmoid functions in the hidden layer. Consequently, the neuro"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00118", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "108 5 Artiﬁcial Neural Networks neuron to the i-th output neuron (the weights w0i are connected to a ﬁxed ' 0 D 1). This output signal being interpreted as the amount of evidence supporting the i-th class, the example is labeled with the i-th class if yi D maxk.yk/. Output-Layer Weights It is relatively simple to establish the output-layerweights, wij. Since there is only one layer of weights to b"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00119", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "5.7 Summary and Historical Remarks 109 • How many weights need to be trained in a radial-basis function network? How can the training be accomplished? • What are the possibilities for the construction of the gaussian hidden layer? 5.7 Summary and Historical Remarks • The basic unit of a multilayer perceptron is a neuron. The neuron accepts a weighted sum of inputs, and subjects this sum to a trans"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00120", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "110 5 Artiﬁcial Neural Networks • Certain aspects of backpropagation by error have been discussed here. Among these, the most important are computational costs, the existence of local minima, adaptive learning rate, the danger of overﬁtting, and the problems of how to determine the size of the hidden layer. • An alternative is the radial-basis function (RBF) network. For the transfer function at t"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00121", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "5.8 Solidify Your Knowledge 111 2. Hand-simulating backpropagation of error as in the previous example, repeat the calculation for the following two cases: High output-layer weights: w.1/ 11 D 3:0; w.1/ 12 D/NUL3:0; w.1/ 21 D 3:0; w.1/ 22 D 3:0 Small output-layer weights: w.1/ 11 D 0:3; w.1/ 12 D/NUL0:3; w.1/ 21 D 0:3; w.1/ 22 D 0:3 Observe the relative changes in the weights in each case. 3. Cons"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00122", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 6 Decision Trees The classiﬁers discussed in the previous chapters expect all attribute values to be presented at the same time. Such a scenario, however, has its ﬂaws. Thus a physician seeking to come to grips with the nature of her patient’s condition often has nothing to begin with save a few subjective symptoms. And so, to narrow the ﬁeld of diagnoses, she prescribes lab tests, and, ba"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00123", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "more appropriate because the “leaf” in question is supposed to be a data abstraction that has nothing to do with the original physical object. 2In as sense, the decision tree can be seen as a simple mechanism for data compression."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00124", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "A domain expert inspecting these rules may then decide whether they are intuitively appealing, and whether they agree with his or her “human"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00125", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "116 6 Decision Trees understanding” of the problem at hand. The expert may even be willing to suggest improvements to the tree; for instance, by pointing out spurious tests that have found their way into the data structure only on account of some random regularity in the data. Missing Edge The reader will recall that, in linear classiﬁers, an example may ﬁnd itself exactly on the class-separating "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00126", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "6.2 Induction of Decision Trees 117 6.2 Induction of Decision Trees We will begin with a very crude induction algorithm. Applying it to a training set, we will realize that a great variety of alternative decision trees can be obtained. A brief discussion will convince us that, among these, the smaller ones are to be preferred. This observation will motivate an improved version of the technique, th"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00127", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "measure the amount of information provided by each attribute (and such a mechanism indeed exists, see Sect. 6.3), we are ready to formalize the technique for induction of decision trees by a pseudocode. The reader will ﬁnd it in ."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00128", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "6.3 How Much Information Does an Attribute Convey? 119 Table 6.2 Induction of decision trees Let T be the training set. grow(T): (1) Find the attribute, at, that contributes the maximum information about the class labels. (2) Divide T into subsets, Ti, each characterized by a different value of at. (3) For each Ti: If all examples in Ti belong to the same class, then create a leaf labeled with thi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00129", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "120 6 Decision Trees Table 6.3 Some values of the information contents (measured in bits) of the message, “this randomly drawn example is positive.” ppos /NULlog2 ppos 1.00 0b i t s 0.50 1b i t 0.25 2b i t s 0.125 3 bits Note that the mes- sage is impossible for p pos D 0 both classes are known to be equally represented so thatppos D 0:5. Here, the guess is no better than a ﬂipped coin, so the mes"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00130", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "6.3 How Much Information Does an Attribute Convey? 121 log 1 D 0). By the way, the case with ppos D 1 or pneg D 1 is regarded as perfect regularity because all examples belong to the same class; conversely, the case with ppos D pneg D 0:5 is seen as a total lack of regularity. Amount of Information Contributed by an Attribute The concept of entropy (lack of regularity) will help us deal with the m"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00131", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "122 6 Decision Trees Table 6.4 The algorithm to ﬁnd the most informational attribute 1. Calculate the entropy of the training set, T, using the percentages, ppos and pneg,o f the positive and negative examples: H.T/ D/NULppos log2 ppos /NULpneg log2 pneg 2. For each attribute, at,t h a td i v i d e sT into subsets, Ti, with relative sizes Pi,d ot h e following: (i) calculate the entropy of each su"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00132", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "6.4 Binary Split of a Numeric Attribute 123 Table 6.5 Illustration of the search for the most informative attribute Example crust shape filling Class size size e1 big circle small pos e2 small circle small pos e3 big square small neg e4 big triangle small neg e5 big square big pos e6 small square small neg e7 small square big pos e8 big circle big pos Here is the entropy of the training set where "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00133", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "124 6 Decision Trees Converting a Continuous Attribute to a Boolean One Let us denote the contin- uous attribute by x. The trick is to choose a threshold, /DC2, and then decide that if x </DC2, then the value of the newly created boolean attribute istrue, and otherwise it is false (or vice versa). Simple enough. But what concrete /DC2to choose? Surely there are many of them? Here is one possibilit"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00134", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "6.4 Binary Split of a Numeric Attribute 125 Table 6.6 Illustration of the search for the best threshold The values of attribute x are sorted in ascending order. The candidate thresholds are located between values labeled with opposite class labels. θ x ___ __ _ θθ1 2 3 + + +++ + + Here is the entropy of the training set, ignoring attribute values: H.T/ D/NULpC log pC /NULp/NUL log p/NUL D/NUL.7=13"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00135", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "126 6 Decision Trees Table 6.7 Algorithm to ﬁnd the best numeric-attribute test 1. For each attribute ati: (i) Sort the training examples by the values of ati; (ii) Determine the candidate thresholds, /DC2ij, as those lying between examples with opposite labels; (iii) For each /DC2ij, determine the amount of information contributed by the boolean attribute thus created. 2. Choose the pair Œati;/DC"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00137", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "128 6 Decision Trees labeled with the class most frequent among the training examples reaching that leaf. Since there are usually several (or many) subtrees that can thus be replaced, a choice has to be made; and the existence of a choice means we need a criterion to guide our decision. Here is one possibility. We know that pruning is likely to change the classiﬁer’s performance. One way to assess"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00138", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "6.5 Pruning 129 The main reason why pruning tends to improve classiﬁcation performance on future examples is that the removal of low-level tests, which have poor statistical support, usually reduces the danger of overﬁtting. This, however, works only up to a certain point. If overdone, a very high extent of pruning can (in the extreme) result in the decision being replaced with a single leaf label"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00139", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "130 6 Decision Trees • Describe the principle of post-pruning and the principle of on-line pruning. • What parameters control the extent of pruning? How do they affect the error rate on the training set, and how do they affect the error rate on the testing set? 6.6 Converting the Decision Tree into Rules One of the advantages of decision trees in comparison with the other classiﬁers is their inter"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00141", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "132 6 Decision Trees We haste to admit, however, that the price for this added ﬂexibility is a signiﬁcant increase in computational costs. What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have problems, return to the corresponding place in the preceding text. • Explain the mechanism that converts a decision tree to a set of rules. How man"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00142", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "6.8 Solidify Your Knowledge 133 the remaining class becoming the default class. The rules are usually easier to interpret. Rule-pruning algorithms sometimes lead to more compact classiﬁers, though at signiﬁcantly increased computational costs. Historical Remarks The idea behind decision trees was ﬁrst put forward by Hoveland and Hund in the late 1950s. The work was later summarized in the book Hun"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00144", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "6.8 Solidify Your Knowledge 135 Computer Assignments 1. Implement the baseline algorithm for the induction of decision trees and test its behavior on a few selected domains from the UCI repository. 4 Compare the results with those achieved by the k-NN classiﬁer. 2. Implement the simple pruning mechanism described in this chapter. Choose a data ﬁle from the UCI repository. Run several experiments a"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00145", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 7 Computational Learning Theory As they say, nothing is more practical than a good theory. And indeed, mathematical models of learnability have helped improve our understanding of what it takes to induce a useful classiﬁer from data, and, conversely, why the outcome of a machine- learning undertaking so often disappoints. And so, even though this textbook does not want to be mathematical, "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00146", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "138 7 Computational Learning Theory expression of attribute values; the expression is true for positive examples and false for negative examples. And, ﬁnally, there exists at least one expression that correctly classiﬁes all training examples.1 Each of the logical expressions can then be regarded as one hypothesis about what the classiﬁer should look like. Together, all of the hypotheses form ahyp"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00147", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "7.1 PAC Learning 139 “survivors,” some will disappoint in the sense that, while being error-free on the training set, their error rates on the entire instance space actually exceed/SI. Let there be k such classiﬁers. The concrete value of k cannot be established without evaluating each single classiﬁer on the entire instance space. This being impossible, all we can say is that k /DC4jHj, which is "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00148", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "140 7 Computational Learning Theory Table 7.1 The variables involved in our studies of PAC-learnability m ::: The number of the training examples jHj ::: The size of the hypothesis space /SI ::: The classiﬁer’s maximum permitted error rate ı ::: The probability that a classiﬁer with error rate greater than /SIis error-free on the training set needed to satisfy the given ( /SI;ı)-requirements is so"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00149", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "a variation of the hill-climbing search from Sect. 1.2 might be used to this end."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00151", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "7.3 Some Practical and Theoretical Consequences 143 conjunction of attribute values can be learned from a reasonably sized training set, whereas a general concept deﬁned by any boolean function usually cannot. Some other corollaries of this analysis will be the subject of Sect. 7.3. What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have pr"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00153", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "7.4 VC-Dimension and Learnability 145 The same result enables us to form an opinion about how a class learnability might be affected by the presence of irrelevant or redundant attributes. The higher the number of these attributes, the higher the value of n, which means that more training examples have to be provided if we want to satisfy the ./SI; ı/-requirements. The lesson is clear: whenever we "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00154", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "146 7 Computational Learning Theory Fig. 7.1 The set of the three points on the left is “shattered” by a linear classiﬁer. The set of the three points on the right is not shattered by a linear classiﬁer because no straight line can separate the point in the middle from those on the sides This, however, is not the case of the three points on the right. As we can see, these points ﬁnd themselves all"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00155", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "7.4 VC-Dimension and Learnability 147 Table 7.2 VC-dimensions for some hypothesis classes in R n Hypothesis class VC-dimension Hyperplane n C 1 Hypersphere n C 2 Quadratic .nC1/.nC2/ 2 r-Order polynomial n C r r ! m /NAKmax.4 /SIlog 2 ı ; 8d /SIlog 13 /SI/ (7.9) Note that this means that the lower bound on m is either ( 4 /SIlog 2 ı )o r( 8d /SI log 13 /SI/, whichever is greater. The engineer inte"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00156", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "148 7 Computational Learning Theory Polynomials of higher order therefore better be avoided. On the other hand, the VC-dimensions of neural networks and decision trees (see Chaps. 5 and 6)a r e known to be more affordable—which is why these classiﬁers are preferred. A Word of Caution Just as in the case of Inequality (7.5), this one (Formula (7.9)) is the result of a worst-case analysis during whi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00157", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "7.6 Exercises and Thought Experiments 149 VC-dimension is denoted by d, then the number of the needed examples is determined by the following inequality: m /NAKmax.4 /SIlog 2 ı ; 8d /SIlog 13 /SI/ (7.11) • Inequalities ( 7.5) and (7.9) have been found by a worst-case analysis. In reality, therefore, much smaller training sets are usually sufﬁcient for the induction of high-quality classiﬁers. The "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00158", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "150 7 Computational Learning Theory if the conjunction is permitted to involve exactly three attributes? For instance, here is one conjunction from this class: att1 = true AND attr2 = false AND att3 = false 4. Consider a domain with n D 20 continuous-valued attributes. Calculate the VC- dimension for a classiﬁer that has the form of a quadratic function, and compare it with that of a third-order p"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00159", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 8 A Few Instructive Applications For someone wishing to become an expert on machine learning, mastering a handful of baseline techniques is not enough. Far from it. The world lurking behind a textbook’s toy domains has a way of complicating things, frustrating the engineer with unexpected obstacles, and challenging everybody’s notion of what exactly the induced classiﬁer is supposed to do "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00160", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "152 8 A Few Instructive Applications Fig. 8.1 As i m p l ew a yt o convert the image of a hand-written character into a vector of 64 continuous attributes, each giving the mean intensity of the corresponding ﬁeld each giving mean intensity of a field numeric attributes, a 6−by−4 matrix of hand-written “t” and “l.” You will be surprised how difﬁcult this is. And to convert the plain-English explana"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00161", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "8.1 Character Recognition 153 The ﬁrst thing to be considered is that, in the attribute-vector obtained by the mechanism from Fig.8.1, only a small percentage of the attributes (if any) are likely to be irrelevant or redundant, which means that this is not an issue to worry about (unless the number of attributes is increased way beyond those shown in Fig. 8.1). Another aspect guiding our choice of"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00162", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "154 8 A Few Instructive Applications The Classiﬁer Should Be Allowed to Reject an Example To decipher a person’s handwriting is far from easy. Certain letters are so ambiguous as to make the reader shrug his shoulders in despair. Yet the k-NN classiﬁer from Chap. 3 is undeterred: it always ﬁnds a nearest neighbor, and then simply returns its class, no matter how arbitrary this class is. Practical "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00165", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "8.2 Oil-Spill Recognition 157 particular project, the problem was side-stepped by ﬁrst inducing a decision tree, and then eliminating all attributes that never appeared in any of the tree’s tests. This was quite logical. After all, the choice of which attributes to include in the tree has been made based on the attributes’ information contents. Since information contents of irrelevant attributes i"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00166", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "158 8 A Few Instructive Applications As already mentioned, this project relied on the k-NN classiﬁer where this requirement is easy to satisfy: the trick consists in manipulating the margin between the number of votes supporting either of the two classes. For the sake of illustration, suppose the 7-NN classiﬁer is used. Here, the number of false positives can be reduced if we instruct the classiﬁe"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00169", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "8.4 Brain–Computer Interface 161 This ambition, however, turned out to be unrealistic. Practical experience showed that no “universal classiﬁer” of sleep data could be obtained in this manner: a classiﬁer induced from one person’s data could not be used to draw a hypnogram for another person without serious degradation in classiﬁcation performance. 1 This does not mean that machine learning is in "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00170", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:index ", "text": "162 8 A Few Instructive Applications Fig. 8.4 Organization of the experiment. After a “ready” signal (WS) comes the CUE (“left” vs. “right”). Once RS appears on the screen, the subject waits 1 s, then presses the button indicated by the CUE WS CUE RS movement 0 12345 6 8 7 Fig. 8.5 Arrangement of electrodes on the subject’s scalp. Only some of the electrodes are actually used (highlighted) C3 4C C"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00172", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "164 8 A Few Instructive Applications left, sending the cursor in the wrong direction; but if the following samples are correctly classiﬁed asright, the cursor will, after all, land in the correct rectangle. What Error Rate Is Acceptable? Under the scenario just described, an error rate of 30% (which seems quite high) turned out to be fully acceptable. Even when the cursor occasionally did start in"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00173", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "Thyroid 0.70 0.73 0.64 Rheumatology 0.67 0.61 0.56"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00175", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "8.6 Text Classiﬁcation 167 What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have problems, return to the corresponding place in the preceding text. • As for the results mentioned in this section, why was it impossible to conclude from them that the induced classiﬁers outperformed the human expert? • Apart from classiﬁcation accuracy, what"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00176", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "168 8 A Few Instructive Applications Typical Observations The great number of attributes makes the induction compu- tationally expensive. Thus in the case of decision trees, where one has to calculate the information gain separately for each attribute, this means that tens of thousands of attributes need to be investigated for each of the tests, and these calculations can take a lot of time, espec"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00177", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "8.7 Summary and Historical Remarks 169 8.7 Summary and Historical Remarks • The number of examples that can be used for learning depends on the particular application. In some domains, examples are abundant; for instance, if they can be automatically extracted from a database. In others, they may be rare and expensive, as was the case of the oil spill domain. • We may be totally in the dark as to "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00179", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:computer assignments", "text": "8.8 Exercises and Thought Experiments 171 Computer Assignments 1. Some realistic data sets for machine-learning experimentation can be found on the website of the National Institute of Standards and Technology (NIST). Find this web site, then experiment with some of these domains. 2. Go to the web and ﬁnd a website about the demography of the 50 states in the U.S. Identify an output variable whose"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00180", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 9 Induction of Voting Assemblies A popular way of dealing with difﬁcult problems is to organize a brainstorming session in which specialists from different ﬁelds share their knowledge, offering diverse points of view that complement each other to the point where they may inspire innovative solutions. Something similar can be done in machine learning, too. A group of classiﬁers is created i"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00181", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "so convenient. If an example is misclas- siﬁed by two out of the three classiﬁers, the voting will result in the wrong class. One can stipulate, however, that this unfavorable situation might be improved if the number of classiﬁers is increased."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00182", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "9.1 Bagging 175 Fig. 9.1 The three classiﬁers were asked to classify the same 17 examples. By doing so each erred on three examples—different for each classiﬁer. The reader can see that these errors can be eliminated by voting classifier 3: classifier 2: classifier 1: ... a correctly classified example ... an incorrectly classified example The most important lesson to be drawn is that bagging work"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00183", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "examples it contains. Induce from T3 classiﬁer C3. 4. For classiﬁcation, use plain majority voting."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00184", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "9.2 Schapire’s Boosting 177 classif.1 classif.2 classif.3 classif.4 classif.5 master classif.A master classif.B Fig. 9.2 Recursive application of Schapire’s boosting. Master classiﬁer A combines the votes of classiﬁers 1–3. Then, master classiﬁer B combines the votes of master classiﬁer A with those of classiﬁers 4 and 5 these, classiﬁer C4 is induced. Finally, training subset T5 is created from e"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00185", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "178 9 Induction of V oting Assemblies master classif.B class.3class.1 class.2 class.4 class.6 class.5 class.7 class.8 class.9 master classif. master classif. master classif. Fig. 9.3 Another approach to recursive application of Schapire’s boosting Recall that, to induce the second classiﬁer, C2, we need to create a training subset, T2, that has been chosen in a way that makes the previous classiﬁe"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00186", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "9.3 Adaboost: Practical Version of Boosting 179 • Explain the two ways of implementing the principle recursively. How many classiﬁers are induced in each case if NR levels of recursion are used? Explain also the voting mechanism that reaches the ﬁnal classiﬁcation. • When compared to the error rate of the individual classiﬁers, what is the error rate of the ﬁnal classiﬁcation? • Explain the practi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00187", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "180 9 Induction of V oting Assemblies For each of the next subsets, Ti, the probabilities of the individual examples are modiﬁed based on the observed behavior of the previous classiﬁer, Ci/NUL1. The idea is to make sure that examples misclassiﬁed by the previous classiﬁers should get a higher chance of being included inT i. This will focus the next classiﬁer,Ci, on those aspects that the previous"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00188", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "9.3 Adaboost: Practical Version of Boosting 181 The sum of these values is 0:4 C 0:2 C 0:2 D 0:8. Dividing each of the three probabilities by 0.8 will give the following normalized values: p1 D 0:4 0:8 D 0:5; p2 D 0:2 0:8 D 0:25; p3 D 0:2 0:8 D 0:25 It is easy to verify that the new probabilities now sum up to 1: p1 C p2 C p3 D 0:5 C 0:25 C 0:25 D 1:0 An Illustration Table 9.4 shows how Adaboost m"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00189", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "182 9 Induction of V oting Assemblies Table 9.4 An example illustrating how Adaboost modiﬁes the probabilities of the individual examples Suppose that the training set, T, consists of ten examples denoted as x1 ::: x10. The total number of examples being m D 10, all initial probabilities are set to p.xi/ D 1=m D 0:1: p1.x1/ p1.x2/ p1.x3/ p1.x4/ p1.x5/ p1.x6/ p1.x7/ p1.x8/ p1.x9/ p1.x10/ 0:1 0:1 0:"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00190", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "9.4 Variations on the Boosting Theme 183 Practically speaking, though, one can just as well use the perceptron-learning algorithm from Chap. 4. The idea is to begin with equal weights for all classiﬁers, and then present the system, one by one, with the training examples. Each time the master classiﬁer misclassiﬁes an example, we increase or decrease the weights of the individual classiﬁers accord"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00192", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "9.5 Cost-Saving Beneﬁts of the Approach 185 Table 9.5 The class labels suggested by the six base-level classiﬁers are used as attributes to redescribe the examples x1 x2 x3 ... xm Classiﬁer 1 1 1 0 ... 0 Classiﬁer 2 0 0 1 ... 1 Classiﬁer 3 1 1 0 ... 1 Classiﬁer 4 1 1 0 ... 1 Classiﬁer 5 0 1 0 ... 1 Classiﬁer 6 0 0 0 ... 1 Real class 1 1 0 ... 1 Each column then represents a training example to be "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00193", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "186 9 Induction of V oting Assemblies Fig. 9.4 In some machine-learning techniques, computational costs grow quickly with the growing size of the training set. Quite often, induction from half of the examples incurs only a small fraction of the costs incurred by induction from all examples NN/2 T T/5 #examples CPU time An Illustration The situation is visualized in Fig. 9.4. Here, the time needed "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00194", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "9.6 Summary and Historical Remarks 187 In the case of Schapire’s Boosting and non-homogeneous boosting, the reader will recall that the sizes of Ti, are user-speciﬁed parameters. The same applies to stacking. The Costs of Example Selection When considering the savings in computational costs, however, we must not forget that the price for them is a certain unavoidable overhead. To be more speciﬁc, "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00195", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "188 9 Induction of V oting Assemblies achieved by the way the training subsets are created: the composition of T1 is random; the composition of T2 is such that C1 experiences 50% error rate on this set; and T3 consists of examples on which C1 and C2 disagree. Each of the three subsets contains the same number of examples. • By contrast, Adaboost chooses the examples probabilistically in a way that"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00196", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "9.7 Solidify Your Knowledge 189 Table 9.6 The probabilities of ten training examples p.x1/ p.x2/ p.x3/ p.x4/ p.x5/ p.x6/ p.x7/ p.x8/ p.x9/ p.x10/ 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.17 0.17 0.17 classiﬁers returning the neg label be Œ/NUL0:1; 0:3; 0:3; 0:4; 0:9/c141. What label is going to be returned by a master classiﬁer that relies onweighted majority voting? 3. Return to Table 9.5 that summar"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00197", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 10 Some Practical Aspects to Know About The engineer who wants to avoid disappointment has to be aware of certain machine-learning aspects that, for the sake of clarity, our introduction to the basic techniques had to neglect. To present some of the most important ones is the task for this chapter. The ﬁrst thing to consider is bias: to be able to learn, the learner has to build on certain"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00200", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "194 10 Some Practical Aspects to Know About underlying class. This is partly due to the way the training set has been created. In some applications, the training set has been created at random. In other domains, it consists of examples available at the given moment, which involves a great deal of randomness, too. And in yet others, the training set has been created by an expert who has chosen the "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00203", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "of randomly picked negative examples."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00204", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "198 10 Some Practical Aspects to Know About The Opposite Solution: Oversampling the Minority Class In some domains, however, the training set is so small that any further reduction of its size by undersampling is impractical. Even the majority-class examples are sparse, here; the deletion of any one of them may remove some critical aspect of the learning task, and thus jeopardize the performance o"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00205", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "separate classiﬁer from each of these training subsets. The second strategy mixes all examples in one big training set, and induces one “universal” classiﬁer."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00207", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "10.3 Context-Dependent Domains 201 such as “the oldest 25% examples.” The motivation for the deletion is simple: the engineer wants the window to contain only recent examples, suspecting that older ones may belong to an outdated context. As already mentioned, the classiﬁer is supposed to reﬂect only the examples contained in the window. In the simplest implementation, the classiﬁer is re-induced e"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00210", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "204 10 Some Practical Aspects to Know About the original training set, T, into a new training set, T0, where the original class label (e.g., pos or neg) becomes one of the attributes, whereas at will be treated as a class label. From this training set, we remove all examples whose values of at are unknown. From the rest, we induce the decision tree, and then use the decision tree to ﬁll in the mis"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00212", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "206 10 Some Practical Aspects to Know About Table 10.3 An wrapper approach to sequential attribute selection Divide the available set of pre-classiﬁed examples into two parts, TT and TE.L e tA be the set of attributes. Create an empty set, S. 1. For every attribute, ati 2 A: (i) add ati to S; let all examples in TT and TE be described by attributes from S; (ii) induce a classiﬁer from TT , then ev"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00213", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "10.6 Miscellaneous 207 This observation suggests a simple mechanism to be used when measuring the amount of regularity in data. The idea is simply to divide the data in two subsets— one for training and one for testing. The classiﬁer is induced from the training set, and then applied to the testing set. In the case of random data, we will observe only a small (if any) error rate on the training ex"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00214", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "208 10 Some Practical Aspects to Know About 10.7 Summary and Historical Remarks • Chapter 7 offered mathematical arguments supporting the claim that “there is no learning without bias.” Certain practical considerations have convinced us that this is indeed the case. • Sometimes, the meaning of the underlying class depends on a concrete context; this context can change in time, in which case we are"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00215", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "10.8 Solidify Your Knowledge 209 Table 10.4 As i m p l e exercise in “unknown values” Crust Filling Example Shape Size Shade Size Shade Class ex1 Circle Thick Gray Thick Dark pos ex2 Circle Thick White Thick Dark pos ex3 Triangle Thick Dark ? Gray pos ex4 Circle Thin White Thin ? pos ex5 Square Thick Dark Thin White pos ex6 Circle Thick White Thin Dark pos ex7 Circle Thick Gray Thick White neg ex8"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00216", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:computer assignments", "text": "210 10 Some Practical Aspects to Know About 4. In this section, the problem of imbalanced training sets was explored only within the framework of two-class domains where each example is either positive or negative. How does the same problem generalize to domains that have more than two classes? Suggest some concrete situations where imbalanced classes in such multi-class domains are or are not a p"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00217", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 11 Performance Evaluation The previous chapters pretended that performance evaluation in machine learning is a fairly straightforward matter. All it takes is to apply the induced classiﬁer to a set of examples whose classes are known, and then count the number of errors the classiﬁer has made. In reality, things are not as simple. Error rate rarely paints the whole picture, and there are s"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00218", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "212 11 Performance Evaluation Table 11.1 The basic quantities used in the deﬁnitions of performance criteria Labels returned by the classiﬁer pos neg True labels: pos NTP NFN neg NFP NTN For instance, NFP is the number offalse positives: negative examples misclassiﬁed by the classiﬁer as positive Correct and Incorrect Classiﬁcation Let us ﬁrst deﬁne four fundamental quan- tities that will be used "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00220", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "214 11 Performance Evaluation rejection rateerror rate 100% Fig. 11.1 Error rate can be increased by allowing the classiﬁer to refuse to classify an example if available evidence fails to “convince” it. At the extreme, the error rate drops to E D 0 because all examples are rejected adjusting the rejection rate. As we move from left to right, the rejection rate increases, whereas the error rate goe"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00224", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "218 11 Performance Evaluation Fig. 11.2 Example of ROC curves for two classiﬁers, c1 and c2. The parameters of the classiﬁers can be used to modify the numbers of false positives and false negatives accuracy on positives 0 0 error rate on negatives 1 1 c1 c2 Something similar is easy to implement in Bayesian classiﬁers and in neural networks. The idea is the same: label the example with the prefer"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00225", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "11.3 Other Ways to Measure Performance 219 • Explain the nature of an ROC curve. What extra information does the curve convey about a classiﬁer’s behavior? How does the ROC curve help the user in the choice between two alternative classiﬁers? 11.3 Other Ways to Measure Performance Apart from error rate, classiﬁcation accuracy, precision, and recall, other criteria are sometimes used, each reﬂectin"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00226", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "220 11 Performance Evaluation Sensitivity and Speciﬁcity The choice of the concrete criterion is often inﬂuenced by the given application ﬁeld—with its speciﬁc needs and deep-rooted traditions that should not be ignored. Thus the medical domain has become accustomed to assessing performance of their “classiﬁers” (not necessarily developed by machine learning) by sensitivity and speciﬁcity. In esse"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00227", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "11.3 Other Ways to Measure Performance 221 Another Numeric Example Again, suppose that the evaluation of a classiﬁer on a testing set resulted in the values summarized in the upper part of Table 11.2.T h e values of sensitivity, speciﬁcity, and gmean are calculated as follows: Se D 20 50 C 20 D 0:29 Sp D 900 900 C 30 D 0:97 gmean D r 20 50 C 20 /STX 900 900 C 30 D p 0:29 /STX0:97 D 0:53 Cost Funct"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00229", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "11.4 Learning Curves and Computational Costs 223 incorrect bias (see Sect. 10.1). By contrast, the learning curve of the second learner, l2, does not grow so fast, but in the end achieves higher levels of accuracy thanl1. Which of the two curves indicates a preferable learner depends on the circum- stances of the given application. When the source of the training examples is limited, the ﬁrst lear"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00230", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "224 11 Performance Evaluation 11.5 Methodologies of Experimental Evaluation The reader understands that different domains will beneﬁt from different induction techniques. The choice is usually not difﬁcult to make. Some knowledge of the available training data often helps us choose the most appropriate paradigm; for instance, if a high percentage of the attributes are suspected to be irrelevant, t"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00231", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "11.5 Methodologies of Experimental Evaluation 225 Fig. 11.4 N-fold cross-validation divides the training set into N equally sized subsets. In each of the N experimental runs, it withholds a different subset for testing, inducing the classiﬁer from the union of the remaining N /NUL1 subsets 2. 1. 3. 4. 5. the original trainig set 5−fold cross−validation ... training ... testing The advantage of N-f"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00232", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "226 11 Performance Evaluation Table 11.3 The algorithm for 5 /STX2 cross-validation (5 /STX2 CV) Let T be the original set of pre-classiﬁed examples. 1. Divide T randomly into two equally-sized subsets. Repeat the division ﬁve times. The result is ﬁve pairs of subsets denoted as Ti1 and Ti2 (for i D 1 ;:::5 ). 2. For each of these pairs, use Ti1 for training and Ti2 for testing, and then the other"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00233", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "11.6 Summary and Historical Remarks 227 11.6 Summary and Historical Remarks • The basic criterion to measure classiﬁcation performance is error rate, E, deﬁned as the percentage of misclassiﬁed examples in the given set. The complementary quantity is classiﬁcation accuracy, Acc D 1 /NULE. • When the evidence for any class is not sufﬁciently strong, the classiﬁer should better reject the example to"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00234", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "228 11 Performance Evaluation Historical Remarks Most of the performance criteria discussed in this chapter are well established in the statistical literature, and have been used for such a long time that it is difﬁcult to trace their origin. The exception is the relatively recent gmean that was proposed to this end by Kubat et al. [51]. The idea to refuse to classify examples where the k-NN class"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00235", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "11.7 Solidify Your Knowledge 229 Give It Some Thought 1. Suggest a domain where precision is much more important than recall; con- versely, suggest a domain where it is the other way round, recall being more important than precision. (Of course, use different examples than those mentioned in this chapter.) 2. What aspects of the given domain is reﬂected in the pair, sensitivity and speciﬁcity? Sug"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00236", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 12 Statistical Signiﬁcance Suppose you have evaluated a classiﬁer’s performance on an independent testing set. To what extent can you trust your ﬁndings? When a ﬂipped coin comes up heads eight times out of ten, any reasonable experimenter will suspect this to be nothing but a ﬂuke, expecting that another set of ten tosses will give a result closer to reality. Similar caution is in place w"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00238", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "12.1 Sampling a Population 233 Estimates Based on Random Samples The answer is provided by a theorem that says that estimates based on samples become more accurate with the growing sample size, n. Further on, the larger the samples, the smaller the variation of the estimates from one sample to another. Another theorem, the so-called central limit theorem, states that the distribution of the indivi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00239", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "234 12 Statistical Signiﬁcance decide to base our estimates on samples of the same size, n D 60, but in a domain where the proportion is higher, say, p D 0:95? In this event, we will realize that n.1 /NULp/ D 60 /SOH0:05 D 3<1 0 , which means that Condition (12.2) is not met, and the distribution of the proportions in samples of this size cannot be approximated by the normal distribution. For this"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00240", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "12.2 Beneﬁting from the Normal Distribution 235 This said, we should also be aware of the fact that increasing the sample size brings diminishing returns. Let us illustrate this statement using a simple example. The calculations carried out in the previous paragraph convinced us that, when proceeding from n D 50 to n D 100 (doubling the sample size), we managed to reduce s E by two percentage poin"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00241", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "236 12 Statistical Signiﬁcance Fig. 12.1 Gaussian (normal) distribution whose mean value is p −3σ 2σ −2σ −σ σ 3σ p distribution of these values can often be approximated by the normal distribution— the approximation being reasonably accurate if Conditions ( 12.1) and ( 12.2)a r e satisﬁed. The normal distribution can help us decide how much to trust the classiﬁcation accuracy (or, for that matter,"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00242", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "12.2 Beneﬁting from the Normal Distribution 237 Table 12.2 For the normal distribution, with mean p and standard deviation /ESC,t h el e f t column gives the percentage of values found in the interval Œ p /NULz /ETX/ESC;p C z/ETX/ESC/c141 Conﬁdence level (%) z/ETX 68 1.00 90 1.65 95 1.96 98 2.33 99 2.58 The formula deﬁning the normal distribution was introduced in Sect.2.5 where it was called the "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00243", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "238 12 Statistical Signiﬁcance The standard error2 is calculated using Eq. (12.3). For instance, if the theoretical classiﬁcation accuracy is p D 0:70, and the size of each testing set is n D 100, then the standard error of the classiﬁcation accuracies obtained from great many different testing sets is calculated as follows: s acc D r p.1 /NULp/ n D r 0:7.1 /NUL0:7/ 100 D 0:046 (12.4) After due ro"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00244", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "12.3 Conﬁdence Intervals 239 • How will you calculate the standard error of estimates based on a given testing set? How does this standard error depend on the size of the testing set? 12.3 Conﬁdence Intervals Let us now focus on how the knowledge gained in the previous two sections can help us specify the experimenter’s conﬁdence in the classiﬁer’s performance as measured on the given testing data"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00245", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "240 12 Statistical Signiﬁcance This was to be expected: the chance that the real, theoretical, classiﬁcation accuracy ﬁnds itself in a longer interval is higher. Conversely, it is less likely that the theoretical value will fall into some narrower interval. Thus for the conﬁdence level of 68% (and the standard error rounded to s acc D 0:04), the conﬁdence interval is Œ0:85 /NUL0:04; 0:85 C 0:04/c1"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00246", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "12.4 Statistical Evaluation of a Classiﬁer 241 What Have You Learned? To make sure you understand the topic, try to answer the following questions. If needed, return to the appropriate place in the text. • Explain the meaning of the term, conﬁdence interval . What is meant by the margin of error? • How does the size of the conﬁdence interval (and the margin of error) depend on the user-speciﬁed co"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00247", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "242 12 Statistical Signiﬁcance Let the experiment result in giving us classiﬁcation accuracy 0:75. Well, this is less than the promised 0:78, but then: is this observed difference still within reasonable bounds? To put it another way, is there a chance that the specialist’s claim was correct, and that the lower performance measured on the testing set can be explained by the variations implied by t"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00248", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "12.4 Statistical Evaluation of a Classiﬁer 243 Type-I Error in Statistical Evaluation: False Alarm The reader now understands the fundamental principle of statistical evaluation. Someone makes a statement about performance. Based on the size of our testing set (and assuming normal distribution), we calculate the size of the interval that is supposed to contain the given 95% of all values. There is"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00249", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "244 12 Statistical Signiﬁcance 12.5 Another Kind of Statistical Evaluation At this moment, the reader understands the essence of statistical processing of experimental results, and knows how to use it when evaluating the claims about a given classiﬁer’s performance. However, much more can be accomplished with the help of statistics. Do Two Testing Sets Represent Two Different Contexts? Chapter 10 "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00250", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "12.6 Comparing Machine-Learning Techniques 245 • Why should we be concerned that a classiﬁer is being applied to a wrong kind of data? • What formula is used here? How do you carry our the evaluation? 12.6 Comparing Machine-Learning Techniques Sometimes, we need to know which of two alternative machine-learning techniques is more appropriate for the given class-recognition problem. The usual metho"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00251", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "246 12 Statistical Signiﬁcance Table 12.4 Example experimental results of a comparison of two alternative machine-learning techniques, ML1 and ML2 T11 T12 T21 T22 T31 T32 T41 T42 T51 T52 ML1 78 82 99 85 80 95 87 57 69 73 ML2 72 79 95 80 80 88 90 50 73 78 d 6 3 4 5 0 7 /NUL3 7 /NUL4 /NUL5 The numbers in the ﬁrst two rows give classiﬁcation accuracies (in percentages), the last row gives the differe"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00252", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "12.7 Summary and Historical Remarks 247 t9 D 2 /NUL0 4:63= p 10 D 1:35 (12.11) Seeing that the obtained value is less than the 2.26 listed for the 95% conﬁdence level in the table, we conclude that the experiment has failed to refute (for the given conﬁdence level) the hypothesis that the two techniques lead to comparable classiﬁcation accuracies. We therefore accept the claim. What Have You Learn"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00253", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "248 12 Statistical Signiﬁcance Œacc /NULz/ETXsacc; acc C z/ETXsacc/c141 The term z/ETXsacc is referred to as themargin of error.For different performance metrics, similar formulas are used. • Suppose we are testing a claim about certain classiﬁcation accuracy, acc.I ft h e result of experimental evaluation falls into the conﬁdence interval deﬁned by the chosen conﬁdence level, we assume we do not "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00254", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "12.8 Solidify Your Knowledge 249 Give It Some Thought 1. Suppose you test a classiﬁer’s performance, using the 95%-conﬁdence interval. What if you change your mind and decide to use the 99%-conﬁdence instead? You will increase tolerance, but what is the price for this? Computer Assignments 1. This assignment assumes that the reader has already implemented a program dividing the data into the ﬁve “"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00255", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 13 Induction in Multi-Label Domains All the techniques discussed in the previous chapters assumed that each example is labeled with one and only one class. In realistic applications, however, this is not always the case. Quite often, an example is known to belong to two or more classes at the same time, sometimes to many classes. For machine learning, this poses certain new problems. After"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00257", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "13.1 Classical Machine Learning in Multi-Label Domains 253 Nearest-Neighbor Classiﬁers Another possibility is the use of nearest-neighbor classiﬁers with which we got acquainted in Chap. 3. When example x is presented, the k-NN classiﬁer ﬁrst identiﬁes the example’s k nearest neighbors. Each of these may have been labeled with a set of classes, and the simplest classiﬁcation attempt in a multi-lab"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00258", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "254 13 Induction in Multi-Label Domains 13.2 Treating Each Class Separately: Binary Relevance Let us now proceed to the main topic of this section, the technique of binary relevance. We will begin by explaining the principle, and then discuss some of its shortcomings and limitations. The Principle of Binary Relevance The most common approach to multi-label domains induces a separate binary classiﬁ"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00260", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "256 13 Induction in Multi-Label Domains a given paradigm) learning from 50% of the examples takes only 5% of the time, considerable savings can be achieved. Problem 3: Performance Evaluation Another question is how to measure the success or failure of the induced classiﬁers. Usually, each of them will exhibit different performance, some better than average, some worse than average, and some dismal"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00261", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "to insist on an inappropriate classiﬁer sequence may be harmful to the point where the classiﬁcation performance of the induced system may fail to reach even that of plain binary relevance."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00262", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "258 13 Induction in Multi-Label Domains Sometimes, however, there is a way out. If the number of classes is manage- able (say, ﬁve), the engineer may choose to experiment with several alternative sequences, and then choose the best one. But if the number of classes is greater, the necessity to try many alternatives will be impractical. Error Propagation The fact that the classiﬁers are forced into"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00264", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "260 13 Induction in Multi-Label Domains interdependence, assuming that the class interdependence (or the lack thereof) is likely to be discovered in the course of the learning process. When treated dogmatically, however, this principle may do more harm than good. The fact that x belongs to Ci often has nothing to do with x belonging to Cj. If this is the case, forcing the dependence link between t"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00267", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "13.6 Aggregating the Classes 263 What Have You Learned? To make sure you understand the topic, try to answer the following questions. If needed, return to the appropriate place in the text. • Give an example of a domain where the individual classes are hierarchically ordered. • Explain the training-set dividing principle for the induction of hierarchically ordered classiﬁers. • What are the most c"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00269", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "13.7 Criteria for Performance Evaluation 265 A solution of the last resort will aggregate the classes only if the given combination is found in a sufﬁcient percentage of the training examples. If the combination is rare, the corresponding Ti is not created. Although this means that the induced classiﬁers will not recognize a certain combination, this may not be such big loss if the combination is "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00270", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "266 13 Induction in Multi-Label Domains PrM D 1 L LX iD1 pri ReM D 1 L LX iD1 rei (13.1) FM 1 D 1 L LX iD1 F1i Macro-averaging is suitable in domains where each class has approximately the same number of representatives. In some applications, this requirement is not satisﬁed, but the engineer may still prefer macro-averaging if he or she considers each class to be equally important, regardless of "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00272", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "268 13 Induction in Multi-Label Domains Averaging the Performance over Examples So far, the true and false positive and negative examples were counted across individual classes. However, in domains where an average example belongs to a great many classes, it can make sense to average over the individual examples. The procedure is in principle the same as before. When comparing the true class label"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00273", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "13.9 Solidify Your Knowledge 269 • One weakness of classiﬁer chains is that the user is expected to specify the sequence of classes (perhaps according to class subsumption). If the sequence is poorly designed, the results are disappointing. • Another shortcoming is known as error propagation: an incorrect label given to an example by one classiﬁer is passed on to the next classiﬁer in the chain, p"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00275", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "13.9 Solidify Your Knowledge 271 4. Suppose you have been asked to develop machine-learning software for induction from multi-label examples. This chapter has described at least four approaches that you can choose from. Write down the main thoughts that would guide your decision. 5. Suggest a mechanism that would mitigate the problem of error propagation during multi-label induction with hierarchi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00276", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 14 Unsupervised Learning It would be a mistake to think that machine learning always requires examples with class labels. Far from it! Useful information can be gleaned even from examples whose classes are not known. This is sometimes called unsupervised learning,i n contrast to the term supervised learning which is used when talking about induction from pre-classiﬁed examples. While super"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00278", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "14.1 Cluster Analysis 275 each example can be seen as representing its own single-example cluster. Practical implementations often side-step the problem by asking the user to supply the number of clusters by way of an input parameter. Sometimes, however, machine- learning software is expected to determine the number automatically. Problems with Measuring Distances Algorithms for cluster analysis u"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00279", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "276 14 Unsupervised Learning Beneﬁt 1: Estimating Missing Values The knowledge of the clusters can help us estimate missing attribute values. Returning to Fig. 14.1, the reader will notice that ifweight is low, the example is bound to belong to the bottom-left cluster. In this case, also height is likely to be low because it is low in all examples found in this cluster. This aspect tends to be eve"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00280", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:index ", "text": "14.2 A Simple Algorithm: k-Means 277 14.2 A Simple Algorithm: k-Means Perhaps the simplest algorithm to detect clusters of data is known under the namek- means.T h e“k” in the name denotes the requested number of clusters—a parameter whose value is supplied by the user. The Outline of the Algorithm The pseudocode of the algorithm is provided in Table 14.1. The ﬁrst step creates k initial clusters "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00281", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "the affected centroids are recalculated."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00282", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "14.2 A Simple Algorithm: k-Means 279 Table 14.2 Illustration of the k-means procedure in a domain with two attributes The table below contains three initial groups of vectors. The task is to ﬁnd “ideal” clusters using the k-means (k D 3). Group-1 Group-2 Group-3 .2; 5/ .4; 3/ .1; 5/ .1; 4/ .3; 7/ .3; 1/ .3; 6/ .2; 2/ .2; 3/ Centroids: .2; 5/ .3; 4/ .2; 3/ Let us pick the ﬁrst example in group-2. T"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00283", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:contents ", "text": "280 14 Unsupervised Learning As for boolean attributes, their values can simply be replaced with 1 and 0 fortrue and false, respectively. Finally, an attribute that acquires n discrete values (such as season, which has four different values) can be replaced withn boolean attributes, one for each value—and, again, for the values of these boolean attributes, 1 or 0 are used. Computational Aspects of"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00284", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "14.3 More Advanced Versions of k-Means 281 The situation changes when we choose for the code vectors examples x and z. In this event, the two initial clusters will have a very different composition, and k- means is likely to converge on a different set of clusters. The phenomenon will be more pronounced if there are “outliers,” examples that do not apparently belong to any of the two clusters. Sum"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00285", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "where c i and cj are centroids: S D X i¤ j d.ci; cj/ (14.4)"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00286", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "14.4 Hierarchical Aggregation 283 Conversely, splitting makes sense when the average example-to-example dis- tance within some cluster is high. The concrete solution is not easy to formalize because once we have speciﬁed that cluster C is to be split into C1 and C2, we need to decide which of C’s examples will go to the ﬁrst cluster and which to the second. Very often, however, it is perfectly acc"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00287", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "284 14 Unsupervised Learning height weight x Fig. 14.4 Note that the leftmost examples in the “bottom” cluster are closer to the “upper” cluster’s centroid than to its own. In a domain of this kind k-means will not ﬁnd the best solution This approach, however, will do a poor job if the clusters are of a different (“non-convex”) nature. To see the point, consider the clusters in Fig. 14.4. Here, th"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00288", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "14.4 Hierarchical Aggregation 285 AB x1 D .1; 0/ y1 D .3; 3/ x2 D .2; 2/ y2 D .4; 4/ Table 14.3 The basic algorithm of hierarchical aggregation Input: a set of examples without labels 1. Let each example form one cluster. For N examples, this means creating N clusters, each containing a single example. 2. Find a pair of clusters with the smallest cluster-to-cluster distance. Merge the two clusters"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00290", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "14.5 Self-Organizing Feature Maps: Introduction 287 x x12 Fig. 14.6 General schema of a Kohonen network How to Model Attraction Each neuron is described by a weight vector, w D .w1;:::; wn/ where n is the number of attributes describing the examples. If x D .x1 :::; xn/ is the example, and if /DC12 .0; 1/ is a user-speciﬁed learning rate, then the individual weights are modiﬁed according to the fo"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00291", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "288 14 Unsupervised Learning Fig. 14.7 The idea of “neighborhood” in the Kohonen network vector is most similar to x. To this end, the Euclidean distance is used—smaller distance means greater similarity. Once the winner has been established, the second step updates the weights of this winning neuron as well as the weights of all neurons in the winner’s physical neighborhood. A Note on “Neighborho"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00292", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "14.6 Some Important Details 289 14.6 Some Important Details Having outlined the principle, let us now take a look at some details without which the technique would underperform. Normalization The technique does not work well unless all vectors have been normalized to unit length (i.e., length equal to 1). This applies both to the vectors describing the examples, and to the weight vectors of the ne"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00293", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "290 14 Unsupervised Learning Table 14.4 The basic algorithm of self-organizing feature maps Input: set of examples without labels a learning rate, /DC1. a set of randomly initialized neurons arranged in a matrix 1. Normalize all training examples to unit length. 2. Present a training example, and ﬁnd its nearest code vector, cwinner 3. Modify the weights of cwinner, as well as the weights of the c"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00294", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "14.7 Why Feature Maps? 291 14.7 Why Feature Maps? Let us now turn our attention to some practical beneﬁts of self-organizing feature maps. Reducing Dimensionality The technique introduced in the previous section essen- tially maps the original N-dimensional space of the original attribute vectors to the two-dimensional space of the neural matrix: each example has its winning neuron, and the winner"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00295", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "292 14 Unsupervised Learning to be mapped to the same neuron, or at least to neurons that are physically close to each other. Similar observations can be made about general data distribution. In this sense, feature maps can be regarded as a useful visualization tool. Initializing k-Means Each of the weight vectors in the neural matrix can be treated as a code vector. The mechanism of SOFM makes it"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00296", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:historical remarks", "text": "14.8 Summary and Historical Remarks 293 14.8 Summary and Historical Remarks • In some machine-learning tasks, we have to deal with example vectors that do not have class labels. Still, useful knowledge can be induced for them by the techniques of unsupervised learning. • The simplest task in unsupervised learning is cluster analysis. The goal is to ﬁnd a way that naturally divides the training set"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00297", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "294 14 Unsupervised Learning 14.9 Solidify Your Knowledge The exercises are to solidify the acquired knowledge. The suggested thought experiments will help the reader see this chapter’s ideas in a different light and provoke independent thinking. Computer assignments will force the readers to pay attention to seemingly insigniﬁcant details they might otherwise overlook. Exercises 1. Look at the th"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00298", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "14.9 Solidify Your Knowledge 295 3. Explain how self-organizing feature maps can be used to deﬁne the code vectors with which k-means sometimes starts. Will it be more meaningful to use the opposite approach (initialize SOFM) by kmeans)? Computer Assignments 1. Write a program that accepts as input a training set of unlabeled examples, chooses among them k random code vectors, and creates the clus"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00299", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 15 Classiﬁers in the Form of Rulesets Some classiﬁers take the form of so-called if-then rules: if the conditions from the if -part are satisﬁed, the example is labeled with the class speciﬁed in the then-part. Typically, the classiﬁer is represented not by a single rule, but by a set of rules, a ruleset. The paradigm has certain advantages. For one thing, the rules capture the underlying "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00301", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "the ruleset will generalize the ruleset because the new rule will add to the set of covered examples."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00302", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "300 15 Classiﬁers in the Form of Rulesets What Have You Learned? To make sure you understand the topic, try to answer the following questions. If needed, return to the appropriate place in the text. • Explain the nature of rule-based classiﬁers. What do we mean when we say that a rule covers an example? Using this term (cover), specify how the induced classiﬁer should behave on a consistent and no"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00304", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "302 15 Classiﬁers in the Form of Rulesets Let NC old be the number of positive examples covered by the original version of the rule, and letN/NUL old be number of negative examples covered by the original version of the rule. Likewise, the numbers of positive and negative examples covered by the new version of the rule will be denoted by N C new and N/NUL new, respectively. Since the rule covers o"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00305", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "next step, we want to add a condition to the antecedent. To this end, we may consider various possibilities, but the simplest appears to be parent(x,y)—which will also be supported by the information-gain criterion. We have obtained the following rule:"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00306", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "high_very_long_token_ratio", "text": "304 15 Classiﬁers in the Form of Rulesets Illustration of induction from examples described using predicate logic Consider the knowledge base consisting of the following positive examples of classes parent andancestor, deﬁned using prolog-like facts (any other example will be regarded as negative). parent(eve,ted) ancestor(eve,ted)ancestor(eve,ivy) parent(tom,ted) ancestor(tom,ted)ancestor(eve,ann"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00307", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "15.4 More Advanced Search Operators 305 next rule, we may consider the following (note that this is always the same predicate, but each time with a different set of arguments): parent(x,z) parent(z,y) Suppose that the ﬁrst leads to higher information gain. Seeing that the rule still covers some negative examples, we want to specialize it by adding another condition to its antecedent. Seeing that t"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00308", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "306 15 Classiﬁers in the Form of Rulesets /NULidentiﬁcation: /SUBb; x ! a b; c; d ! a /ESC ) /SUBb; x ! a c; d ! x /ESC /NULabsorption: /SUBc; d ! x b; c; d ! a /ESC ) /SUBc; d ! x b; x ! a /ESC /NULinter-construction: /SUBv; b; c ! a w; b; c ! a /ESC ) 8 < : u; b; c ! a v ! u w ! u 9 = ; /NULintra-construction: /SUBv; b; c ! a w; b; c ! a /ESC ) 8 < : v; u ! a w; u ! a b; c ! u 9 = ; Note that th"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00309", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "15.6 Solidify Your Knowledge 307 • If a rule’s antecedent is true, for an example, we say that the rule covers the example. • In the course of rule induction, we often rely on specialization. This reduces the set of covered examples to its subset. A rule is specialized if we add a condition to its antecedent. Conversely,generalization enlarges the set of covered examples to its superset. • Usually"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00310", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:computer assignments", "text": "308 15 Classiﬁers in the Form of Rulesets 2. Show that, when we choose different ways of specializing a rule (adding different attribute-value pairs), we in the end obtain a different ruleset, often of a different size. Give It Some Thought 1. Think of some other examples of classes (different from those discussed in this chapter) that are best deﬁned recursively. 2. Think about how classes that a"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00311", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 16 The Genetic Algorithm The essence of machine learning is the search for the best solution to our problem: to ﬁnd a classiﬁer which classiﬁes as correctly as possible not only the training examples, but also future examples. Chapter 1 explained the principle of one of the most popular AI-based search techniques, the so-called hill-climbing, and showed how it can be used in classiﬁer indu"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00313", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "16.2 Implementing the Individual Modules 311 Table 16.1 The principle of the genetic algorithm initial state: a population of individual chromosomes 1. The ﬁtness of each individual is evaluated. Based on its value, individuals are randomly selected for survival. 2. Survivors select mating partners. 3. New individuals are created by chromosome recombination of the mating partners. 4. Individual ch"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00314", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "312 16 The Genetic Algorithm Initial Population The most common approach to creating the initial population will employ a random-number generator. Sometimes, the engineer can rely on some knowledge that may help her create initial chromosomes known to outperform randomly generated individuals. In the “pies” domain, this role can be played by the descriptions of the positive examples. However, one "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00315", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "16.2 Implementing the Individual Modules 313 function. There is a difference, though: the notion of sex is usually ignored—any chromosome can mate with any other chromosome. An almost trivial mating strategy will pair the individuals arbitrarily, perhaps generating random pairs of integers from the intervalŒ1; Ns/c141, where Ns is the number of specimens in the population. However, this technique "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00316", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "314 16 The Genetic Algorithm In many applications, the recombination operator is applied only to a certain percentage of individuals. For instance, if 50 pairs have been selected for mating, and if the probability of recombination has been set by the user as 80%, then only 40 pairs will be subject to recombination, and the remaining 10 will just be copied into the next generation. The Mutation Ope"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00317", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "16.3 Why It Works 315 Table 16.2 Illustration of the genetic algorithm Suppose we want the genetic algorithm to ﬁnd the maximum of f .x/ D x2 /NULx.L e tx be an integer represented by a binary string. The initial population consists of the four strings in the following table that for each of them gives the integer value, x,t h e corresponding f .x/, the survival chances (proportional to f .x/), an"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00318", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "is almost ﬂat. The fact that different individuals have here virtually the same chances to survive defeats the purpose of the survival game. When the survivors are"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00319", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "16.4 The Danger of Premature Degeneration 317 xx f(x)f(x) Fig. 16.4 Examples of two ﬁtness functions that are poor guides for the genetic search. To be useful, the survival function should not be too ﬂat and it should not contain isolated narrow peaks chosen according to a near-uniform distribution, the qualities of the individuals will not give these individuals any perceptible competitive advant"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00320", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "more populations in parallel, in isolation from each other. Then, either at random intervals, or whenever"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00321", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "16.5 Other Genetic Operators 319 premature degeneration is suspected, a specimen from one population will be permitted to choose its mating partner in a different population. When implementing this technique, the programmer has to decide in which population to place the children. The Impact of Population Size Special attention has to be paid to the size of the population. Usually, though not alway"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00323", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "16.6 Some Advanced Versions 321 01000 01001 01000 01000 Inverting the middle three bits of the ﬁrst chromosome, and the last three bits of the second chromosome will result in the following population: 00010 01100 01000 01000 The reader can see that the diversity has indeed increased. This observation suggests a simple way to handle premature degeneration: just increase, for a while, the frequency"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00324", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "322 16 The Genetic Algorithm the genetic information remains unchanged throughout the specimen’s entire life. One pre-Darwinian biologist, Jean-Baptiste Lamarck, suggested something more ﬂexible: in his view, evolution might be driven by the individuals’ needs. A giraffe that keeps trying to reach the topmost leaves will stretch his neck that will thus become longer. This longer neck is then passe"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00326", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "324 16 The Genetic Algorithm What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have problems, return to the corresponding place in the preceding text. • What is the difference between the darwinian and the lamarckian evolution processes? Which of them is faster? • What weakness is remedied by the multi-population genetic algorithm? In what"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00327", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "16.7 Selections in k-NN Classiﬁers 325 attributesexamples chromosome 1 chromosome 2 SPECIMEN: Fig. 16.6 Each specimen is described by two chromosomes, one representing examples and the other representing attributes. Recombination is applied to each of them separately the chromosome would be the number of relevant attributes plus the number of representative examples. This mechanism is known as val"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00328", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "326 16 The Genetic Algorithm Note that the ﬁtness of a specimen is high if its error rate is low, if the set of retained examples is small, and if many attributes have been eliminated. The function is controlled by three user-set parameters, c1; c2, and c3, that weigh the user’s preferences. For instance, if c1 is high, emphasis is placed on classiﬁcation accuracy. If c2 or c3 are high, emphasis i"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00329", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "16.8 Summary and Historical Remarks 327 One possibility is to select, randomly, a pre-speciﬁed percentage of the locations in the newly created population and to add to each of them a random integer generated separately for the location. The result is then taken modulo the number of examples/attributes. Let the original number of examples/attributes be 100 and let the location selected for mutatio"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00330", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "328 16 The Genetic Algorithm pioneering the idea of genetic programming. The concrete way of applying the genetic algorithm to selections in the k-classiﬁer is from Rozsypal and Kubat [82]. 16.9 Solidify Your Knowledge The exercises are to solidify the acquired knowledge. The suggested thought experiments will help the reader see this chapter’s ideas in a different light and provoke independent th"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00331", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "16.9 Solidify Your Knowledge 329 Computer Assignments 1. Implement the baseline genetic algorithm to operate on binary-string chromo- somes. Make sure you have separate modules for the survival function, the wheel of fortune, recombination, and mutation, and that these modules are sufﬁciently general to enable easy modiﬁcations. 2. Create the initial populations for the “pies” and “circles” domain"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00332", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:doi ", "text": "Chapter 17 Reinforcement Learning The fundamental problem addressed by this book is how to induce a classiﬁer capable of determining the class of an object. We have seen quite a few techniques that have been developed with this in mind. In reinforcement learning, though, the task is different. Instead of induction from a set of pre-classiﬁed examples, the agent “experiments” with a system, and the"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00333", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "too_short", "text": "rkC1 is the .k C 1/st reward. 1At this point, let us remark that the returns can be negative—”punishments,” rather."}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00334", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "17.1 How to Choose the Most Rewarding Action 333 Table 17.1 The algorithm for the /SI-greedy reinforcement learning strategy Input: user-speciﬁed parameter /SI, e.g., /SID 0:1; a set of actions, ai, and their initial value-estimates, Q0.ai/; for each action, ai,l e tki D 0 (the number of times the action has been taken); 1. Generate a random number, p 2 .0; 1/, from the uniform distribution. 2. If"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00335", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "334 17 Reinforcement Learning What Have You Learned? To make sure you understand this topic, try to answer the following questions. If you have problems, return to the corresponding place in the preceding text. • Describe the /SI-greedy strategy to be used when searching for the best machine in the N-armed bandit problem. Explain the meaning of actions and their values. What is meant by exploitati"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00336", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "ocr_markers", "text": "17.2 States and Actions in a Game 335 pair has a certain value, Q.s; a/. Based on these values, the /SI-greedy policy decides which action should be taken in the particular state. The action results in a reward, r, and this reward is then used to update the value of the state-action pair by means of Formula (17.1). The most typical way of implementing the learning scenario is to let the program pl"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00337", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "336 17 Reinforcement Learning Fig. 17.3 The task: keep the pole upright by moving the cart left or right R D † 1 kD1/CRkrk (17.2) Note how the growing value of k decreases the coefﬁcient by which rk is multiplied. If the ultimate reward comes at the 10th step, and if “1” is the reward for the winning game, then the discounted reward for /CRD 0:9 is R D 0:910 /SOH1 D 0:35. Illustration: Pole Balanc"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00338", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:what have you learned", "text": "17.3 The SARSA Approach 337 17.3 The SARSA Approach The previous two sections introduced only a very simpliﬁed mechanism to deal with the reinforcement-learning problem. Without going into details, let us describe here a more popular approach that is known under the name of SARSA. The pseudocode summarizing the algorithm is provided in Table 17.2. Essentially, the episodic formulation with discoun"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00339", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "338 17 Reinforcement Learning 17.4 Summary and Historical Remarks • Unlike the classiﬁer-induction problems from the previous chapters, reinforce- ment learning assumes that an agent learns from direct experimentation with a system it is trying to control. • In the greatly simpliﬁed formalism of the N-armed bandit, the agent seeks to identify the most promising action—the one that offers the highe"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00340", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:solidify your knowledge", "text": "17.5 Solidify Your Knowledge 339 Give It Some Thought 1. This chapter is all built around the idea of using the /SI-greedy policy. What do you think are the limitations of this policy? Can you suggest how to overcome them? 2. The principles of reinforcement learning have been explained using some very simple toy domains. Can you think of an interesting real-world application? The main difﬁculty wi"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00341", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:bibliography", "text": "Bibliography 1. Ash, T. (1989). Dynamic node creation in backpropagation neural networks. Connection Science: Journal of Neural Computing, Artiﬁcial Intelligence, and Cognitive Research, 1 , 365–375. 2. Ball, G. H. & Hall, D. J. (1965). ISODATA, a novel method of data analysis and clasiﬁcation. Technical Report of the Standford University, Stanford, CA 3. Bellman, R. E. (1956). A problem in the se"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00342", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:bibliography", "text": "342 Bibliography 20. Cover, T. M. & Hart, P. E. (1967). Nearest neighbor pattern classiﬁcation.IEEE Transactions on Information Theory, IT-13, 21–27. 21. Dasarathy, B. V . (1991). Nearest-neighbor classiﬁcation techniques . Los Alomitos: IEEE Computer Society Press. 22. Dietterich, T. (1998). Approximate statistical tests for comparing supervised classiﬁcation learning algorithms. Neural Computati"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00343", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:bibliography", "text": "Bibliography 343 47. Koller, D. & Sahami, M. (1997). Hierarchically classifying documents using very few words. In Proceedings of the 14th International conference on machine learning, ICML’07 ,S a n Francisco, USA (pp. 170–178) 48. Kononenko, I., Bratko, I., & Kukar, M. (1998). Application of machine learning to medical diagnosis. In R. Michalski, I. Bratko, & M. Kubat (Eds.),Machine learning and"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00344", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:bibliography", "text": "344 Bibliography 72. Neyman, J. & Pearson E. S. (1928). On the use and interpretation of certain test criteria for purposes of statistical inference. Biometrica, 20A, 175–240. 73. Ogden, C. K. & Richards, I. A. (1923). The meaning of meaning (8th ed., 1946). New York: Harcourt, Brace, and World. 74. Parzen E. (1962). On estimation of a probability density function and mode. Annals of Mathematical "}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00345", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:bibliography", "text": "Bibliography 345 98. Widrow, B. & Hoff, M. E. (1960). Adaptive switching circuits. In IRE WESCON convention record, New York (pp. 96–104). 99. Wolpert, D. (1992). Stacked generalization. Neural Networks, 5, 241–259. 100. Wolpert, D. (1996). The lack of a priori distinctions between learning algorithms. Neural Computation, 8, 1341–1390. 101. Zhang, M.-L. & Zhou, Z.-H. (2007). ML-KNN: A lazy learing"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00346", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:index ", "text": "Index A applications, 151, 167 attributes continuous, 30, 38, 45–47, 84, 122, 145 discrete, 8, 22, 44, 137 irrelevant, 13, 49, 74, 75, 118, 144, 156 redundant, 13, 59, 118, 144, 156 selection, 204, 205, 324 unknown, 202 B backpropagation, 98 bias, 67, 143, 191, 193 C clustering hierarchical aggregation, 283 intercluster distance, 275, 284 k-means, 277, 281 normalization, 278 principle, 273 SOFM, 2"}
{"chunk_id": "2017_Book_AnIntroductionToMachineLearnin-00347", "source": "/workspace/t2v/data/2017_Book_AnIntroductionToMachineLearnin.pdf", "reason": "pattern:index ", "text": "348 Index neural networks backpropagation, 97 MLP architecture, 100 MLP as classiﬁers, 91 RBF networks, 106 noise in attributes, 14, 45, 57 in class labels, 14 normalization, 51, 278, 279, 289 P performance criteria Fˇ , 219 error rate, 15 macro-averaging, 265 micro-averaging, 266 precision, 215 recall, 215 sensitivity, 220 speciﬁcity, 220 polynomial classiﬁers, 79 predicates alternative search op"}
